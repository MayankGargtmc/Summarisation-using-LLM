# Summarisation-using-LLM

To start with, I had to load the dataset "zqz979/meta-review" into the jupyter notebook of Google Colab. Google Colab is a brilliant environment that provides a dedicated amount of TPU and GPU for running the code without any cost. The dataset itself consisted of train, test, and validation parts. After much research and a continuous process of trial and error, I decided to use two pre-trained models for training and tokenization - 'google-t5/t5-small' and 'gemma' but gemma also provided some errors. I could not use large models that were trained on billions of parameters due to resource limitations.

The dataset was preprocessed by truncating the spaces and converting them to lowercase. Then I tokenized the text but the main problem I faced here was it was throwing an error on empty rows so decided to drop off them as they were very small in number as compared to the whole dataset. 

Used the rouge method for evaluating the model and AutoModelForSeq2SeqLM specifically for the summarization task. I had to connect to Hugging Face which provides an interactive interface for smooth connection to deploy our model to be public for everyone. Did some hyperparameter tuning but it was limited due to fewer resources at hand. 

Then we can use that model for getting our desired output summary through prompt engineering and pipelines. But you need to present your data very wisely and in a clear format which requires great preprocessing techniques. I was provided with the JSONL file which contained a list of strings and in that were 'metareviews' for which we needed the summaries. So extracting the metareviews in a separate pandas dataframe after analyzing all the data was an interesting task for me. 

Finally iterated through the dataframe and got the output in the summary array for each of the metareview. 

Sample metareview: 'This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non-Gaussian prediction maps are obtained using copulas. \n\nTechnically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.'

Sample summary output: 'This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks.'

Credits: Google Colab, Hugging Face, CHATGPT, Google, StackOverflow, etc.
