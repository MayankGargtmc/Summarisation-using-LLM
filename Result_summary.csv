Metareview,Summary
"This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. Results are presented on synthetic data, an electroencephalogram dataset and on a climate modeling problem. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks. Non-Gaussian prediction maps are obtained using copulas. 

Technically speaking, the reviewers found the approach to be incremental and only marginally significant and I agree with them. Issues such as estimates of computational cost, using fixed lengthscales for the covariances and relationships/using normalizing flows have been addressed by the authors satisfactorily. Empirically, the contribution of the paper is somewhat significant, as it provides similar flexibility to other more computationally expensive processes and more general assumptions than conditional neural processes.","This paper proposes a class of neural processes that lifts the limitations of conditional neural processes (CNPs) and produces dependent/correlated outputs but that, as CNPs, is inherently scalable and it is easy to train via maximum likelihood. The proposed model is extended to multi-output regression and to capture non-Gaussian output distributions. The paper parameterizes the prediction map as a Gaussian, where the mean and covariance are determined using neural networks."
"Exploration can happen at various levels of granularity and at different times during an episode,  and this work performs a study of the problem of exploration (when to explore/when to switch between exploring and exploitation, at what time-scale to do so, and what signals would be good triggers to switch). The study is performed on atari games.

Strenghts:
------------
The study is well motivated and the manuscript is overall well written
Studies a new problem area, and proposes an initial novel method for this problem
extensive study on atari problems

Weaknesses
--------------
some clarity issues as pointed out by the reviewers
no illustrative task is given to give a more intuitive exposition of the ""when to explore"" problem
comparison to some extra baselines like GoExplore would have been insightful

Rebuttal:
----------
Most clarity issues have been addressed satisfactorily. It has been explained why some requests for extra baselines would be challenging/or not relevant enough. While the authors agree that GoExplore would be an interesting baseline, they seem to have not added it. An illustrative task was not provided.

Summary:
------------
All reviewers agree that this manuscript opens up and tackles a novel direction in exploration, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While I agree with the reviewers that point out that this paper could have been made stronger by adding an illustrative task and additional baselines like GoExplore, there is a general consensus that the provided empirical study on this novel problem setting is a good contribution in itself. Because of this I recommend accept.","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that this paper opens up and tackles rebuttal, and provides an extensive empirical study on atari games (a standard benchmark for such problem settings). While the reviewers agreed that the paper could have been made stronger by adding an illustrative task and additional baselines."
"This paper proposes an unsupervised graph learning method [Iterative Graph Self-Distillation (IGSD)] by iteratively performing self-distillation to contrast graph pairs under different augmented views. This idea is then extended to semi-supervised setting where via a supervised contrastive loss and self-training. The method is empirically evaluated on some semi-supervised graph classification and molecular property prediction tasks, and has achieved promising results.

Reviewers agree that the method is interesting and the paper is well-written. The biggest concern from reviewers related to experimental evaluations of the method. The authors responded to this and included additional experiments. Although the reviewers appreciate the provided results and explanations, at the end they were not convinced about the empirical assessments. In particular, R1's post rebuttal comment indicates concerns about the reported performance of GCKN, which is different from the published one in Table 1 of GCKN paper. I encourage authors to improve on these experimental discrepancies and resubmit. ","This paper proposes an unsupervised graph learning method (IGSD) by iteratively performing self-distillation to contrast graph pairs under different augmented views. The method is empirically evaluated on some semi-supervised graph classification and molecular property prediction tasks, and has achieved promising results. Reviewers agree that the method is interesting and the paper is well-written."
"This paper suggests that adversarial vulnerability scales with the dimension of the input of neural networks, and support this hypothesis theoretically and experimentally. 

The work is well-written, and all of the reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. (The AC did not consider the criticisms raised by Reviewer 3 justified. The norm-bound perturbations considered here are a sufficiently interesting unsolved problem in the community and a clear prerequisite to solving the broader network robustness problem.) 

However, many of the reviewers also agreed that the theoretical assumptions - and, in particular, the random initialization of the weights - greatly oversimplify the problem. Reviewers point out that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results, and also does not properly address the issue of gradient masking. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is well-written, and all reviewers appreciated the easy-to-read and clear nature of the theoretical results, including the assumptions and limitations. The reviewers also agreed that the lack of data dependence and only considering the norm of the gradient considerably limit the significance of the corresponding theoretical results."
"The paper shows that a form of Fictitious Self-Play converges to the Nash equilibria in Markov games. Understanding the theoretical properties of Fictitious Self-Play is important, however the paper in its current form is not ready for publication. The paper needs a more thorough discussion on related works, the assumptions made, and as pointed out by Reviewer3, the convergence argument needs to be expanded and explained in more detail. Further, I encourage authors to add experiments and compare their algorithm with other methods. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper needs a more thorough discussion on related works, the assumptions made, and the argument needs to be expanded and explained in more detail."
"This paper proposes a relatively complicated method for source-free unsupervised domain adaptation, which integrates several techniques into a divide and contrast framework. The idea of dividing the target data into source-like subset and target-specific subset and employing global alignment and feature consistency for each subset is novel when the source data is inaccessible. The contrastive learning and memory-based MMD are novel in the context of source-free domain adaptation and introduce theoretical benefits in terms of the expansion theory and domain alignment theory, respectively. Reviewers were on the positive side while holding some concerns on the marginal improvement over the SoTA methods, which were addressed in the author rebuttal. AC generally agreed that the paper has introduced a novel and solid contribution to the field, with a nice connection between algorithmic methods and theoretical insights, and  recommended the paper for acceptance. Authors are suggested to incorporate all rebuttal material in the revision and if possible, to work out a recipe for easing the adoption of their relatively complicated framework that comes with many modules and loss terms.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were on the positive side while holding some concerns about the marginal improvement over the SoTA methods, which were addressed in the author rebuttal."
"This paper explores the use of partial rejection control (PRC) for improved SMC-based variational bounds. While an unbiased SMC variant with PRC has been previously introduced by Kudlicka et al. (2020), this work introduces innovations that can help apply such ideas to variational inference. These bounds result in improvements in empirical performance. 

This paper was heavily discussed, with significant engagement by both the authors and the reviewers. Most reviewers recommended acceptance of this paper, with one reviewer (R4) recommending against acceptance. R4's central concerns regard the novelty of the proposed approach and its positioning relative to the existing SMC literature. The authors argued vigorously in the comments that this paper should be judged as a contribution to the VI literature and not the SMC literature.  Unfortunately, I will recommend that this paper is rejected. It is my opinion that R4's concerns were not fully addressed.

On the one hand, I agree with the authors that there is significant value to be had in exploring variants of SMC for VI. Indeed, some prior art, like FIVO and IWAE, contributed little to the Monte Carlo literature. I believe that these were good contributions.

On the other hand, I am concerned that the current draft does not clearly circumscribe its contributions. I read the sections that disuss the works of Schmon et al. (2019) and Kudlicka et al. (2020), and the writing did not leave me with a clear enough sense of the differences. I also read the abstract and introduction of the paper. The introduction of the paper positions this work clearly within the VI literature, but does not clearly discuss prior SMC art, e.g., it does not cite Kudlicka et al. (2020). Despite citing rejection control for SMC, the writing of the abstract and introduction left me with the impression that this work was the first to introduce *unbiased, partial* rejection control for SMC. I believe that impressions matter and that the machine learning community should be generous to adjacent communities when assigning credit.

I realize that my decision is a matter of taste. I also want to say that I am confident that the authors have a clear sense of where their contribution sits, and I suspect that it is a valuable contribution. However, I cannot recommend the draft in its current form. If this is a contribution to the VI literature, as the authors argue, then the authors should not hesitate to give full credit to prior SMC art. My reading of the current draft still leaves me confused about which aspects of the SMC estimator are actual contributions.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. However, the reviewers argued strongly that this paper should be considered as a contribution to the VI literature and not the SMC literature. I agree with the authors that the current draft does not clearly circumscribe its contributions."
"This paper addresses a meta-learning method which involves bilevel optimization. It is claimed that two limitations (myopia of MG and restricted consideration of geometry of search space) that most of existing methods have can be resolved by the MBG with a properly chosen pseudo-metric. The algorithm first bootstraps a target from the meta- learner, then optimizes the meta-learner by minimizing the distance to that target under a chosen pseudo-metric. The authors also establish conditions that guarantee performance improvements and show that metric can be sued to control meta-optimization. All the reviewers agree that the idea is interesting and experiments well support it. Authors did a good job in the rebuttal phase, resolving most of concerns raised by reviewers, leading that two of reviewers raised their score. While the current theoretical results are limited to a simple case where L=1$, the method is attractive for meta-learning community. All reviewers agree to champion this paper. Congratulations on a nice work.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that the paper is interesting and experiments well support it."
"Dear Authors,
thank you very much for your submission. We are happy to inform you that
we have decided to accept it and we look forward to your talk in the workshop.
Please, go over the feedback in the reviews and correct or update your papers
in time for the camera ready date (May 24). In particular, please address the
comments raised by both reviewers regarding clarity, discussion of related work,
and additional experimental results, by making use of the additional space (9 pages) 
allowed by HSDIP.
Best regards
HSDIP organizers","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This work presents a novel environment-free invariant learning method that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. The method is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations, and the label predictions, produced by a multi-headed neural network. The proposed model is compared to different alternative models from the literature of the field, in different challenging benchmarks, and the results show that it closely achieves the best possible invariant learning performance.

After some initial discussions, all reviewers agreed that this work is ready for publication, as the work addresses an important problem, presents good empirical results, and will be of significant interest to the community. 
","This paper presents a novel neural network training approach that uses an auxiliary network to learn environment-specific features, from which environment inferences can be derived. The method is composed of two jointly learned models, that take care of the environment identification, the learning of the invariant representations, and the label predictions, produced by a multi-headed neural network. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
All the reviewers appreciate the value of the proposed benchmarks. The remaining concerns seem addressable. I recommend accepting the paper while asking the authors to incorporate the review feedback into the camera-ready paper.,"Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"I thank the authors for their submission and active participation in the discussions. This papers is borderline. On the positive side, reviewers emphasized this is a well written [ovqB,1zPe] and sound paper [BUDa] with good theoretical [td5N,ovqB,1zPe] and empirical [BUDa,td5N,ovqB] results. On the negative side, reviewers remarked clarity [KyZj,AVki], incremental with respect to Tasse et al (2020) [KyZj], relatively restricted Boolean task algebra [td5N], toyish nature of the environments considered [ovqB], and some missing details [1zPe]. During discussion, the sentiment seems to be somewhat lukewarm with none of the reviewers strongly favoring acceptance or rejection. It seems the main remaining concern is around the toyish nature of the environments used in this paper. I acknowledge that and I believe the authors could include experiments on more complex environments. However, I also give the authors credit for addressing most of the reviewer's concerns during rebuttal and for presenting a solid empirical and theoretical result that the research community can build upon in the future. I am therefore recommending acceptance of this paper and highly encourage the authors to further improve their paper based on the reviewer feedback.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers emphasized that this is a well written [ovqB,1zPe] and sound paper [BUDa] with good theoretical [td5N,ovQB,1,1ZPe], relatively restricted Boolean task algebra [e.g.,"
"This paper explores the performance of Q-learning in the presence of either one-sided feedback or full feedback. Such feedbacks play an important role in improving the resulting regret bounds, which are (almost) not affected by the dimension of the state and action space. The motivation of such feedback settings stems from problems like inventory control. However, the assumptions underlying the theory herein are often quite strong, which might limit the applicability of the theory. The dependency on the length per episode H can also be improved.   ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This work provides evaluations on serveral backdoor attacks and defenses on NLP data. The evaluation can be further improved by discussing related defenses, maintaining high quality and clear documentation, and discussing the stealthiness of the attacks.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper studies using low-degree polynomials for analyzing statistical/computational gaps for high-dimensional inference problems and identify average-case settings that exhibit this gap.  This is a nice paper and above the bar, though it perhaps appeal to only a theoretical audience.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The reviewers highly appreciated the replies and the additional experiments. We also had a private discussion on the paper. To summarize: the replies alleviated quite a few concerns, however the consensus was that the paper still does not meet the bar for a highly competitive conference like ICLR.

The idea of combining MPC (on a 'wrong' model)  with a learned cost function is very interesting and a promising direction. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. On the downside the reviewers are still not entirely convinced about the contribution and believe that the paper requires a significant re-write to incorporate the discussed points as well as an additional round of reviews."
"The paper surveys existing differentially private data synthesis
methods, and introduces an algorithm that learns both a generator and
a classifier in a differentially private mode.

The problem is highly timely and important. Results are promising.

Main remaining concerns after discussion between the reviewers and the
authors are:

- reason why the proposed scheme can give better classification
accuracy, should be clarified more

- unclarity on conclusions that can be drawn from the experiments. The
revised version has improved on this somewhat.

One explanation for the problems was suggested to be that the paper
tries, at the same time, to both present a new method and be a
survey. Is hard to do in a short paper, and as a result, the paper
lacks focus. At the very least, more work is needed.

The authors are encouraged to continue their work on this
important problem, and the review comments hopefully help in that.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are encouraged to continue their work on this important problem, and the review comments hopefully help in that."
"This paper has been reviewed by four experts. Their independent evaluations were consistent, all recommended rejection. I agree with that assessment as this paper is not ready for publication at ICLR in its current form. The reviewers have provided the authors with ample constructive feedback and the authors have been encouraged to consider this feedback if they choose to continue the work on this topic.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"This work provides an analysis explaining why FedAvg can produce more generalizable representations than distributed SGD. Theoretical guarantees are presented for a multi-task linear regression setting and further empirical results demonstrate the effectiveness of learning representations with image classification tasks. The theoretical analysis presented can be an important building block for the study of more complex settings in federated optimization. All reviewers recommend acceptance.

Please take the (few) suggestions by the reviewer into account, and also incorporate the explanations and clarifications provided during the rebuttal in the camera ready version.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers recommend acceptance. Please take the reviewers' suggestions into account, and incorporate the explanations and clarifications provided during the rebuttal in the camera ready version."
The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance. ,Your task is to generate concise summaries for each metareview following the instructions above. The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization. The experiments are convincing experiments. The reviews are convincing and the reviewers encourage the authors to incorporate additional references suggested in the reviews.
"This paper presents a method for unsupervised learning of disentangled representations by first training a VAE with a tangled set of latents, and then sequentially learning disentangled latent variables one at a time from the entangled initial VAE latent space. On several toy disentanglement benchmarks, the method is shown to perform competitively with previous VAE and GAN approaches. 

There were several concerns from reviewers around the clarity and description of the proposed one-factor-at a time (OAT) training procedure. While the updated draft addressed several typos and some clarity issues, multiple reviewers continued to find the method description problematic. There were additional concerns around the viability of the method on real-world datasets where the number of factors are not known, and as the authors stated the proposed method can also result in one factor of variation encoded into mulitple latent variables, which hurts on many of the disentanglement metrics.  The addition of CelebA downstream task evaluation begins to address this concern of real-world data, but more rigorous experiments (including more description of how models were selected) and discussion of the limtiations of the proposed method are needed. There is also no theoretical motivation as to why the proposed intervention-based factor learning algorithm should recover the ground truth factors.

Given the concerns over experimental results, clarity, and lack of theoretical motivation, I suggest rejecting this paper in the current form.","This paper presents a method for unsupervised learning of disentangled representations by first training a VAE with a tangled set of latents, then sequentially learning disentangle latent variables one at a time from the entangled initial VAE latent space. The method is shown to perform competitively with previous VAE and GAN approaches. The reviewers expressed concerns about the clarity and description of the proposed one-factor-at-a-time training procedure, and the lack of theoretical motivation."
"This paper presents a rich dataset aimed at creating supervision for 2D cartoon tasks. The dataset is created by procedurally converting open source 3D computer graphics movies/shorts into ""flattened"" 2D frames. Care is taken in describing both the motivation and limitations of this approach. In an area with a dearth of good data, this paper's contributions are warmly welcomed. Reviews appreciated these aspects of the paper while highlighting potential weaknesses that appear addressed in the revisions. I recommend accepting this paper to the NeurIPS 2022 Datasets and Benchmarks program.","This paper presents a rich dataset aimed at creating supervision for 2D cartoon tasks. The dataset is created by procedurally converting open source 3D computer graphics movies/shorts into ""flattened"" 2D frames. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers appreciated these aspects of the paper while highlighting potential weaknesses that appear addressed in the revisions."
"Thank the authors for their submission.

The paper studies combinatorial multi-armed bandit with probabilistically triggered arms. That is an MAB setting in which, at each round, the learner chooses a subset of the arms and obtains a reward that is some function of expected rewards of the chosen arms. In addition, the learner only observes feedback on a random subset of her chosen arms (triggered arms). 

The paper relaxes a smoothness assumption laid by a previous work, and further improves the dependence on K in the regret bound, where K is the batch size (maximum number of triggered arms)
The authors provide computationally-efficient algorithms that are based on Bernstein concentration inequality, facilitating the improved bounds. 
The paper is well-written and organized, and the theoretical results are sound.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors provide computationally-efficient algorithms that are based on Bernstein concentration inequality, facilitating the improved bounds."
"The authors propose a semi-supervised novelty detection method which tries to identify out-of-distribution samples in the unlabeled data (consisting of in- and out-distribution samples) using a disagreement score of an ensemble. The ensemble is generated by fine-tuning the trained classiifer on the labeled training data plus the unlabeled data which all get a fixed label (which is repeated several times to generate the ensemble). The main idea is that one uses early stopping based on an in-distribution validation set in order to avoid overfitting on the unlabeled points which allows then identification of the out-distribution points via the disagreement score.

The reviewers appreciated the simplicity of the approach and the extensive experimental results. The authors did a good job in trying to answer all questions and concerns of the reviewers. 

However, some concerns remained:
- the setting assumes that the OOD data is fixed which was considered as partially unrealistic and thus evaluation of the OOD detection performance on unseen OOD distributions was requested in order to understand the limitations of the method (this was only partially done by the authors). 
- the theoretical result is for a two-layer network and completely based on previous work. As the authors use much deeper networks later on in the experiments, this result cannot be used to theoretically justify the approach. 
- there remained concerns about the necessary diversity of the ensemble and the early stopping procedure

While I think that the paper has its merits, it is not yet ready for publication. I encourage the authors to to take into account the above points and other remaining concerns of the reviewers in a revised version.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers appreciated the simplicity of the approach and the extensive experimental results."
"The theory and results presented in this paper provide a new method to avoid collapse in contrastive learning.  All but one reviewer recommend acceptance.  The lone negative reviewer is concerned with the limited experiments, but the other reviewers, and the AC, find the experimentation convincing enough to warrant acceptance.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions. All but one reviewer is concerned with the limited experiments, but the other reviewers, and the AC, find the experimentation convincing enough to warrant acceptance."
"This paper empirically demonstrates the sensitivity of unsupervised OD methods to hyperparameters and proposes ensembles of models with differing hyperparameters along with training techniques based on weight sharing to do so efficiently. 

The authors provided additional experiments to answer some reviewer's major concerns regarding the (meta-)HP robustness of the ensemble methods. While there are natural fluctuations with respect to the (meta) hyperparameters, a larger number of hyperparameters included usually resulted in a close to optimal AUROC.

Another concern for two reviewers was the extendability of the efficient ensemble training techniques to other models such as GANs etc. As the authors replied, even though skip connections might not be adaptable to other techniques, e.g. for ensembling various widths, zero-masked layers with BatchEnsemble would also be broadly applicable to ensemble-learn other unsupervised representation learning models. Perhaps in the final version, the authors further discuss with a short experiment how the AE-specific scaling techniques add to the performance.

A further concern was the lack of theoretical underpinning about the (meta)HP-robustness. Given that the reviewers agree that this paper provides ample empirical evidence and praise the experimental value, we think theoretical work (providing HP sensitivity results is highly nontrivial in general, let alone for neural networks) can be part of future work but the lack of it in the current manuscript should not prevent the publication of this work.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that this paper provides ample empirical evidence and praises the experimental value of the paper. However, some concerns were raised regarding the generalization of the approach to different datasets."
"This paper studies the last iterate rate of convergence of the well-known extragradient and optimistic gradient algorithms in smooth monotone games with continuous convex action sets. The main result of the paper is to show that both algorithms (with constant step size) enjoy tight last-iterate convergence rates for setting (previous papers either 1) only applied to unconstrained domains 2) were asymptotic, or 3) required dependence on arbitrarily large problem-dependent constants).

This paper resolves a well-known open problem within the min-max optimization community, and is likely to have significant impact. The reviewers agree that the paper is well-written, and the the techniques (using the ""tangent residual"" as a potential function) are novel. For the final version, the authors are encouraged to incorporate the reviewers' suggestions to improve the presentation.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the paper is well-written, and the techniques (using the ""tangent residual"" as a potential function) are novel. For the final version, the authors are encouraged to incorporate the reviewers' suggestions."
"I thank the authors for their submission and active participation in the discussions. All reviewers are unanimously leaning towards acceptance of this paper. Reviewers in particular liked that the paper is presenting an interesting and systematic study of reward hacking [GVMn] that is useful to the research community [bfGN] and targets an important problem [uYeb] in a rigorous way [16uL]. I thus recommend accepting the paper, but I strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular in regards to improving positioning with respect to related work and a better formalization of their work.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"The paper provides novel guarantees for the well-studied distributed sparse regression problem. Their theoretical results improve upon the state of the art and extend to settings that many previous results could not handle. From a technical perspective, their result builds upon previous frameworks, but also requires a number new, novel ideas. The paper does have some downsides; as mentioned previously, some of their ideas do build quite strongly off of previous work, and the presentation of the paper is quite dense, as several reviewers noted. However, the consensus of the reviewers overall is that the technical contribution of the paper is above the bar for acceptance, and would be of interest to the distributed optimization community.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper provides novel guarantees for the well-studied distributed sparse regression problem. The theoretical results improve upon the state of the art and extend to settings that many previous results could not handle. The reviewers agree that the technical contribution of the paper is above the bar for acceptance."
"This paper studies the problem of producing distribution-free prediction sets using conformal prediction that are robust to test-time adversarial perturbations of the input data. The authors point out that these perturbations could be label and covariate dependent, and hence different from covariate-shift handled in Tibshirani et al 19, the label-shift handled in Podkopaev and Ramdas 21, and the f-divergence shifts of Cauchois et al 2021. 

The authors propose a relatively simple idea that has appeared in other literatures like optimization but appears to be new to the conformal literature: (i) use a smoothed (using Gaussian noise on X, and inverse Gaussian CDF) nonconformity score function, in order to control its Lipschitz constant, (ii) utilize a larger score cutoff than the standard 1-alpha quantile of calibration scores employed in conformal prediction. The observation that point (i) alone lends some robustness to adversarial perturbations of the data is interesting. As several experiments in the paper and responses to reviewers show, this comes at the (apparently necessary) price of larger prediction sets. 

I read through all the comments and also the supplement. The authors have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement as a result. Three reviewers are convinced, but one remaining reviewer requested additional experiments to compare with Cauchois et al (in addition to all the others already produced by the authors originally and in response to reviewers). However, the authors point out that the code in the aforementioned paper was not public, but they were able to privately get the code from the authors during the rebuttal period. At this point, I recommend acceptance of the paper even without those additional experiments, since it is not the authors' fault that the original code was not public. Nevertheless, I suggest to the authors that, if possible, they could add some comparisons to the camera-ready since they now have the code.

I congratulate the authors on a nice work, a very solid rebuttal, and also the astute reviewers on pointing out various aspects that could be improved. 

Minor point for the authors (for the camera-ready): I would like to comment on the Rebuttal point 4.4 in the supplement, which then got further discussed in the thread. The reviewer points out four references [R1-R4]. I will add one more to the list [R5] https://arxiv.org/pdf/1905.10634.pdf (Kivaranovic et al, appeared in 2019, published in 2020). I think the literature reviews in this area are starting to be messy, and all authors need to do a better job. Clearly, the original paper of Vovk et al already establishes various types of conditional validity (and calls it PAC-style guarantee), produces guarantees that others in this area produce, and it appears that much recreation of the wheel is occurring. For eg, [R2, R4] do not cite [R5], despite [R5] appearing earlier and being published earlier, and having PAC-style guarantees and experiments with neural nets, etc. However, in turn, [R5] do not cite Vovk [R1], but [R2, R4] do cite [R1]. (And [R3] does not seem to be relevant to this discussion of conditional validity?) In any case, I am not sure any of these papers need citing since the current paper does not deal with conditional validity. If at all, just one sentence like ""Conditional validity guarantees, of the styles suggested by Vovk [2012], would be an interesting avenue for future work"".","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns were raised about generalization. Overall, the paper received a positive recommendation with minor revisions. The reviewers have responded very well to all the reviewers questions/concerns, adding significant sections to their supplement, and adding some comparisons to the camera-ready since they now have the code."
"The authors present Neural Acoustic Fields (NAF), which render sounds for arbitrary emitter and listener positions in a scene. Overall, the reviewers are very positive (8-8-8-5). The authors addressed many of the reviewers' questions about previous related work and rendering spatial binaural audio. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors present Neural Acoustic Fields (NAF), which render sounds for arbitrary emitter and listener positions in a scene. The reviewers are very positive (8-8-8-5)."
"All reviewers agreed that analysis of PPO is interesting. 
During the discussion, however, there was an agreement that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. Meanwhile, for conventional policy gradient, recent works provided convergence rates.
As one reviewer pointed out - this work does not further our theoretical understanding on why PPO is better than vanilla policy gradient, as all the established results hold for policy gradient, even with less assumptions.
I encourage the authors to strengthen their paper by relaxing Assumption 4 (perhaps based on the robust classification idea raised in the discussion), and by further providing rate results.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the current work is too thin in novelty and contribution: it provides only convergence analysis under very strong assumptions, and heavily builds on techniques from prior works. The paper does not further our theoretical understanding on why PPO is better than vanilla policy gradient."
"This paper proposes a method for learning physics combining symbolic computation and learning in an interesting way, targeting sample efficiency. At the initial evaluation, it was on the fence but leaning towards acceptance, with 3 slightly positive and one slightly negative review. 

The strengths lie in the combination between symbolic reasoning and statistical ML with a formulation around the classical EM framework. On the other hand, an important issue of the paper is its quite simplistic evaluation on now very easy problems and benchmarks. While benchmarks tend to be simple in the field of learning physics, current work does address more difficult problems than the problems tackled in this paper.

Another issue discussed was the simple trade-off in injecting hand-crafted inductive bias into a system leading to increased sample efficiency, which was perceived as unsurprising by some reviewers. While this is common in ML, and even strongly more so in learning physics from data synthetically generated with known physical laws, it was perceived to be particularly unsurprising in this paper where the benchmarks are indeed very simple and the laws directly encoded.

The AC discussed this paper with the PCs, and it was judged that the weaknesses in evaluation, in particular the simplicity of the tasks, cannot compensate for the interesting hybrid symbolic/ML formulation, and decided to reject the paper.","This paper proposes a method for learning physics combining symbolic computation and learning in an interesting way, targeting sample efficiency. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were concerned about the generalization of the approach to different datasets, although some concerns were raised about generalization were noted."
"This paper proposes AUTOMATA, an approach that uses GradMatch to select subsets of data in order to accelerate hyperparameter tuning. The reviewers all found the approach to be practical and empirically effective. There were concerns about the robustness to different subset sizes, particularly across different datasets, but the authors demonstrated that AUTOMATA works well across a number of settings during the rebuttal period. The remaining criticism largely revolves around the novelty of the approach, but the majority of the reviewers believe that this is a useful application of gradient-based subset selection.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers all found the approach practical and empirically effective."
"The paper presents some interesting insights, but all reviewers have agreed that it does not meet the bar of ICLR. The theoretical results require revision as several issues have been indicated in the reviews. The authors have tried to correct them during the rebuttal, but the reviewers remain unconvinced.  Also the novelty is limited as re-ranking is a well-known concept and decoupling of head and tail labels is an approach often used in practice across many applications.

The authors should also clarify the way the RankNet method is used and implemented to clarify the issue raised by Reviewer 1. Finally, let me notice that adjusting thresholds for labels has been considered in the XMLC literature, in the context of optimization of the macro F-measure (Extreme F-measure Maximization using Sparse Probability Estimates, ICML 2016).

","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The theoretical results require revision as several issues have been indicated in the reviews. The reviewers have tried to correct them during the rebuttal, but the reviewers remain unconvinced."
"The paper introduces a procedure to control the churn (i.e. differences in the predictive model due o retraining) using distillation.

This is a strong paper, with novel technique which is clearly presented, and is backed by sound theory. The experimental results were also deemed extremely convincing by reviewers TJ4g and pZBb. 

Reviewer nqfu raised a question about the similarities between churn reduction and domain adaptation. The authors have addressed this by pointing out similarities to their work but also noting that, in the settings mentioned by the reviewer, alternative approaches such as completely retraining the model might be more appropriate. This part of the rebuttal is convincing.

Reviewer TJ4g has pointed out several points of improvement, to which the authors have responded adequately.

All in all, this paper is ready for and deserving of acceptance.","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have addressed this by pointing out similarities to their work but also noted that alternative approaches such as completely retraining the model might be more appropriate."
"This paper extends an earlier work with scalar output to vector output. It establish a relationship of two-layer ReLu network and convex program. The result can be used to design training algorithms for ReLu networks with provably computational complexity. Overall, this is an interesting idea, leading to better theoretical insights to computational issues of two-layer ReLu networks. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a framework based on principle components analysis (PCA) to speed up the missing data imputation. It divides the feature sets into two partitions -- the fully observed one and the one that contains missing values. The proposed method applies PCA to the fully observed partition to do dimensionality reduction, followed by the existing imputation methods. The authors further propose to apply PCA to the imputed data to speed up the downstream classification task.

The major weakness is that the methodological contribution is quite limited. Projecting data into lower dimensional spaces to speed up downstream tasks is not new. In particular, the main assumption of random missingness has been considered before 10-20 years ago and the more challenging setting of non-random missingness was not considered. Overall the reviewers mostly agree that the contribution is limited.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The major weakness is that the methodological contribution is quite limited. Projecting data into lower dimensional spaces to speed up downstream tasks is not new."
"The paper describes a self-supervised learning method based on an information maximization criterion that naturally prevents dimensional collapse. The authors consider the Shannon mutual information under the assumption that the data is Gaussian. A first-order approximation to the log-determinant of the sum of two matrices is used to simplify the final objective. Experiments on 4 image datasets show that the proposed approach gives better results than contrastive and non-contrastive methods.

Strengths:

1 - The paper is well written and easy to follow.
2 - The paper is theoretically grounded on correlative information measure of representation.
3 - Strong results on some downstream classification problems.
4 - Initially the experiments included only one downstream task regarding classification, but the paper has been updated to include also results for object segmentation and detection task.
5 - Novel and well motivated.
6 - state-of-the-art SSL performance.

Weaknesses:

- Some weaknesses are pointed out by reviewer GZwK, but these are not well justified.

Decision:

A majority of reviewers vote for acceptance. The only reviewer voting slightly towards rejection is GZwK, with a reasoning that is not well justified. For example, the main criticisms mentioned by reviewer GZwK

- The paper directly generalizes the earlier proposed log-determinant mutual information to the field of self-supervised learning. 
- this paper does not give a deep-going analysis that why the second-order statistics can play a important role in self-supervised learning

are not mentioned by any of the other reviewers.

Because of this, I have decided to accept the paper.

","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is well written and easy to follow. The main criticisms are pointed out by reviewer GZwK, with a reasoning that is not well justified."
"This paper proposes environment fields, a representation that models reaching distances within a scene. Dense environment fields are learnt using a neural network, and the effectiveness of this representation is shown on 2D maze environments and 3D indoor environments. This paper received hugely contrasting reviews, with two reviewers being very supportive and one reviewer providing the lowest score of 1. In light of this, I'll start with providing my takeaways on the review and discussion with reviewer z3Y4 (rating of 1) and then proceed to the remaining discussion.

Reviewer z3Y4 has provided the score of 1 and has made strong remarks that include: ""what is proposed in this paper is simply not comprehensible"", ""description of the method itself is simply devoid of all required detail"", ""The main claims of the paper are incorrect or not at all supported by theory or empirical results."" and "" what is being proposed in this paper is simply too unclear and vague to be assessed"". **Such dismissive remarks, in my opinion, are completely unnecessary and create a toxic discussion and review environment.**

Reviewer z3Y4 has many criticisms of the submission, but the primary ones include: (a) the lack of details throughout the paper (b) the positioning of the paper in the abstract and introduction, and (c) the lack of experiments in continuous environments. Re (a): It is well understood in our research community that providing every last detail in the main submission is nearly impossible due to the restriction on the number of pages. Providing excess details in the main paper also often reduces the readability of the paper. Such details are better addressed in the appendix and crucially, the code. The authors have provided some details in the appendix and have indicated that they will release a code base.  I also agree with the authors that justifying every last detail in the network architecture such as choice of an activation function is not necessary for this submission. The same goes with describing methods in past works in detail vs referring the reader to the appropriate citation. As a result, I believe that the authors have addressed (a) well. Re (b): This has also been addressed by the authors, by pointing out relevant parts of the paper that had the necessary details. Re (c): In this regard, the paper clearly contains a well laid out experiment in 3D indoor scenes, so as far as I am concerned, this has been addressed in the main submission.

Reviewers AhgQ and fAEP have supported this submission but also laid out some concerns that include:
(1) Are the gradients suboptimal ?
(2) Positioning the paper with regards to past works
(3) Motivation behind using the VAE
(4) Qualitative analysis and failures
The authors have addressed these 4 concerns well using the rebuttal as well as via a revision of the appendix. The reviewers, post discussion have indicated their satisfaction with the revised submission.

I think this paper is interesting and proposes a novel scene representation which can be useful for others in the Embodied AI community. I am in agreement with reviewers AhgQ and fAEP, and in spite of the strong reject score by z3Y4, I recommend accepting this paper.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are very supportive and one reviewer provides the lowest score of 1 and has made strong remarks that include: ""What is proposed in this paper is simply not comprehensible"", ""description of the method itself is simply devoid of all required detail"" and ""the main claims of the reviewers have a rebuttal."
"The paper proposes a supervised adversarial method for disentangling the latent space of a VAE into two groups: latents z which are independent of the given attribute y, and \hat{y} which contains information about y. Since the encoder also predicts \hat{y} it can be used for classification and the paper shows competitive results on this task, apart from the attribute manipulation task. Reviewers had raised points about model complexity and connections to prior works which the authors have addressed and the paper is on the borderline based on the scores. 

Though none of the reviewers explicitly pointed out the similarity of the paper with Fader networks (Lample et al., 2017), the adversarial setup for getting attribute invariant 'z' is exactly same as in Fader networks, as also pointed out in an anonymous comment. The only difference is that encoder in the current paper also predicts the attribute itself (\hat{y}), which is not the case in Fader n/w, and hence the encoder can be used as a classifier as well (authors have also mentioned and discussed this difference in their response). However, the core idea of the paper as outlined in the title of the paper, ie, using adversarial loss for information factorization, is very similar to this earlier work, which diminishes the originality of the work. 

With the borderline review scores, the paper can go in either of the half-spaces (accept/reject) but I am hesitant to recommend an ""accept"" due to limited originality of the approach. However, if there is space in the program, the paper can be accepted. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers had raised concerns about the generalization of the approach to different datasets."
"The authors present a GAN for learning a continuous representation of disease-related image patterns from regional volume information generated from structural MRI images.
The reviewers find the problem relevant and appreciate the proposed solution. They find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community.
 
The overall objective function includes several hyper-parameters. As pointed out as the main weak point by multiple reviewers this may hint at overengineering/overfitting to a data set. However, the reviewers also mention that the regularizers are all sufficiently well-motivated in the paper and the author response.

Reviewers highlight comparisons on the real data as a strong result demonstrating that Surreal GAN was able to isolate two major sources/locations of atrophy in Alzheimer’s disease. Overall, the reviews are positive in majority.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers find the paper well-written and find the empirical results on Alzheimer brain MRIs relevant for the neuroscience community."
"This paper introduces a physical evaluation dataset framework for scientific computing pipelines that map one high-dim state space into another high- or low-dim one, providing a suite of simple representative physics problems.  Reviewers appreciated for motivation, clarity, comprehensiveness, and overall contribution to the space.","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper presents a large corpus of Korean legal documents, paired with labels corresponding to two classification tasks, two legal judgment prediction tasks, and one summarization task. Reviewers praised the uniqueness of the new resource.

However, there is some criticism of the results section, with several reviewers bringing up possible comparisons to other models and wondering whether the performance of this model has been analyzed thoroughly.  The authors argue in particular that mT5 and KoGPT-2 can benefit from continued pre-training this corpus, which seems satisfactory, although there is the question raised by 7fpy that this is essentially pre-training on the test tasks and may be too generous when assessing the performance on other legal-domain tasks that arise down the road.  Reviewer 7fpy brings up some valid critiques of the dataset construction: several points about filtering were unclear, as was the strength of the automatic pipeline used to construct the dataset.

There are a few ethical concerns raised by the ethics reviewer, which I believe are serious. Point [1] is resolved. Point [2] about misuse seems valid and harder to argue against.  The data does have value for use in the kind of more benign studies the authors report in the response there. But its very existence could encourage the construction of certain kinds of automated tools for legal judgment prediction, which has been a hotly-debated issue in the NLP community before. Ultimately I will defer to the ethics reviewer on this one, who seems satisfied.

Overall this paper seems like a well-done effort. The main question is whether its utility (modulated by issues with the results) outweighs the risks of putting it out there. Reviewers seem to lean positive on this and I would lean positive as well.","This paper presents a large corpus of Korean legal documents, paired with labels corresponding to two classification tasks, two legal judgment prediction tasks, and one summarization task. Reviewers praised the uniqueness of the new resource, although there are some criticisms of the results section, with several reviewers bringing up possible comparisons to other models and wondering whether the performance of this model has been analyzed thoroughly. Overall, the reviewers seem to lean positive on this and I would lean positively on this one. The reviewers are"
"This paper introduces the first NAS benchmarks on graphs. Ratings were quite diverse, with scores of 4,5,6,7,7,8.
The many positive reviewers highlighted that GraphNAS is very relevant and that a benchmark for them would be very useful for the community. The benchmark includes comprehensive evaluations on as many as 9 different datasets, which may also help facilitate research on meta-learning for (Graph)NAS. Initially, the code for creating the benchmark and analysis was not available, which is a no-go for tabular NAS benchmarks, but the authors fixed this in the rebuttal period.
The most negative reviewer, 8asK, appears to not be familiar with tabular NAS benchmarks and their usefulness, and did not react to the discussion about it, as well as to the modification of the paper to highlight it. The other borderline negative review, by reviewer LkTe, had as their main ciriticism that NAS methods should be benchmarked on many spaces, but the point of tabular NAS benchmarks is *not* to provide a conclusive assessment of the performance of NAS methods that holds in general across search spaces (as a ""horserace"" paper might try to achieve), but rather to facilitate the cheap evaluation of current and future NAS methods on a single search space. For these reasons, I do agree more with the positive reviewers and recommend acceptance of this work as a poster.

","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted that GraphNAS is very relevant and that a benchmark for them would be very useful for the community. The authors also noted that tabular NAS benchmarks should be benchmarked on as many as 9 different datasets, which may also help facilitate research on meta-learning."
"This paper proposes an architecture of a policy network (WaveCorr) that is particularly effective for portfolio management tasks.  A key observation that leads to the design of WaveCorr is that the dependency across asset should be treated differently from the dependency across time.  The proposed WaveCorr has the property that it is ""permutation invariant"" with respect to assets, which means that the class of functions that can be represented by WaveCorr is invariant to permutation of assets.  WaveCorr is shown to achieve the state-of-the-art performance in a portfolio management task.

A major point of discussion was the definition of ""permutation invariance"".  The reviewers and AC understood the difference between the permutation invariance defined in this paper and that studied in the prior work (the output of a network is insensitive to the permutation of the particular values of the input).  With the definition in this paper, however, a fully connected layer is permutation invariant, but the Corr layer proposed in the paper appears to have more structure.  It is unclear exactly what properties of the Corr layer leads to the performance improvement.","This paper proposes an architecture of a policy network (WaveCorr) that is particularly effective for portfolio management tasks. The proposed WaveCorr is shown to achieve the state-of-the-art performance in a portfolio management task. The reviewers and AC understood the difference between the definition of ""permutation invariance"" defined in this paper and that studied in the prior work (the output is insensitive to the permutation of the particular values of the input). However, the Corr layer proposed in the paper appears to have significant improvements in the field of neural network training."
The paper proposes a new technique to handle oversquashing in GNNs by introducing a novel rewiring technique. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the method and it's impact.,"The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are quite positive about the paper and the rebuttal phase greatly helped clarify the paper's impact."
"Reviewers agree that this paper presents a systematic study on the impact of hyper-parameters and training strategies of previous works. Based on those empirical observations, they propose a simplified model with layer reduction and single-stage distillation, which do not rely on a complicated and ad-hoc training strategy. Extensive experiments are conducted with thorough comparison with existing works. Authors also clearly point-out their current limitations.

The major concern is that this paper is more focused on discussion of the effectiveness of training strategies in previous methods, while the theoretical contribution is somehow limited. It would be much better if authors could explain their observations (more training epochs is needed while additional distillation stages can be discarded) from a theoretical perspective, although this may be far out of the scope of this work. Nonetheless, this paper presents valuable empirical study over existing Transformer compression methods and may inspire following research; therefore, AC recommends acceptance.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The paper provides additional empirical evidence that self-supervised learning methods can help disentangling factors of variation in a dataset. That said, the paper can benefit from better framing and perhaps comparison with existing work (e.g., https://arxiv.org/abs/2102.08850 and https://arxiv.org/abs/2007.00810). Furthermore, the authors acknowledge that there was a bug in their code, which I believe should at least lead to softening the claims about group disentanglement. Accordingly, please consider revising the paper and re-submitting to other venues.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors acknowledge that there was a bug in their code, which I believe should at least lead to softening the claims about group disentanglement."
"This paper proposes a methodology to create cheap NAS surrogate benchmarks for arbitrary search spaces. Certainly, the work is interesting and useful, with comprehensive studies to validate such approach. It should be credited as belonging to the first efforts of introducing and comprehensively studying the concept of surrogate NAS benchmarks. In AC's opinion, it is a solid paper that will (or has already) inspire many follow up works. The paper is well written. 

This paper received highly mixed ratings. Although the authors might not see, all reviewers actually participated in the private discussions. Reviewer 1eb8 indicated hesitation in her/his support. Reviewer yTPb stated that if not considering the arXiv complicacy, she/he ""would certainly raise score by one level"".  AC also reached out to Reviewer yTPb about her/his mentioned possibility of updating scores, and got confirmed that her/his original opinions wasn't changing after rebuttals. Besides, AC agrees the arXiv/NeurIPS complicacy shouldn't brought into the current discussion, and ignored that factor during decision making. 

The main sticking (and considered-as-valid) critique is on the relatively outdated and incomplete selection of baselines. As a benchmark paper, it should capture and diversify the recent methods. For example, the authors might consider adding: https://botorch.org/docs/papers (latest methods in Bayesian Optimization) https://github.com/facebookresearch/LaMCTS (latest methods in Monte Carlo Tree Search) https://facebookresearch.github.io/nevergrad/ (latest methods in Evolutionary algorithms)

Given the above concerns, AC considers this paper to sit on the borderline, and perhaps with pros outweighing the cons. Hence, a weak accept decision is recommended at this moment.","This paper proposes a method to create cheap NAS surrogate benchmarks for arbitrary search spaces. The reviewers found the experimental results compelling, although some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The main sticking (and considered-as-valid) critique is on the relatively outdated and incomplete selection of baselines. Moreover, the reviewers agreed that the arXiv/NeurIPS complicacy shouldn't bring into the current discussion, and ignored that factor."
"This paper proposes measures of consistency between back-doored and clean models, proposes regularization using those consistency measures, and showcases that such trained models indeed exhibit better consistency. Also, it is demonstrated that the fine-tuned model does not deviate too far from the original clean model. The reviewers' comments are all well addressed. Some concerns related to the notion of consistency and how it relates to the detection of backdoors are still left open, but the reviewers seem to be satisfied with the answers. Given the overwhelmingly positive reviews, I propose accept.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers' comments are all well addressed and the reviewers seem to be satisfied with the answers."
"The paper introduces the maximum n-times coverage, a new NP-hard (and non-submodular) optimization problem. It is shown that the problem can naturally arise in ML-based vaccine design, and two heuristics are given to solve the problem. The results are used to produce a pan-strain COVID vaccine. 

The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the author response period. Given this, I am delighted to recommend acceptance. Please incorporate the feedback in the reviews in the final version of the paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers and I think that this is an interesting paper with a compelling application. There were some concerns about theoretical novelty and biological accuracy but these were addressed during the review period."
"The authors propose to use large batch training of neural networks, where each batch contains multiple augmentations of each sample. The experiments demonstrate that this leads to better performance compared to training with small batches. However, as noted by Reviewers 2 and 3, the experiments do not convincingly show where the improvement comes from. Considering that the described technique is very simplistic, having an extensive ablation study and comparison to the strong baselines is essential. The rebuttal didn’t address the reviewers' concerns, and they argue for rejection.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were concerned about the generalization of the approach to different datasets, although some concerns were raised about generalization were noted."
The paper proposes latency-aware spatial-wise dynamic neural networks under the guidance of a latency prediction mode. reviewers arrived at a consensus to accept the paper. ,"The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"While two our of three reviewers pointed out that the method is very similar to a previously published approach, DODNet,  these reviewers still see value in the extensive evaluation presented in this paper, and both suggested weak accept. The first reviewer also increased the score to borderline after the rebuttal owing to additional experiments for evaluation. I also agree with the reviewers that this paper addresses an important problem in the field, i.e.  partially labelled data, and presents extensive evaluation and benchmarking against important baseline approaches, and therefore suggest acceptance of this paper.
","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"All Reviewers agree that the paper has a clear and solid contribution. Furthermore, all of them highlight that the paper has improved significantly after revision. Hence, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.

Pros:
- Comparison across network architectures.
- Comparison across a broad range of different data sets.
- Compactness of the representation (few parameters to learn).
- Authors will share code.

Cons:
- Role of L2 normalization could be further discussed.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"While there is no unanimity, the majority is positive and sees the potential value of the approach for machine learning. It is good to mention that the authors did a good job in replying to comments and criticisms, which has clarified a few misunderstandings. Personally I would recommend the authors not to use the confusing terminology between Bayesian and frequentist approaches that are employed in the paper, because it has generated more harm than benefit (at least among those in the discussion here). The theoretical results seem sound and useful and the contribution is good (even if the targeted problem might be seen as too specific by some).","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors did a good job in replying to comments and criticisms, which has clarified some misunderstandings."
"This paper builds upon existing works to prove that learning (correlated) equilibrium can be fast, i.e., faster than \sqrt{n} even in extensive form games.

Three reviewers are rather lukewarm, and one reviewer is more positive (but seems less confident in his score). The two major criticisms is that this paper is very difficult to read and that the results might seem rather incremental with respect to the literature.

I tend to agree with both points but the paper still as merits: the reason is that extensive form games are intrinsically way harder than normal form games and they more or less all have a burden of notations. We agreed  that the authors actually did some efforts to make it fit within the page limit. but another a conference or a journal would have been better suited than ICLR.

Our final conclusion is that the result is interesting yet maybe not breathtaking for the ICLR community; we are fairly certain that another venue for this paper will be more appropriate and that it will be accepted in the near future (I can only suggest journals based on the large amount of content and notations, such as OR, MOR, or GEB - yet, conferences such as EC should be more scoped too) . It does not, unfortunately, reach the ICLR bar.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are rather lukewarm, and one reviewer is more positive (but seems less confident in his score)."
"An interesting contribution with relevant results. Some more exploration of hyper-parameter tuning could be a good contribution, and a rephrasing of certain ways of describing results (see reviewer 9G6p's comments). ","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"As both reviewers stated, this paper is a good fit for the workshop, with several positives. 
I am not repeating all of them here, since I agree with the reviewers' assessment comments.
From the a given a natural language instruction and an input and an output scene,
the paper investigates how to  train a neuro-symbolic model which  manipulates
a program that can be executed by a robot on the input scenes and generates a goal state.
The authors carry out experiments to demonstrate how the neuro-symbolic model
is end-to-end and show generalization to novel scenes and instructions. 
I recommend acceptance of this paper, as it can lead to relevant directions in neuro-symbolic robotics.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. I recommend acceptance of this paper, as it can lead to relevant directions in neuro-symbolic robotics."
"This paper provides two routines to replace gradient updates with low-rank unitary updates, and provides extensive technical discussion and experiments.  Reviewers are uniformly positive, and I also voice similar praises, e.g., I too appreciate the extensive discussion in appendices A and B, and the detailed experiments in the later appendices.  As such, it is easy to recommend acceptance, and I will push for this to receive at least a spotlight.  Even so, I urge the authors to make careful revisions for remaining issues raised by the reviewers, and to perform a full pass of their own.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The reviewers are uniformly positive, and I also voice similar praises, e.g., I appreciate the extensive discussion in appendices A and B, and the detailed experiments in the later annexes. As such, it is easy to recommend acceptance."
"Reviewers are all positive and excited about the paper: interesting and natural model, novel and robust mechanism with theoretical guarantees, nice sample complexity analysis, experiments on real-world data.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a discrete and compositional representation of goal states for goal-conditioned RL. The idea is to learn a goal representation via self-supervised learning and discretize the learned representation via VQ-VAE, and finally use the learned goal representation for goal-conditioned RL. The proposed method improves performance on several goal-conditioned RL benchmarks.

All of the reviewers found the idea simple and reasonable, and the results on a variety of benchmarks are quite comprehensive and strong. Although there were concerns around why the proposed discretized representation forms a semantically meaningful latent space and where the improvement comes from, the authors addressed them during the rebuttal period with updated results. All of the reviewers became in favor of the paper as a result. Thus, I recommend accepting this paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agreed that the paper is simple and reasonable, and the results on a variety of goal-conditioned RL benchmarks are quite comprehensive and strong. While there were concerns about why the proposed discretized representation forms a semantically meaningful latent space, and where the improvement comes from,"
"This work proposes a simple but useful way to train RNN with binary / ternary weights for improving memory and power efficiency. The paper presented a sequence of experiments on various benchmarks and demonstrated significant improvement on memory size  with only minor decrease of accuracy. Authors' rebuttal addressed the reviewers' concern nicely. 

","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about generalization of the approach to different datasets."
"This submission proposes a method for learning sparse DNNs which consists of three components: First, a ""dense"" network is maintained and updated in each backwards pass, but the forward pass is done via a sparsified version of the network; sparsification is done via ""soft"" thresholding; and the sparsity ratio is increased over the course of training. Reviewers noted that each of these components had been previously proposed, and that the state-of-the-art baselines are not actually state-of-the-art anymore. They also noted that the paper read more like a draft and needs substantial improvement. The consensus was therefore to reject.","This paper proposes a method for learning sparse DNNs which consists of three components: First, a ""dense"" network is maintained and updated in each backwards pass, but the forward pass is done via a sparsified version of the network; sparsification is done through ""soft"" thresholding; and the sparsity ratio is increased over the course of training. The consensus was therefore to reject the paper."
"This work is well written and easy to follow and proposes a novel framework to utilize unlabeled output data. The authors have also given a detailed proof that the denoiser reduces the required complexity of the predictor. However, ultimately the experimental results are somewhat weak and leave doubts as to how effective the approach is. More convincing experimental results such as significant improvements on a well understood task and acknowledging that the approach is mostly useful when combined with pre-training and back translation would improve the work.

Pros
- Well written.
- Technically novel approach to the problem of utilizing unlabeled output data.
- Interesting proof on the reduced complexity requirement for the predictor.

Cons:
- Experimental results are not convincing. Showing significant improvements on a well understood task would be more convincing.
- The approach is only really useful when combined with pre-training or back-translation.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors have also given a detailed proof that the denoiser reduces the required complexity of the predictor. The reviewers have also provided a clear proof on the reduced complexity requirement for the predictedor. However, some concerns about generalization were raised regarding generalization of the approach."
"The reviewers all liked the paper. The authors' response clarified most points raised by the reviewers. In view of that, the authors are strongly invited to take the feedback on board for the final version. The main ethical issue raised by reviewers is the risk of erasure and invisibility of linguistic variability in Chinese language training data. Data cards need to be added to the final version.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The authors present BASGD and asynchronous version of SGD that attempts to be robust against byzantine failures/attacks. 
The papers is overall well written and clearly presents the results. Some novelty is present as there have been limited work in asynchronous algorithms for byzantine ML. 

However, there have been several concerns raised by the reviewers, on which I agree, and they have not been fully addressed:
1) the tradeoff between asynchrony and robustness, as BASGD cannot handle the case of a buffer being straggler, which limits some of the novelty in this work
2) issues with the definition of privacy leakage has not been fully addressed
3) some reviewers mentioned the theoretical results being of limited importance, but arguably this is true for other related work in this area. Perhaps a general criticism is valid as to what is the operational value of the proposed guarantees. That is convergence does not exclude a model that has undesirable properties, eg has bad prediction accuracy for a small subset of tasks.
4) Finally, the motivation of the system model of the paper ( eg storing gradients as opposed to instances) paper is of unclear practical relevance, as was raised by multiple reviewers. 

Overall the consensus was that the paper does have merits, however, some of the most major concerns were not properly addressed. This paper can potentially be improved for a future venue.

","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. Overall the paper is overall well written and clearly presents the results. Some concerns are raised regarding the tradeoff between asynchrony and robustness."
"This paper studies post-training quantization by proposing Network-Wise Quantization (NWQ) an end-to-end quantization approach that takes into account relationships between layers rather than treating layers independently. Using this approach, the paper demonstrates compelling empirical gains across a number of architectures and compression factors. Reviewers recognized the practical success of the approach as demonstrated by these empirical results and praised the clarity of the manuscript. However, there were concerns regarding the novelty of the approach and whether the proposed method is simply a composition of previous methods. While I understand these concerns, I think there is a significant delta between this work and previous approaches, especially when taking into account the markedly improved performance and the challenges of determining how to apply these lines of thinking to end-to-end training. The authors also expanded their discussion of these works in their updated manuscript, clarifying the differences. There were also concerns regarding the hyperparameter tuning, but the authors clarified in their response that the large majority of experiments used a constant set of hyperparameters, suggesting that these results are not simply the effect of tuning. Altogether, I think this paper makes an impactful contribution and will be a valuable addition to the conference.","This paper studies post-training quantization by proposing Network-Wise Quantization (NWQ) an end-to-end quantization approach that takes into account relationships between layers rather than treating layers independently. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors also expanded their discussion of these works in their updated manuscript, clarifying the differences."
"This paper does two things. First, it proposes an approach to estimating the mutual information between the input, X, or target label, Y, and an internal representation in a deep neural network, L, using MINE (for I(Y;L)) or a variation on MINE (for I(X;L)) and noise regularization (estimating I(X;L+ε), where ε is isotropic Gaussian white noise) to avoid the problem that I(X;L) is infinite for deterministic networks and continuous X. Second, it attempts to validate the information bottleneck theory of deep learning (Tishby and Zaslavsky, 2015) by exploring an approach to training DNNs that optimizes the information bottleneck Lagrangian, I(Y;L) − βI(X;L+ε), layerwise instead of using cross-entropy and backpropagation. Experiments on MNIST and CIFAR-10 show improvements for the layerwise training over cross-entropy training. The penalty on I(X;L+ε) is described as being analogous to weight decay. The reviewers raised a number of concerns about the paper, the most serious of which is that the claim that the layerwise training results validate the information bottleneck theory of deep learning is too strong. In the AC's opinion, R1's critique that ""[i]f the true mutual information is infinite and the noise regularized estimator is only meant for comparative purposes, why then are the results of the training trajectories interpreted so literally as estimates of the true mutual information?"" is critical, and the authors' reply that ""this quantity is in fact a more appropriate measure for “compactness” or “complexity” than the mutual information itself"" undermines their claim that they are validating the information bottleneck theory of deep nets because the information bottleneck theory claims to be using mutual information. The AC also suggests that if the authors wish to continue this work and submit it to another venue, they (1) discuss the fact that MINE estimates only a lower bound that may be quite loose in practice and (2) say in their experimental section whether or not the variance of the regularizing noise was tuned as a hyperparameter, and if so, how results varied with different amounts of noise. Finally, the AC regrets that only one reviewer participated in the discussion (in a very minimal way), despite the reviewers' receiving several reminders that the discussion is a defining feature of the ICLR review process.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers raised a number of concerns about the paper, including the fact that the layerwise training results validate the information bottleneck theory of deep learning."
"Meta Review: Generally, the idea of querying the label comparisons to is an interesting and natural choice to enable active learning in real-world tasks. Both theoretical analyses and empirical studies have been reported in this paper. The whole paper is well organized and easy to follow.

The expeirmental studies performed in this paper can be improved, such as considering performance comparison in passive learning, the inclusion of more SOTA baselines, etc.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. Both theoretical analyses and empirical studies have been reported in this paper."
"The paper presents a ""conceptual  advance connecting causality, disentangled representation learning, invariant representations and robust classification"". Authors propose to decompose the image generation process to independent mechanism that can be composed (foreground masks (shapes), forground texture, and backgrounds), allowing for a specific image to generate counterfactuals , by changing some variations factors, while keeping other fixed. One can use interventional data to augment classifiers, this can lead in certain cases to improvement in accuracy and in other in improving the robustness. 

There was concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples ) and some missing references. The rebuttal of the authors and their updated paper reflected comprehensively all those concerns and addressed them, highlighting limitations of the method and adding more examples of its failures. 

I liked the ideas and concepts in  this paper , and it will be exciting to generalize such generative approach to other domains, this  work is a first step. I think it will be good addition to ICLR program ","The paper presents a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about the clarity of the paper regarding the structured causal model considered and its applicability beyond image generation, experimental protocol for choosing hyperparameters (loss scaling and ratios of real data and interventional samples) and some missing references. I think the paper will be good addition to ICLR program."
"The paper proposes new techniques for improving the generalization ability of deep learning models for Knowledge Tracing (KT). Instead of designing more sophisticated models, the paper investigates simple data augmentation techniques that can be applied to train existing models. In particular, three different augmentation strategies are proposed based on replacement, insertion, and deletion in the training data. These strategies are then applied with appropriate regularization loss ensuring consistency and monotonicity in the training process. Extensive experiments are performed using three popular neural models for KT and four publicly available datasets. Overall, the paper studies an interesting problem in an important application domain of online education. The results are promising and open up several exciting follow-up research directions to explore more complex data augmentation techniques for KT.

I want to thank the authors for actively engaging with the reviewers during the discussion phase and sharing their concerns about the quality of the reviews.  The reviewers generally appreciated the paper's ideas; however, there was quite a bit of spread in the reviewers' assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should incorporate the reviewers' feedback to better position the work w.r.t. the existing literature on data augmentation and state of the art results, better motivate the data augmentation strategies in the context of educational applications possibly through additional data analysis, and add more ablation studies w.r.t. the hyperparameters associated with data augmentation. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers' feedback when preparing future revisions of the paper.
","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have provided detailed and constructive feedback for improving the paper. The authors should incorporate the reviewers' feedback to better position the work w.r.t. the existing literature on data augmentation and state of the art results, better motivate the data hausse strategies in the context of educational applications."
"The paper introduces a new procedure to initialize the optimisation in training process of DNN models, including the recent ViT architecture. All the reviewers recommend acceptance and appreciate the promising empirical results backed by the strong theoretical foundations. AC recommends acceptance as well.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers recommend acceptance and appreciate the promising empirical results."
"This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns. 

Given that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers expressed concerns about generalization of the approach to different datasets."
"The consensus among the reviewers was that this work covers an important topic and a broad number of tasks. There were concerns about the documentation, reproducibility, and accessibility of the dataset. But the authors have done a good job in addressing most of these concerns with documentation, an API, and example notebooks.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have done a good job in addressing most of these concerns with documentation, an API, and example notebooks."
"In this work, the utilize a graph based approach to create the largest Twitter bot detection dataset, with over 100K bot users identified. The dataset is of great use for the social media mining community in many different aspects. The work was greatly improved after a very interactive and fruitful discussion period, making the contribution a lot more refined and useful.

Pros:
- Largest dataset available.
- Thorough evaluation
- Addition of most reviewer's suggestions have strengthen the details in the paper

Cons:
- Minor statistical rigor elements are missing, but not completely necessary.
 ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The AC has taken into account all strengths and weaknesses mentioned by reviewers in making a recommendation for this paper. Here is the summary of what was considered in carefully considering a final decision:

* Reviewers agree that the presented resource, the Dollar Street dataset, addresses a need for models that can be held more accountable in terms of geographic diversity and demographic diversity. 
* There is generally no issues in terms of methodology or soundness of the proposed resource.
* Unfortunately, experiments on demonstrating the usefulness of this resource are limited, and largely replicate what was already found in Devries et al. 2019, a workshop paper at CVPR from three years ago which also used the same source of data -- and at the same scale. The analysis in this previous paper in some way go further by matching image categories using human evaluators individually for each image -- as pointed out by reviewer dcyB.
* The dataset was downloaded from a third party's website and as such the authors had no control in how the images were collected, nor there was any effort to enhance this resource in terms of annotations, diversity or size.
* The dataset size is 38k images which is a rather small dataset for training or enhancing a model, and its use would be limited mostly as a benchmark dataset. As a benchmark however experiments seem limited as pointed by reviewer eAoC. An experiment on training on this dataset and evaluating on this dataset is performed only with a Resnet network, however it is hard to assess what is the significance of this experiment. 

There are various suggestions made by reviewers below that would enhance the quality of this paper. Despite shortcomings, reviewers are still enthusiastic to see this dataset in an easy to use format as a benchmark. I also strongly suggest the authors to revise their section on licensing where it says ""Training machine learning models on public domain work is widely accepted legally."" as it does not seem appropriate for this paper to be making determinations or recommendations as to what is considered legal or not. 
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the presented resource addresses a need for models that can be held more accountable in terms of geographic diversity and demographic diversity."
"My main concern, that was addressed by the reviewers and was not answered by the authors, is that the improvement is q times faster algorithm for an algorithm that takes time exponential in q. In addition, the missing experimental results makes this a very theoretical paper.

Still, I recommend to accept the paper due to the significance of the problem, and conditioned on the promise of the authors to update the requested changes in the final verison.

","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about generalization of the approach to different datasets, although some concerns were raised regarding generalization were noted."
"How to design RL algorithms that directly acquire good representations? This paper gives an answer that contrastive representation learning can be cast as a goal-conditioned RL using the inner product of learned representations.
The technical novelty of this paper is sound, with the thorough theoretic motivation of the proposed method and solid experiments. The presentation of this paper is also satisfactory.
All the reviewers provided positive feedback on this paper. I also enjoy reading this paper.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The technical novelty of the paper is sound, with the thorough theoretic motivation of the proposed method and solid experiments. The presentation of this paper is also satisfactory."
"This work addresses a novel and important real-world setting for semi-supervised learning – the open-world problem where unlabeled data may contain novel classes that are not seen in labeled data.  The paper provides an approach by combining three loss functions: a supervised cross-entropy loss, a pairwise cross-entropy loss with adaptive uncertainty margin, and a regularization towards uniform distribution.  

The authors were responsive to reviewers’ comments and have respectively improved their paper by adding experiments, including an ablation study of each component of the objective function, study of the effect regularization on unbalanced class distributions, reporting accuracy on pseudo-labels.  While two reviewers have slightly increased their scores, some concerns still remain.

This is a borderline paper, and after some discussion and calibration, we decided that the work in its current form does not quite meet the bar for acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were responsive to reviewers’ comments and have respectively improved their paper by adding experiments, including an ablation study of each component of the objective function, study of the effect regularization on unbalanced class distributions, and reporting accuracy on pseudo-labels."
"## A Brief Summary
This paper uses offline algorithms that can see the entire time-series to approximate the online algorithms that can only view the past time-series. The way this is done is basically, the offline algorithm is used to provide discrete class targets to train the online algorithm. The paper presents results on synthetic and historical stock market data.

## Reviewer s1H9
**Strengths:**
- Practical problem.
- Novel approach.
- Clear presentation.
**Weaknesses:**
- No other baselines.
- No theoretical guarantees behind the approach.
- Writing could be improved.

## Reviewer EgW9
**Strengths:**
- Clear writing.
- Interesting research direction.
**Weaknesses:**
- The primary claim seems incorrect and unclear. 
- Due to the unclarity about the primary claim of this paper, it is difficult to evaluate the paper. 
- Lack of baselines.
- The lack of discussions of the related works.

## Reviewer gii5
**Strengths:**
- Interesting and novel approach.
**Weaknesses:**
- Difficult to evaluate, with no empirical baselines or theoretical evidence.
- The datasets used in the paper are not used in the literature before. Authors should provide experimental results on datasets from the literature as well.
- The paper needs to compare against the other baselines discussed in the related works.
- More ablations and analysis on the proposed algorithm is required.
- Unsubstantiated claims regarding being SOTA on the task, since the paper doesn't compare against any other baselines on these datasets.
- The paper can be restructured to improve the flow and clarity.

## Reviewer zoKR
**Strengths:**
- Novel and interesting research topic.
- Bridging classical algorithms and ML.
- Clearly written.
 
**Weaknesses:**
- Lack of motivation for the problem.
- The approach only works with offline algorithms that work on time-segmented data.

## Reviewer aaFn
**Strengths:**
- Novel algorithm.

**Weaknesses:**
- Potentially overfitting to the offline data.
- Data hungry approach.
- Confusion related to the occurrence moments of predicted future actions.
- Section 2 is difficult to understand.

## Key Takeaways and Thoughts
Overall, I think the problem setup is very interesting. However, as pointed out by reviewers gii5 and EgW5, due to the lack of baselines, it is tough to compare the proposed algorithm against other approaches, and this paper's evaluation is challenging. I would recommend the authors include more ablations in the future version of the paper and baselines and address the other issues pointed out above by the reviewers.","This paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"I thank the authors for their submission and active participation in the discussions. This paper introduces a method for learning a policy that can ask an expert for help, i.e., to obtain the expert action. On the positive side, reviewers found the method to be general [uya8], original and significant [gw2r], intruiging in terms of being able to reuse an existing policy [HGan], and tackling an important problem [rsmr,bRWC], and the paper to be clear [uya8,gw2r,rsmr,bRWC]. In terms of negative points, reviewers were concerned about the novelty [bRWC], unimpressive qualitative results despite strong quantitative results [bRWC], and issues with the range of baselines [bRWC,rsmr] and ablations considered [uya8]. Overall, the paper is borderline. However, bRWC indicated they would raise their score but I don't see this being reflected. Furthermore, in my view reviewer rsmr's concerns regarding baselines and ablations has been addressed by the author rebuttal. Thus, I am siding with reviewers gw2r and HGan, and recommend acceptance. However, I very strongly encourage the authors to further improve their paper based on the reviewer feedback, in particular the points raised by reviewer bRWC regarding the importance of the Success Prediction component of the method.  ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers were concerned about the novelty [bRWC], unimpressive qualitative results [gw2r],"
"All reviewers agree that this paper comprehensively studies a fundamental question of PAC learning under instance-targeted poisoning, including the study of realizable, agnostic, deterministic learning settings; overall this paper makes a nice contribution to the field of robust machine learning. 

The authors are strongly encouraged to incorporate the comments by the reviewers, including revising on motivating examples, terminologies, etc. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that this paper comprehensively studies a fundamental question of PAC learning under instance-targeted poisoning, including the study of realizable, agnostic, deterministic learning settings; overall this paper makes a nice contribution to the field of robust machine learning."
"The paper proposes a new defense against adversarial attacks on graphs using a reweighting scheme based on Ricci-flow. Reviewers highlighted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a novel and promising contribution. Reviewers also recognized that the paper has significantly improved after rebuttal and clarified some aspects of their initial reviews.

However, there exist still concerns around the current version of the manuscript. In particular, important aspects of the method and algorithm, as well as some design choices are currently unclear. This includes evaluating and discussing robustness, training method, and practicality/improvements in real-world scenarios. I agree with the majority of the reviewers that the current version requires an additional revision to iron out the aforementioned issues. However, I also agree with the reviewers that the overall idea is promising and I'd encourage the authors to revise and resubmit their work with considering the feedback from this round of reviews.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers noted that the paper introduces interesting ideas and that the use of Ricci-curvature/flow is a promising contribution. However, there are still concerns about the current version of the paper."
"This paper proposes a learning-based method for shape registration that conditions on regions of the shape rather than learning from the entire point cloud in one shot.  The reviewers point out several questions about the method, thanks to expository issues as well as missing comparisons/ablation studies.  As the authors have chosen not to submit a rebuttal, I will refer them to the original reviews for details here for additional points of improvement.","This paper proposes a learning-based method for shape registration that conditions on regions of the shape rather than learning from the entire point cloud in one shot. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted several questions about the method, thanks to expository issues as well as missing comparisons/ablation studies."
"Post discussion update
------------------------

The authors completed a rebuttal that addressed all of the reviewers' critiques, and also uploaded a revised and improved manuscript. The most significant improvement seems to be the addition of a small related work section in the introduction and a limitation plus future work section at the end of the paper. However, since the final paper has an 8 page limit, the current 9-page manuscript will need to be shortened. 

The clarifications seemed to sway two of the reviewers toward increasing their ratings. Generally, it's clear that although this is a non-standard paper, it raises interesting and potentially provocative points that would be great for the CORL community to discuss. Therefore, I am recommending acceptance.



Original review
----------------


This paper addresses the HRI challenge of measuring how well a robot has learned a user's reward function. It describes two main types of measures---parameter-based and reward-based---and provides both theoretical and empirical examples of the shortcomings of each. 


Strengths
* This seems like the type of paper that would be useful to assign in an HRI course or give to a new grad student, because it clearly explains the different measures and provides a comparison of them.

* Reviewers noted that the issues brought up by this paper are important and very relevant to current approaches. The paper is timely.

* Reviewers agreed that paper balances well between theory and empirical examples. 

* The paper was well written and easy to understand.


Weaknesses

* The main critique is that reviewers feel that the paper doesn't go deeply enough into the discussion of what to do with these findings. How should robotics researchers use this information to design better measures? In what cases are these measures likely to fail, or to be accurate? Section 6 poses such questions, but it would add substantially to the novelty of the paper if there was some suggestion of where to go from here.  

* Although the paper bills itself as a survey, reviewers noted that it doesn't provide much of a discussion of related work. Reviewers also provided additional citations that address some of the issues brought up regarding current measures of performance. 

* A couple of reviewers questioned the connection to robot learning in particular.

* There is no limitations statement, which is a required component.","This paper addresses the HRI challenge of measuring how well a robot has learned a user's reward function. The reviewers agreed that the paper is well written and easy to understand. Overall, the reviewers recommended acceptance with minor revisions. Reviewers noted that this is a non-standard paper, but it raises interesting and potentially provocative points that would be great for the CORL community to discuss."
"Authors present a method for single object tracking (SOT) that is entirely comprised of transformers. The architecture is simple: 

1) Swin is used to generate embeddings for both template and search region
2) Embeddings are concatenated
3) An encoder transformer performs MHSA of the embeddings.
4) A decoder performs cross-attention from search tokens to template tokens and a special ""motion token"" which is constructed from a linear operation over the prior motion trajectory relative to the frame. 
5) Output token is fed to final layers that perform IoU aware classification and bounding box regression.

Evaluations are performed on 5 SOT datasets, achieving SOTA on all of them.


Pros:
- [AC/R] Important problem, technically sound, and new SOTA on this task.
- [AC/R] New motion token approach is novel and provides significant improvement.
- [AC/R] Simple and elegant architecture
- [AC/R] Insightful discussions
- [AC/R] Clearly written and easy to follow
- [AC/R] Interesting ablations
- [AC/R] High frame rate

Cons:
- [R] Low novelty of transformer approach, but this is negated by the novelty of the motion token.
- [R] Details regarding pretraining are missing. Authors provide in response.
- [R] Motivation of motion token design is not clear. Authors provided further ablations of different implementations of the motion token, showing the current form performs best.
- [R] Provide additional details regarding where and how SwinTrack outperforms other approaches on the benchmarks. Authors provided additional granularity of performance stratifications within the LaSOT benchmark.
- [R] Add more recent high performing trackers. Authors added several methods published in 2022.
- [R] Some additional questions about various details were posed by reviewers, which will all answered by the authors. 

The single reviewer with reject recommendation changed to accept in their comments after the discussion period but did not update their score. Given unanimous agreement on accept, AC recommendation is accept. 

AC Rating: Strong Accept","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of each review. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The authors study representations obtained from image classifiers and contrast the classic training with adversarial training, so-called non-robust and robust networks, respectively. The authors primarily use the CKA metric on CIFAR10 and subsets of ImageNet2012 provide several novel insights on ""salient pitfalls"" in robust networks which suggest that robust representations are less specialized with a weaker block structure, early layers in robust networks are largely unaffected by adversarial examples as the representations seem similar for benign vs. perturbed inputs, deeper layers overfit during robust learning, and that models trained to be robust to different threat models have similar representations.

The reviewers agreed that these contributions are interesting to the larger community and that the presentation of the results is clear and straightforward. The main issues raised by the reviewers were carefully addressed in the rebuttal. Please update the manuscript as discussed.","This paper proposes a novel approach to neural network training, leveraging recent advances in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that these contributions are interesting to the larger community and that the presentation of the results is clear and straightforward. The main issues raised by the reviewers were carefully addressed in the rebuttal."
"The paper proposes a novel curriculum learning method for RL based on the concept of boosting. The proposed method builds on the curriculum value-based RL framework and uses boosting to reuse action-values from previous tasks when solving the current task. The method is analyzed theoretically in terms of approximation accuracy and convergence. Moreover, extensive experiments demonstrate the effectiveness of the method. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results. I want to thank the authors for their detailed responses that helped in answering some of the reviewers' questions and increased their overall assessment of the paper. At the end of the discussion phase, there was a clear consensus that the paper should be accepted. The reviewers have provided detailed feedback in their reviews, and we strongly encourage the authors to incorporate this feedback when preparing a revised version of the paper.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers acknowledged the importance of the studied problem setting and generally appreciated the results."
The authors present a compelling case study describing a novel system for literate programming. Comparison with other similar publishing platforms is scarce and would be a welcome addition.,"The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors present a compelling case study describing literate programming."
"The paper's strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The theory is novel (but it seems to relate closely to the work https://arxiv.org/abs/1711.02771.) The main drawback the reviewer raised includes a) it's not clear how tight the lower bound is; b) the theory only applies to a particular subcase of GANs --- it seems that the only reasonable instance that allows efficient generator is the case where Y = G(x)+\xi where \xi is Gaussian noise. The authors addressed the issue a) with some new experiments with linear generators and quadratic loss, but it lacks experiments with deep models which seems to be necessary since this is a critical issue. Based on this, the AC decided to recommend reject and would encourage the authors to add more experiments on the tightness of the lower bound with bigger models and submit to other top venues. 
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper's strength is in that it shows the log likelihood objective is lower bounded by a GAN objective plus an entropy term. The authors addressed the issue with some new experiments with linear generators and quadratic loss, but it lack experiments with deep models which seems to be"
"The paper proposes an interesting step in the direction of neuro-symbolic reasoning. While there is no consensus among reviewers about the key novelty of the method, all acknowledge the interest of the direction. All of them also recognize that the submission improved greatly during the discussion phase: clarification of motivations, of experimental settings and results, of discussion with previous work.

However, despite those improvements, the submission is not yet ready for publication at ICLR. We encourage the authors to use the very detailed reviews and comments to improve the work. In particular, we encourage them to pay attention at three aspects:

1/ Comparison with large language models: the discussion wrt T5 is important. A key motivation for the proposed model is that it is bringing information and elements for QA (or other reasoning tasks) that purely scaling up language models can not bring. Or maybe they can bring the same kind of improvement but at a much lower computational cost. In any case, this is a very important point to justify the interest of such approach, and neuro-symbolic reasoning overall, empirically.

2/ Using GPT2 (or equivalent): the discussion on using GPT-2 for generating new facts is key too. It is essential to bring this description from appendix to the core of the paper. But more discussion are expected.  For instance, what if GPT-2 generates facts that are false and lead to answering and justifying a wrong answer? In other words, how does it impact the integrity of the contextualized KG? This is an essential point that needs to be worked on more thoroughly. 

3/ Overall there have been a lot of discussion to improve the motivations and the contributions. But they are not reflected in the paper necessarily. Following R2, we encourage the authors to ""refocus the existing version (e.g., from vague discussion about neural-symbolic models towards establishing solid comparison to the most related previous work in various sections of the submission)""



","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers acknowledge that the paper improved greatly during the discussion phase: clarification of motivations, of experimental settings and results, of discussion with previous work. However, despite those improvements, the submission is not yet ready for publication at ICLR."
"This paper proposes a distributed containerized multi-agent reinforcement learning(CMARL) framework that addresses three challenges in MARL: 1) Demanding data transfer. 2) Inter-process communication. 3) Effective Exploration. Using a container that collects environment experiences from parallel actors into buffers and learns local policies, CMARL demonstrates notable performance improvements with respect to time as compared to state-of-the-art benchmarks.

Although the reviewers acknowledge that the paper addresses a relevant topic, proposes an effective method, and is well written, after reading the authors' feedback and discussing their concerns, the reviewers reached a consensus about rejecting this paper in its current form. They feel that the contribution is too incremental and that the experimental comparisons are somehow unfair.

I suggest the authors take into consideration the reviewers' suggestions while preparing an updated version of their paper for one of the forthcoming machine learning conferences.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the paper addresses a relevant topic, proposes an effective method, and is well written."
"The paper's initial evaluation was below par, but the author feedback helped clarify several crucial points after which two of the reviewers increased their scores by a point, bringing the current evaluation to borderline. 

The paper addresses a relevant and challenging problem in the RL domain. However, in my opinion, from the reviewers' and authors' remarks and from my own reading of the paper, there are concerns that need to be addressed before the paper can be publication worthy. Primary among these is the quantum of novelty -- as many reviews point out, the key idea of viewing an episodic trajectory as a multivariate (vector) sample for running hypothesis tests is not novel in itself, as is the claim that new tests have been devised. Another crucial issue is the (parametric) assumption of normality for the episodic reward sequence which is not adequately justified in the paper -- even a two time-step trajectory with normal rewards per state transition can exhibit a mixture-of-Gaussians type reward distribution for the second state, breaking the assumption. As it transpired from the reviews of Reviewer4, reducing environment shift/degradation to just a mean change problem, without even considering a change in the variances (2nd order statistics), seems to be too stylized to be effective. There are other, nonparametric approaches in statistics based on testing for changes in the distribution function (kernel density estimation approaches, for instance), which could perhaps be applied without normality assumptions and yield favourable results. The experimental results for detection delay often show significant overlaps of the delay distributions for different procedures (e.g., Hotelling vs. Mean vs. UDT etc.), which does not indicate an advantage of the proposed method. 

I would urge the author(s) to assimilate the feedback and delve deeper as to why and how parametric procedures based on normality assumptions may or may not succeed, so as to significantly strengthen the theoretical and practical message of this work.  ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. However, some concerns were raised regarding the generalization of the approach to different datasets, although some concerns about generalization were noted. The reviewers also noted that the paper is not novel in itself, as is the claim that new tests have been devised."
This paper proposes a novel communication-efficient learning method that significantly reduces feature size and communication traffic. The rebuttal solved the reviewers concerns about the dataset size and accuracy / latency trade off. ,"This paper proposes a novel communication-efficient learning method that significantly reduces feature size and communication traffic. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"
This paper presents ""Automunge"" a python library for pre-processing tabular data. 
The authors develop a useful library that can be used by practicioners for data engineering in NNs applications. 
The reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the 
performance of the models that is applied on. A common concern was the lack of performance plots compared to other alternatives. 
In the response the authors have done a rather thorough job of addressing the reviewers comments and
adding material in the supplementary. However, given the current presentation, the manuscript needs a considerable amount of  rewriting to incorporate the suggested changes into the main paper. As it is, I don't think ICLR is the right venue for the manuscript.  It might reach its audience better in venues like SysMl or PyCon also suggested by a reviewer. 
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The reviewers raised a common concern regarding the lack of focus on the actual usefulness of the librabry in improving the performance of the models that is applied on."
"The paper proposes a method to generate attention masks to interpret the performance of RL agents. Results are presented on a few ATARI games. Reviewers unanimously vote for rejecting the papers. R1, R3 give a score of 5, whereas R4, R5 give a score of 4. Their concerns are best explained in their own words: 

R1 says, ""The use of attention maps to analyze and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either.""

R3 says, ""the analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze-like, and ball-and-paddle) may shed light on the learned attention's general property.""

R5 says, ""I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive""

In their rebuttal, to address R1's concern authors suggested that the use of attention on both value and policy networks is novel. This is not sufficient, because it does not show why such attention maps are more useful than ones proposed by prior work. As suggested by reviewers, a systematic study or a human study clearly showing that the proposed method adds more interpretability is critical. However, this is missing.  In response to R3, the authors provided experiments on more games. But this is not the point -- because it's not about the number of environments in which experiments are provided, but rather the nature of the analysis that is performed. Finally, R5 comments that it's unclear whether attention actually provided interpretability or not. 

Due to the lack of convincing analysis that demonstrates the utility of the proposed method in advancing the understanding of decisions made by RL agents, I recommend that the paper be rejected.

","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers unanimously vote for rejecting the paper."
"The paper's stated contributions are:

(1) a new perspective on learning with label noise, which reduces the problem to a similarity learning (Ie, pairwise classification) task

(2) a technique leveraging the above to learn from noisy similarity labels, and a theoretical analysis of the same

(3) empirical demonstration that the proposed technique surpasses baselines on real-world benchmarks

Reviewers agreed that (1) is an interesting new perspective that is worthy of study. In the initial set of reviews, there were concerns about (2) and (3); for example, there were questions on whether the theoretical analysis studies the ""right"" quantity (pointwise vs pairwise loss), and a number of questions on the experimental setup and results (Eg, the computational complexity of the technique). Following a lengthy discussion, the authors clarified some of these points, and updated the paper accordingly.

At the conclusion of the discussion, three reviewers continued to express concerns on the following points:

- *Theoretical justification*. Following Theorem 3, the authors assert that their results ""theoretically justifies why the proposed method works well"". The analysis indeed provides some interesting properties of the reduction, such as the fact that it preserves learnability (Appendix F), and that the ""total noise"" is reduced (Theorem 2). However, a complete theoretical justification would involve guaranteeing that the quantity of interest (Ie, the clean pointwise classification risk) is guaranteed to be small under the proposed technique. Such a guarantee is lacking. 
  - This is not to suggest that such a guarantee is easy -- as the authors note, this might involve a bound that relates pointwise and pairwise classification in multi-class settings, and such bounds have only recently been shown for binary problems -- or necessary for their method being practical useful (per discussion following Theorem 3). Nonetheless, without such a bound, there are limits to what the current theory justifies about the technique's performance in terms of the final metric of interest.

- *Comparison to SOTA*. Reviewers noted that the gains of the proposed technique are often modest, with the exception of CIFAR-100 with high noise. Further, the best performing results are significantly worse than those reported in two recent works, namely, Iterative-CV and DivideMix. The authors responded to the former in the discussion, and suggested that they might be able to combine results with the latter. While plausible, given that the latter sees significant gains (Eg, >40% on CIFAR-100), concrete demonstration of this point is advisable: it is not immediately apparent to what extent the gains of the proposed technique seen on ""simple"" methods (Eg, Forward) would translate more ""complex"" ones (Eg, DivideMix).
  - In the response, the authors also mentioned that (at least the initial batch of) the experiments are intended to be a proof-of-concept. This would be perfectly acceptable for a work with a strong theoretical justification. However, per above, this point is not definitive.

- *Creation of Clothing1M*. The authors construct a variant of Clothing1M which merges the classes 3 and 5. Given that prior work compares methods on the original data, and that this potentially reflects noise one may encounter in some settings, it is advisable to at least report results on the original, unmodified version.

- *Issues with clarity*. There are some grammatical issues (Eg, ""is exact the""), typos (Eg, ""over 3 trails""), notational inconsistencies (Eg, use of C for # of classes in Sec 2, but then c in Sec 3.1), and imprecision in explanation (Eg, Sec 3.2 could be clearer what precise relationships are used from [Hsu et al. 2019]).
  - These are minor but ought to be fixed with a careful proof-read.

Cumulatively, these points suggest that the work would be served by further revision and review. The authors are encouraged to incorporate the reviewers' detailed comments.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about the generalization of the approach to different datasets, although some concerns about generalization were raised regarding generalization. However, the reviewers agreed that the paper is worthy of study, and that a complete theoretical justification is lacking."
"The paper presents some exciting results on the convergence of averaged SGD for overparameterized two-layer neural networks. The AC and reviewers all agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews. The corresponding revisions on assumptions and references, and the added simplified proposition in the introduction have nicely improved the manuscript. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the contributions are significant and well presented, and appreciate the author feedback to the reviews."
"This paper presents a novel neural network architecture to predict interacting residues among two interacting proteins, and evaluates its performance on benchmarks. While the reviews were initially mixed, there has been a productive discussion and significant improvements in the paper during the discussion, including in particular much needed clarifications about the proposed methods, and more experimental results with an ablation study to better assess the benefits of various design choices. While no reviewer is willing to champion this paper as a ""strong accept"", due to the relatively modest novelty compared to existing methods, there is a consensus towards ""weak accept"" given the final quality of the work presented and potential usefulness of the method for the problem tackled.","This paper presents a novel neural network architecture to predict interacting residues among two interacting proteins, and evaluates its performance on benchmarks. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The authors propose a particle-based entropy estimate for intrinsic motivation for pre-training an RL agent to then perform in an environment with rewards. As the reviewers discussed, and also mentioned in their reviews, this paper bears stark similarity to work of 5 months ago, presented at the ICML 2020 Lifelong ML workshop, namely, ""A Policy Gradient Method for Task-Agnostic Exploration"", Mutti et al, 2020--MEPOL. What is novel here is the adaptation of this entropy estimate to form an intrinsic reward via a contrastive representation and the subsequent demonstration on standardized RL environments.  The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin. Unfortunately this work does not meet the bar for acceptance relative to other submissions.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors have added a comparison to MEPOL, and in these experiments, APT outperforms this method, sometimes by some margin."
"The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel. They show that inference is more efficient and training is easier.

In practice, the synthetic experiments seem to show a greater improvement than appears in real data.  There are concerns about the clarity, lack of theoretical proofs, and at times overstated claims that do not have sufficient support.

The ratings before the rebuttal and discussion were 7-4-6.  After, R1 adjusted their score from 6 to 4.  R2 initially gave a 7 but later said ""I think the authors missed an opportunity here. I rated it as an accept, because I saw what it could have been after a good revision. The core idea is good, but fully agree with R1 and R3 that the paper needs work (which the authors were not willing to do). I checked the latest revision (as of Monday morning). None of R3's writing/claims issues are fixed, neither were my additional experimental requests, not even R1's typos."" There is therefore a consensus among reviewers for reject.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about the generalization of the approach to different datasets, although some concerns were raised about generalization were noted."
"The paper contributes to the understanding of straight-through estimation for single hidden layer neural networks, revealing advantages for ReLU and clipped ReLU over identity activations.  A thorough and convincing theoretical analysis is provided to support these findings.  After resolving various issues during the response period, the reviewers concluded with a unanimous recommendation of acceptance.  Valid criticisms of the presentation quality were raised during the review and response period, and the authors would be well served by continuing to improve the paper's clarity.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a Role Diversity metric, meant to quantify how different roles are in a multi-agent RL setting. There's actually three versions of this metric, or three aspects (the distinction is not entirely clear to this area chair).

The reviewers are generally not very enthusiastic about the paper, with scores hovering at or just below the acceptance threshold. There has been extensive discussion between reviewers and authors, but there a sense that there is confusion about the exact purpose and contribution of the paper. This is reinforced by the authors' ""letter to area chair"", which outlines several ways the reviewers have not gotten the message. Reading the paper, it appears to me that the root cause is that the authors are indeed not communicating clearly what the paper contributes and why. It is, after all, the authors' responsibility that the reviewers understand the work. My own impression is that the text is dense and not particularly easy to get through. Perhaps the authors are simply trying to cram too many contributions into a single conference paper? This is a classic error which leads to hard-to-read papers. In addition to this, there is a lingering concern about the generalizability of the proposed methods.

I think the authors need to work more on their presentation, and perhaps reconsider which parts to include in their paper and exactly which measure they want to send, before they submit to another venue.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are generally not very enthusiastic about the paper, with scores hovering at or just below the acceptance threshold. The authors are not communicating clearly what the paper contributes and why."
"This paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. The technical parts in the original write-up were not very clear, as noted by multiple reviewers. During the review period, the presentation was improved. Unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"After the authors’ rebuttal and long discussion between reviewers and authors, the paper unanimously receives positive rates thanks to reasonable proposed ideas and thorough experiment evaluation. The camera-ready version may need to be updated to fully reflect reviewers’ comments and authors’ answers to them. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received positive rates thanks to reasonable proposed ideas and thorough experiment evaluation. The camera-ready version may need to be updated to fully reflect reviewers’ comments and answers to them."
"While some of the scores on this paper are mixed, even the negative reviews highlight the quality and interest of the work and have specific (and somewhat debatable) technical concerns.  Overall, the AE recommends accept, especially in light of the detailed and thoughtful responses during the rebuttal phase.

In the camera ready, the authors are encouraged to see if they can squeeze some of the new results (e.g., transfer learning attempt in Figure 6 and comparisons to Shapeflow) in the main body of the paper, where they're more likely to be noticed.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers expressed concerns about generalization of the approach to different datasets, although some concerns were raised regarding generalization were noted."
"This works considers limitations of rehearsal-based methods in the context of continual learning (classification and object detection). Rehearsal-based methods provide a strong baseline, but a loss in predictive performance arises when the memory is limited in size. The authors propose to leverage compression (JPEG) to increase the number of data (images) stored in the memory. The approach is evaluated in the context of an autonomous driving application.

The additional experiments conducted by the authors were highly appreciated and helped clarify open questions (e.g., class-incremental learning set-up, DPP objective to determine size of the memory, quantity vs quality of compressed data, etc.). The authors addressed the issues raised by three out of four reviewers, who did not have further comments. The remaining reviewer found that the methodological contributions of this paper, namely of using compression in the context of CL, was pretty straightforward. However, the authors addressed the concerns raised by the reviewer regarding the selection of the compression quality q as far as I am concerned and conducted additional experiments to further demonstrate the usefulness of the approach. I would encourage the authors to include this discussion in the final version of the paper. I would also encourage them to include the additional experiments they conducted with fixed memory size and amount of memory that can be saved.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers addressed the concerns raised by the reviewers regarding the selection of the compression quality q as far as I am concerned, and conducted additional experiments to further demonstrate the usefulness of the approach."
"Initially, the paper had mixed reviews (455).  The major concerns from the reviews were:

1. missing refs about unprojection. (K314)
2. quality advantage is not convincing, slightly better than FSM, while qualitative results show not obvious improvements. (K314)
3. visualize the depth maps as point clouds (K314)
4. what is the trade-off between resolution/memory, computation, and depth estimation? (K314, ZBww)
5. insufficient experiments (9Ef1)
6. comparison with monocular methods (9Ef1)
7. cubic vs spherical space? (9Ef1)
8. can it be trained on real data w/o GT supervision? (9Ef1)
9. how to handle collision of multiple pixel rays? (ZBww)
10. why use different MLPs to fuse ""overlap"" and ""non-overlap"" features? No ablation study on this. (ZBww)

The authors wrote a response to address these concerns, providing more qualitative results and ablation studies, as well as further explanations.  The reviewers were satisfied with the response, and K314 upgraded their rating to 6, while other reviewers maintained 5s. The reviewers appreciated the novel problem and the solution that can produce more consistent depth maps across views, and also synthesize depth maps in novel views. After reading the paper, the AC agrees with the reviewers, noting that the paper  addresses the limitations of the problem setup of previous work [13], thus developing a new line of research.  Thus, the AC recommends accept. The authors should prepare a revised version of the paper according to the reviews, rebuttal, and discussion. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were satisfied with the response, and K314 upgraded their rating to 6, while other reviewers maintained 5s."
This paper was reviewed by six experts and received all positive scores. AC feels this work is interesting and deserves to be published on NeurIPS 2022 dataset track. The reviewers did raise some valuable concerns that should be addressed in the final camera-ready version of the paper. The authors are encouraged to make the necessary changes in the final version.,"This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers did raise some valuable concerns that should be addressed in the final version of the paper."
"This paper describes how to use normalizing flows for selecting features in a way that controls the type-1 error by using a normalizing flow along with MCMC to sample from the null distribution. The majority of the reviewers were positive, however the most confident reviewer was negative. From taking a look at that reviewers concerns, I tend to agree with most of them.

The paper is titled knockoff-free, which means in the context of this paper that both 1) 1-bit p-values are not used and 2) The full knockoff property is not required, only sampling from complete conditionals are required. Most of the experiments compare knockoff methods to the proposed approach, so it's not clear if 1) 1-bit p-values are not great or 2) the model-X process/complete conditional sampling process is better with normalizing flows. The former point is known and the latter point on the best way to sample from the complete conditionals is really the value. 

If we take the paper as, 

1) complete conditionals are 1-D
2) MCMC can be used to sample from a 1-D unnormalized density
3) Simple MCMC won't be bad because the problem is 1-D

-> Any likelihood based deep generative model can be used to sample complete conditionals

then it's a solid paper.

On the other hand, the belief that flows are the correct choice versus other likelihood-based deep generative models is harder to take as there's only a comparison with a mixture density network used in the original HRT paper. Also from other uses of these models, different models are better in different situations. I'd suggest a heavy discussion in the paper on this point at the minimum. Maybe even a reframing of the paper is needed.

Finally, for the test statistic, the HRT may not be the best choice for work like this paper that studies the problems with estimating X-distribution. The  paper ""CONTRA: Contrarian statistics for controlled variable selection"" at AISTATS 2021 shows that the HRT test statistic is more sensitive to model-X estimation errors than a simple mixture statistic that doesn't give up much power. The choice of test statistic also merits some discussion in step 3.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, although some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers were positive, however the most confident reviewer was negative. The paper is titled knockoff-free, which means that both 1) 1-bit p-values are not used and 2) The full knockoff property is not required, only sampling from complete conditionals is required."
"The authors have introduced a method of data augmentation for image-based reinforcement learning that performs masking in the frequency domain, combined with techniques for stabilizing Q-learning, to achieve improved performance on a number of DMControl Generalization Benchmark tasks.

There was agreement among the reviewers that this work is novel and technically sound, and their concerns were mainly related to the breadth of tasks explored in the initial submission. During the review process the authors have gone to considerable effort to introduce new tasks (e.g. DrawerWorld, Robosuite and CARLA) and the improved performance of their method appears to generalize well. I believe that this work will be of broad interest to the RL community and recommend it for acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that this work is novel and technically sound, and their concerns were mainly related to the breadth of tasks explored in the initial submission. During the review process the reviewers have gone to considerable effort to introduce new tasks (e.g. DrawerWorld, Robosuite and"
"This paper proposes TopicGAN, a generative adversarial approach to topic modeling and text generation. TopicGAN operates in two steps: it first generates latent topics and produces bag-of-words corresponding to those latent topics. In the second step, the model generates text conditioning on those topic words.

Pros: 
It combines the strength of topic models (interpretable topics that are learned unsupervised) with GAN for text generation.

Cons: 
There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, and (3) novelty. Of these, the first two were the main concerns. In particular, R1 and R2 raised concerns about insufficient component-wise evaluation (e.g., text classification from topic models) and insufficient GAN-based baselines. Also, the topic model part of TopicGAN seems somewhat underdeveloped in that the model assumes a single topic per document, which is a relatively strong simplifying assumption compared to most other topic models (R1, R3). The technical novelty is not extremely strong in that the proposed model combines existing components together. But this alone would have not been a deal breaker if the empirical results were rigorous and strong.

Verdict:
Reject. Many technical details require clarification and experiments lack sufficient comparisons against prior art.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. Cons: There are three major concerns raised by reviewers: (1) clarity, (2) relatively thin experimental results, (3) novelty, and (3) novelty. The reviewers also raised concerns about insufficient component-wise evaluation (e.g., text classification from topic models) and insufficient GAN-based baselines."
"All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods.  I believe this paper will be of broad interest to different communities at ICLR.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers suggest acceptance of this paper, which reports the relationship between perceptual distances, data distributions, and contemporary unsupervised machine learning methods."
"In this paper, the authors propose the temporal quantile adjustments (TQA) that can improve longitudinal coverage and preserve cross-sectional coverage for the prediction interval built for regression on cross-sectional time series data. While previous works focus on either of the coverage guarantees, a major contribution of this paper is to achieve both. The research questions addressed in this paper are of critical importance for practitioners in relevant areas. The paper is well written. The presented approach has solid empirical support and decent theoretical guarantees. Including a simulation study to demonstrate the validity of the proposed approach under specific (and controlled) setup will further improve the paper.

","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors propose the temporal quantile adjustments (TQA) that can improve longitudinal coverage for the prediction interval built for regression on cross-sectional time series data. The paper has solid empirical support and decent theoretical guarantees. The research questions addressed in this paper are of critical importance for practitioners in relevant areas."
"Reviews were mixed here and all quite borderline. There are legitimate points raised for why this is being consistently given borderline ratings, with two in particular resonating with my own reading (novelty and comparisons with other methods). However, despite these issues the paper itself is a solid contribution, and I think could easily lead to others building on the core ideas and approach. The paper has also improved notably during revisions and at present does not have any fundamental flaws that should preclude publication. Therefore, I recommend acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were mixed here and all quite borderline. There are legitimate points raised for why this is being consistently given borderline ratings, with two in particular resonating with my own reading (novelty and comparison with other methods). However, despite these issues the paper is a solid contribution."
"The focus of this paper is to analyze an end to end network to reconstruct matrices originating from non-Euclidean data which are corrupted. The authors present an untrained network for this task. In the review period the reviewers raised a variety of concerns including concerns about novelty of the paper with respect to existing work, technical depth and clarity. The authors did not respond to these concerns. Therefore, I recommend rejection.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper was recommended for acceptance with minor revisions."
"TAP-Vid presents a benchmark that will be highly useful for tracking research for tracking arbitrary physical points on surfaces over long video clips. The reviews are all positive, with one reviewer raising ethical aspects in preparing the benchmark.  I find the rebuttal sufficient and adequate and hence recommend acceptance of the paper.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The paper introduces a transformer-based method for non-stationary time series forecasting. 
This research addresses a clear need, as acknowledged by the reviewers. Also, most reviewers found the method clearly described and the experiments compelling, demonstrating an improvement of the state of the art.

The reviewers asked questions about the baselines, evaluation methods and ablation studies. They also made requests related to clarifying the wording and some of the theory. The authors put in significant effort in addressing the comments, offering detailed responses to every reviewer. Only one of the reviewers responded during the discussion period, and the response came very late in the discussion period. However, I read the authors' response and concluded that they adequately addressed most issues raised by the reviewers.

As the model is in the Transformer space, and transformers have previously been shown to be state of the art on a number of tasks, I do not find it necessary to compare against other 'families' of methods. So I will consider that issue addressed as well.","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper considers a subset of dynamic problems, in which the optimal policy is a threshold-policy. The authors use this attribute to formulate tailored off-policy actor-critic algorithms, for both MDPs and RMABs which are gradient-based, so can utilize neural networks. They empirically compare their method to SOTA methods in three MDP domains and three RMAB parameterizations, the results show that their method, DeepTOP, performs better than the compared methods in all the experiments.

The paper is well written and the claims are correct.  The performance of DeepTOP compared to the other methods is impressive.  All four reviewers were on the positive side for acceptance.
","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors use this attribute to formulate tailored off-policy actor-critic algorithms, for both MDPs and RMABs which are gradient-based, so can utilize neural networks."
"This paper demonstrates compellingly that transformers are able to in-context learn simple function classes (e.g., linear functions), to the extend that they can recover solutions from algorithms like LASSO. The experiments are well designed and executed, which lead to surprising and intriguing results. While the paper does not provide any explanation for why transformers exhibit such capabilities, it will spur both empirical and theoretical work studying how transformers learn algorithms from in-context examples. Congratulations on a nice work!","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"Dear Authors,

The paper was received nicely and discussed during the rebuttal period. There is consensus among the reviewers that the paper should be accepted:

- The new result about query complexity of regression problem that the authors have added. Along with the result on 
 for (noisy) Vandemonde matrix, these make the paper lie above the accept bar.
- The authors have providing satisfying clarifications during the rebuttal that convinced reviewers to increase further their scores.

The current consensus is that the paper deserves publication.

Best AC","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have provided satisfying clarifications during the rebuttal period that convinced reviewers to increase further their scores. The current consensus is that the paper deserves publication."
"Meta Review: I find this paper to present an interesting perspective, analyzing the sampling bias in active learning and correcting for it.  There was some debate with Review YKx9 about the assumptions and rigor of the theory, but I found the author responses adequate.  I recommend acceptance.  The authors should take care to improve the readability of Figure 1.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper developed an accelerated gradient flow in the space of probability measures. Unfortunately, the reviewers think the practical usefulness of the proposed approach is not sufficiently supported by realistic experiments, and the clarity of the paper need to be significantly improved. The authors' rebuttal resolved some of the confusion the reviewers had, but we believe further substantial improvement will make this work a much stronger contribution. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were disappointed by the reviewers' rebuttal, but we believe further substantial improvement will make this work a much stronger contribution."
"Description of paper content:

The authors propose a dynamics model that can generalize to novel environments. The train and test MDPs have the same state and action spaces but different dynamics. Environment specific inference is achieved by estimating latent vectors Z that describe the non-stationary or variable part of the dynamics. These Z-s are inferred from trajectory segments in unlabeled environments. The Z-s are learned contrastively: Z-s from the same trajectory are pulled together, and Z-s from separate trajectories are pushed apart. However, to mitigate the error of distancing Z-s from different trajectories but the same environment, Z-s on trajectories with similar transitions are also pushed together using a soft clustering penalty. These losses are justified based on ideas from Pearl’s causal inference.

Summary of paper discussion:

The reviewers concluded that the contributions are conceptually interesting and “somewhat” novel. The reviewers felt that the empirical performance gains of the method over baselines were demonstrated but not extremely impressive.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers concluded that the contributions are conceptually interesting and “somewhat” novel."
"In an attempt to understand generalization, this paper aims at understanding the dynamics of functions presented by the network for different images in the training set. Authors look at activation patterns (whether a ReLU activation is on or off) as a way of characterizing the active paths in the network and approximating the function presented by the network for each image. Authors study different related statics (eg. correlation) and how they evolve during training including.

Pros: 
- Understanding the dynamics of training, how diversity is encouraged by the training procedure and its relationship to generalization is an important problem.
- This paper takes an empirical approach and tries to make interesting empirical observations about the dynamics of the training.

Cons:
- The paper is poorly written in terms of structure, making clear arguments with enough evidence, notation, etc.
- Some empirical trends are shown but their connections to the main claim of the paper about generalization is very weak. The main attempt to connect the observations to generalization is Fig. 7 which shows model accuracy correlated with the ratio of early to mid overlap. This is problematic both because it only has 6 data points and also because a simple correlation analysis is not enough to establish this claim which is more about the cause of generalization.

Reviewers have pointed to various concerns including but not limited to clarity of the paper, lack of rigorous arguments, not providing enough evidence for the arguments, etc. Unfortunately, authors did not participate in the discussion period.

Given the above concerns, I recommend rejecting the paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted some concerns about generalization, although some concerns were raised regarding the generalization of the approach to different datasets."
"I'm quite concerned by the conversation with Anonymous, entitled ""Why is the dependence..."". My issues concern the empirical Rademacher complexity (ERC) and in particular the choice of the loss class for which the ERC is being computed. This  class is obviously data dependent, but the Reviewers concerns centers on the nature of its data dependence. It is not valid to define the classes by the Jacobian's norm on the input data, as this _structure_ over the space of classes is data dependent, which is not kosher. The reviewer was gently pushing the authors towards a very strong assumption... i'm guessing that the jacobian norm over all data sets was bounded by a particular constant. This seems like a whopping assumption. The fact that I can so easily read this concern off of the reviewer's comments and the authors seem to not be able to understand what the reviewer is getting at, concerns me.

Besides this concern, it seems that this paper has undergone a rather significant revision. I'm not convinced the new version has been properly reviewed. For a theory paper, I'm concerned about letting work through that's not properly vetted, and I'm really not certain this has been. I suggest the authors consider sending it to COLT.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were gently pushing the authors towards a very strong assumption."
"Reviewer KVLA raises some concerns on the completeness of this paper, suggesting that this paper needs further improvements for publication. Based on the comments of both reviewers, this paper is accepted as a long paper. Please address the reviewers' comments in the final version.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper provides a novel and non-trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Eigenvectors of the Laplacian have been used for proto-value functions and eigenoptions, but it has remained an open problem to extend their use to the non-tabular case. This paper makes an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Laplacian. 

The paper could be made stronger by including a short discussion on why the limitations of this approach. Its an important new direction, but there must still be open questions (e.g., issues with the approach used to approximate the orthogonality constraint). It will be beneficial to readers to understand these issues.","This paper proposes a novel and non-trivial method for approximating the eigenvectors of the Laplacian, in large or continuous state environments. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. It is an important advance towards this goal, and will be of interest to many that would like to learn state representations based on the geometric information given by the Lalacian."
"Interesting approach aiming to leverage cross domain schemas and generic semantic parsing (based on meaning representation language, MRL) for language understanding. Experiments have been performed on the recently released SNIPS corpus and comparisons have been made with multiple recent multi-task learning approaches. Unfortunately, the proposed approach falls short in comparison to the slot gated attention work by Goo et al.

The motivation and description of the cross domain schemas can be improved in the paper, and for replication of experiments it would be useful to include how the annotations are extended for this purpose.

Experimental results could be extended to the other available corpora mentioned in the paper (ATIS and GEO).
","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The proposed approach falls short in comparison to the slot gated attention work by Goo et al."
"The main topic of this work is stochastic bilevel optimization. It provides an efficient algorithm for this task, and provides theoretical results in this setting.  

The reviewers are unanimous that this is well-presented work of high quality and should be accepted, and so do I.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main topic of this work is stochastic bilevel optimization. The reviewers are unanimous that this is well-presented work of high quality and should be accepted, and so do I."
"This paper addresses a promising and challenging idea in Bayesian deep learning, namely thinking about distributions over functions rather than distributions over parameters.  This is formulated by doing MCMC in a functional space rather than directly in the parameter space.  The reviewers were unfortunately not convinced by the approach citing a variety of technical flaws, a lack of clarity of exposition and critical experiments.  In general, it seems that the motivation of the paper is compelling and the idea promising, but perhaps the paper was hastily written before the ideas were fully developed and comprehensive experiments could be run.  Hopefully the reviewer feedback will be helpful to further develop the work and lead to a future submission.

Note: Unfortunately one review was too short to be informative.  However, fortunately the other two reviews were sufficiently thorough to provide enough signal.  ","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"Based on the paper, reviewers' comments and discussions, and the responses, the meta-reviewer would like to suggest the authors to improve the paper and resubmit.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper proposes an approach for learning a decomposition of a scene into 3D objects using single images without pose annotations as training data. The model is based on Slot Attention and NeRF. Results are demonstrated on CLEVR and its variants. 

The reviewers point out that the method is reasonable and the paper is quite good, but even after considering the authors' feedback agree that the paper is not ready for acceptance. In particular, the key concern is around experimental evaluation - that it is performed on one dataset (and variants thereof) and that the evaluation of the 3D properties of the model is not sufficiently convincing: it does not outperform 2D object learning methods on segmentation and is not compared to those on ""snitch localization"".

Overall, this is a reasonable paper, and the results are promising but somewhat inconclusive, so I recommend rejection at this point, but encourage the authors to improve the paper and resubmit to a different venue.

(One remark. The paper makes a point of not using any annotation. It is technically true, but in practice on CLEVR unsupervised segmentation works so well that it's basically as if segmentation masks were provided. If the authors could demonstrate that their method - possibly with provided coarse segmentation masks - works on more complex datasets, it would be a nice additional experiment)","The paper proposes an approach for learning a decomposition of a scene into 3D objects using single images without pose annotations as training data. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers point out that the method is reasonable, and the results are promising but somewhat inconclusive, so I recommend rejection at this point, but encourage the authors to resubmit to a different venue."
"The authors present a method called ""AdaRL"" that learns a structured latent representation that characterizes relationships between different variables in an RL system. The method is evaluated on modified Pong and Cart-Pole domains and it is shown to outperform other transfer learning baselines. The reviewers agree that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments. The reviewers also point out that the evaluated domains are rather simple and the paper would benefit from evaluations in a more complex environment as well as better writing. Please focus on improving these aspects in the final version of the paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the method makes sense and addresses an important problem of transfer in RL. The authors did a good job in the rebuttal to empirically validate their claims and provided extra experiments."
"The reviewers indicated a number of concerns (which I agree with) which have not been addressed by the authors as they have not provided any response.  Indeed, the paper would be significantly improved once these issues are addressed. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper provides an interesting generalized perspective on SSL techniques and proposes a debiasing technique that can be viewed as decreasing the variance of the risk estimate. The authors argue that this leads to estimators that are better than the purely supervised estimator under a rather weak assumption called MCAR - which assumes that the probability of a missing label is independent of covariate and label. 

Although the exposition and perspective are interesting, this paper is borderline for the following two main reasons:

1. A few reviewers were not convinced of the theoretical result that is supposed to show that unlabeled data strictly helps - indeed, whereas usual variance reduction techniques (e.g for optimization schemes such as SGD etc.) lead to a strict gain in terms of convergence rate, there is no clear asymptotic/high probability statement that indicates statistical gain of the corresponding estimator (which is what we really care about - not the risk estimate). The dependence of lambda_opt on theta (which changes every iteration) does not help in providing such a statement. Since this is the primary contribution of the paper, I would suggest the authors follow through with the analysis to show a gain for the actual estimator compared to the ""complete case"" (only using supervised data). 

On that note, the authors claimed in their rebuttal that they have added an asymptotic variance analysis in Appendix I, which indeed would have made a very valuable point - however, I could only find a copy-pasted version of Theorem 3.1. in Appendix I? Similarly, Appendix F does not seem to include the comparison between debasing using labeled and unlabeled data but instead contains the proof of Theorem 3.2. Perhaps the wrong revision was uploaded, but unfortunately, given the current version, this point is not adequately addressed.

2. If the experimental results were more extensive and conclusive, then the current theorem could have perhaps been alright as a mainly methodological contribution. However, as the authors note, extensive experiments require a lot of compute power - however, given the lack of the ultimate theorem, the methodology becomes the primary contribution and would thus require more experimental evidence as the reviewers asked for.
  
Addressing one of the above points would push the paper above the acceptance threshold which we hope the authors can pursue in their next submission. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors argue that this leads to estimators that are better than the purely supervised estimator under a rather weak assumption called MCAR - which assumes that the probability of a missing label is independent of covariate and label."
"Meta Review: The paper presents a new means to learn sparse Choquet integral, using for instance classical L_1 penalization techniques. This research participates to a long trend of research aiming at learning preference functions in various settings (here, having only limited preferential information). 

All reviewers agree that the paper is well-written and the contribution nicely presented. Two main critics were done by the reviewers:

* The first is the potential significance of the research, i.e., that it addresses an actual real-world problem. After discussion and rebuttals, this critic appears to be of less importance

* The second (with which I would concur) is that the experimental part is limited in different ways. The main critics regarding those are as follows:
1. The authors only use synthetic data that mostly comply with their assumptions, thereby confirming that their approach is valid. THey do not really challenge it.
2. There is no realistic data sets used (which is very difficult to obtain in incremental preference learning, but less in passive preference learning).
3. There is also no real comparisons with other methods intending to learn value functions from observed preferences. Given the large literature on preference learning and the fact that most learning papers require the comparisons with other baselines when possible, one could have hoped to have such comparisons. 

Depsite that, the paper appears to be well-made with a relevant algorithmic contribution. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that the paper is well-written and the contribution nicely presented."
"This work presents a method to obtain slide level representations in computational pathology. The specific contributions of this work are a positive-negative-aware module (PNM) and a weakly-supervised cross-slide contrastive learning (WSCL) module and a loss to encourage intra-WSI local patch separation and inter-WSI global feature contrast. The idea is to use the attention weights in a MIL framework as patch level pseudo labels. These are used to compute ""weights"" for positive and negative patches (the latter of which is assumed to be significantly larger in number for a given WSI). By using these weights in a contrastive manner to push the representations from positive patches away from those of negative patches, the model learns more effectively since it is less susceptible to the noise from the negative patches.

The reviewers found these contributions novel. During the review, the largest source of concern was along the choices made during the empirical evaluation. Specifically, the manuscript in its current form lacked empirical backing for several of the choices made regarding the neural architecture, the algorithm for self-supervised learning etc. In response to this the authors conducted several different kinds of ablation studies (which were incorporated into the supplement) during the rebuttal process, which other reviewers found convincing as a potential explanation of the outcomes.

Overall, I found the main contributions of this work (leveraging patch level attention weights as psuedo labels) to be an interesting use case for computational pathology to better focus the learning signal on positive patches. My additional comment is that I think the additional ablation experiments (and references) are an important part of the contributions of this work and should be incorporated into the main paper rather than in the appendix (several equations can be compressed to make space in the manuscript in addition to the additional page). ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main contributions of this work are a negative-negative-aware module (PNM) and weakly-supervised cross-slide contrastive learning (WSCL) module and a loss to encourage intra-WSI local patch separation and inter-WWI global feature contrast."
"This work studies the effect of initialization scaling on the gradient flow training dynamics of three layer (two hidden layer) MLPs. . Extending Luo et al. 2021's analysis, the authors identify ""linear"", ""critical"" and ""condensed"" regime depending on initialization scaling parameters

Strength pointed out by the authors include, ""new insight into training dynamics of MLPS with more than one hidden layer"", ""elegant and insightful analysis"" for a ""dauntingly difficult problem"" and that ""phase diagram is neat and valuable contribution""

Reviewer `PDY9` saw major weakness as treating the second layer as linear regime only, which makes the analysis similar to previous work (Luo et al., 2021). In general, the reviewer believes that the full comprehensive analysis on different phases for each layer is lacking. Although the reviewer did not respond to the AC, this issue has been addressed by author response. Also please do follow the recommendation of the reviewers in terms of improving writing (better description of gamma_2, gamma_3 for example), improving related works, as well as making figures more legible. 

Overall, while the paper is somewhat borderline, there are interesting insights and analysis as the reviewers pointed out without critical issues. I recommend accepting this work at NeurIPS 2022. 
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted its potential for significant improvements in the field, although some concerns about generalization were raised regarding the generalization of the approach to different datasets."
"All three referees have provided detailed comments, both before and after the author response period. While the authors have carefully revised the paper and provided detailed responses, leading to clearly improved clarity and quality, there remain clear concerns on novelty (at least not sufficiently supported with ablation study) and experiments (neither strong enough nor sufficient to support the main hypotheses). The authors are encouraged to further improve their paper for a future submission.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."""
"The paper describes a genetic algorithm for molecular optimization under constraints. The aim is to generate molecules with better properties while close to an initial lead molecule. The proposed approach is a two-stage one. The first stage aims to satisfy constraints and searches for feasible molecules that are similar to the lead. The second stage optimizes the molecular property. The method is evaluated on logP optimization task, with minor improvement over previous work.

The reviewers point out the following strengths and weaknesses:

Strengths:

- Molecular optimization under structural constraints is an important research direction.
- Comprehensive related work section.

Weaknesses:

- Lack of novelty because it is a standard application of genetic algorithm.
- The results show that the proposed method did not outperform existing baselines.
- The main claim of the paper (benefit of two-stage procedure) is not supported by ablation study.
- The authors only conduct experiments on improving LogP, which is a benchmark that is too easy and not challenging.
- The objective function and cross-over operation are the same or very similar to previous work.
- The experimental evaluation is limited, and the overall setting is not very relevant to real-world tasks.

Overall, all reviewers vote for rejection. It is clear that the paper needs more work before it can be published.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors only conduct experiments on improving LogP, which is a benchmark that is too easy and not challenging. The main claim of the paper is not supported by ablation study."
"This was a somewhat unusual submission in that the authors tried to motivate their paper by pointing to a separate anonymous manuscript.  However, the authors didn't seem to want to confirm they would merge the manuscripts when asked about this. It was thought that in fairness the submitted manuscript should be judged on its own. After discussion, it was agreed that the submitted paper on its own, did not generate enough enthusiasm to merit acceptance.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"The paper describes the use of tactile sensors for exploration.  An important topic which has been addressed in various previous publications, but is unsolved to date.

The research and the paper are unfortunately in a raw state.  Rejected unanimously by the reviewers, without rebuttal chances used by the authors.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were not satisfied with the reviewers' feedback."
"This paper got mixed reviews. One for reject and three for acceptance. The reviewers and authors have extensive discussion. Authors also provided additional experiments for further clarifying some questions from the reviewers. The paper has some clarify issue in the theoretical justification part as pointed out by AR1. Authors should extensively improve this part or revise the statement. However, the method proposed in this paper is simple and the results are indeed good. This paper is valuable and should be shared within the community to advance research on ZSL. Therefore, AC recommends acceptance. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions. The reviewers and authors have extensive discussion."
The paper provide a good and exciting improvement over LSH based widely used Falcon Library. All the reviewers found the contribution worthy of publication. ,"Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"The reviewers, AC, and PCs participated in a very thorough discussion. AC ultimately felt that the work was unfinished, and in particular that details in the proofs still needed work before publication.

","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"This meta review is based on the reviews, the authors rebuttal and the discussion with the reviewers, and ultimately my own judgement on the paper. There was a consensus that the paper contributes interesting insights on uncertainty quantification, and most reviewers praised several aspects of the submission. I feel this work deserves to be featured at NeurIPS and will attract interest from the community. I would like to personally invite the authors to carefully revise their manuscript to take into account the remarks and suggestions made by reviewers. Congratulations!","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."""
The reviewers came to consensus that this paper makes a good contribution to the study on the tail behavior of the regret of bandit problems. I agree with these opinions and please polish the paper so that the minor concerns raised by the reviewers become clear in the final version.,"Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"Overall, this paper provides a great starting point for future benchmarking experiments. The reviewers engaged in a lively discussion with the authors and provided valuable suggestions for future improvements, which the authors have integrated in their submission. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The paper proposes a framework for characterizing harmful text generated from LLMs. This is a timely and extremely important topic, and the paper engages with the complex socio-technical questions around it. The reviewers are split in their opinions (three in favor of acceptance; three opposed). The key argument raised against the paper is that it does not introduce a new benchmark or dataset, and does not make falsifiable claims. The meta-reviewers consider the paper to be well in-scope for the track: despite lacking empirical results, it provides a ""framework for responsible dataset development"" and clearly identifies ""significant problems with existing datasets.""  The authors successfully address the other concerns. The paper makes an important contribution to the community and deserves to be accepted.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are split in their opinions (three in favor of acceptance; three opposed). The main argument raised against the paper is that it does not introduce a new benchmark or dataset, and does not make falsifiable claims."
"The authors propose training-free neural architecture search using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure ""trainability"" of the architecture), and the number of linear regions in the input space (to measure ""expressivity""). These two heuristics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training-free Neural Architecture Search. It is certainly not the first training-free NAS proposal, but achieves competitive results with much more expensive NAS methods.

A few reviewers mentioned limited novelty of the method, a claim with which I agree. The contribution of the paper, however, is something different than how it was presented. The core message seems to be that the two proposed heuristics can greatly speed up NAS, and should be a baseline method against which more expensive methods should test.

I feel like this is a borderline paper, but may be of interest to researchers in the field.","The paper proposes a novel approach to neural network training using two theoretically inspired heuristics: the condition number of the Neural Tangent Kernel (to measure ""trainability"" of the architecture), and the number of linear regions in the input space. The two heuriistics are negatively and positively correlated with test accuracy, respectively, allowing for fast, training-free Neural Architecture Search. The authors propose a new method to train-free neural architecture search using a combination of theoretically-inspired e.g.,"
"During the discussion among reviewers, we have shared the concern that this work has a significant overlap with [Liu et al. 2018] and [Liu & Motani 2020]. Although the authors tried to address this concern by the author response, I also think that the difference is not enough. In particular, the reviewers pointed out that Figure 1, Table 1, and Figure 3 are exactly the same with those in [Liu, 2020], and Proposition 2 in [Liu & Motani 2020] is Proposition 1 in this paper. Since these overlaps are not acceptable, I will reject the paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about the generalization of the approach to different datasets."
"This paper studies the underlying training and memorization dynamics of very large language models. The main take aways are that larger-sized language models memorize training data faster, and that this memorization happens before the overfitting of language modeling. Tokens with certain part-of-speech tags (nouns, numerals) seem to be memorized faster during training. 

Overall, most reviewers feel positively about this paper, agreeing that it tackles an important problem and that it provides a solid contribution. The experimental results are detailed and use reasonable metrics for data memorization, including the forgetting identifier experiments. Some of the weaknesses that have been pointed out (e.g. regarding the significance of the part-of-speech tags experiment, clarifying the criteria for memorization, etc.) seem to have been well addressed during the author response. Therefore, I recommend acceptance.

","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers feel positively about the paper, agreeing that it tackles an important problem and that it provides a solid contribution."
"This paper proposes a novel method, IterefineE, for cleaning up noise in KGs. This method combines the advantages of using ontological information and inferences rules and KG embeddings with iterative co-training. IterefineE improves the task of denoising KGs on multiple datasets. While the importance of multiple iterations is mixed, reviewers agree that the combination of two significantly different types of reasoning is a promising direction.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers agree that the combination of two significantly different types of reasoning is a promising direction."
"The authors demonstrate that complete neural network verification methods that use limited precision arithmetic can fail to detect the possibility of attacks that exploit numerical roundoff errors. They develop techniques to insert a backdoor into networks enabling such exploitation, that remains undetected by neural network verifiers and a simple defence against this particular backdoor insertion. 

The paper demonstrates an important and often ignored shortcoming of neural network verification methods, getting around which remains a significant challenge. Particularly in adversarial situations, this is a significant risk and needs to be studied carefully in further work.

All reviewers were in agreement on acceptance and concerns raised were adequately addressed in the rebuttal phase, hence I recommend acceptance. However, a few clarifications raised by the official reviewers and public comments should be addressed in the final revision:
1) Acknowledging that incomplete verification methods that rely on sound overapproximation do not suffer from this shortcoming.
2) Concerns around reproducibility of MIPVerify related experiments brought up in public comments.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were in agreement on acceptance and concerns raised by the official reviewers and public comments should be addressed in the final revision."
"This paper proposes a novel type of variational auto encoder, referred to as HELO. The latent space is decomposed into a content space and a motion space, and the main contribution is the proposal to model the motion space using Hamiltonian dynamics. All reviewers agree that the idea of using Hamiltonian dynamics is interesting and novel. One main critique, that the authors agreed on, was that the operator does not contain any stochasticity and that this might be a limitation when applying the idea to model more complex data. Another remark was that the experiments are limited and experiments on less constraint data are missing. A quick look at the baseline methods revealed that they also use the same kind of data sets to evaluate their methods, so this latter concern might be of minor importance. 
All in all, the potential positive outcomes of this paper outweight its current limitations, so we recommend acceptance at this point, while urging authors to address the remaining concerns in the final version. 
","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that the idea of using Hamiltonian dynamics is interesting and novel. The main contribution is the proposal to model the motion space using HELO. The latent space is decomposed into a content space and a motion space, and the main contribution to the proposal is the"
"This paper introduces SUNMASK for modeling discrete sequences. It builds upon previous works such as SUNDAE, Coconet and order-agnostic NADE, but uses a masking scheme that enables fine-grained or human-in-the-loop control during the generation. The qualitative experiments about musical inpainting and masking terms in language modeling do support this motivation to some extent. However, the reviewers are mainly concerned with both the algorithmic novelty and experiments of the paper.
 
Regarding the algorithmic novelty, some reviewers are concerned that the method is a straightforward combination of SUNMASK and Coconet. I tend to agree with this. Also the reviewers are concerned that the paper could have done a better job in the introduction and background section by putting the method in a better context and better describing related methods such as SUNDAE. This could help highlight the novelty of the paper.
 
Regarding the experiments, some reviewers are concerned about language modeling (Fig. 2) experiments, and that they do not show much improvement over SUNDAE. I tend to agree, and this is also shown in the results of Table 3 where it is clear that the quality of the language model is not on a par with the recent developments.
 
Also some of the motivations of the paper, especially the arguments about ""high trust / low trust"" interpretation of the mask, was unclear to me.
 
In short, I believe the paper should be clarified and improved by addressing the above concerns.","This paper introduces SUNMASK for modeling discrete sequences. It builds upon previous works such as SUNDAE, Coconet and order-agnostic NADE, but uses a masking scheme that enables fine-grained or human-in-the-loop control during the generation. The reviewers are concerned about the novelty and experiments of the paper, and that they do not show much improvement over SUNDE. Overall, the reviewers received a positive recommendation with minor revisions."
"This paper proposed a long-term object-based memory system for robots.  The proposed method builds on existing ideas of data association filters and neural-net attention mechanisms to learn transition and observation models of objects from labelled trajectories.  The proposed method was compared with baseline algorithms in a set of experiments.

The initial reviews raised multiple concerns about the paper. Reviewers nrGQ and  V7qP commented on the conceptual gap between the problem proposed in the introduction and the extent of the experiments. Reviewer qPet understood the paper to be a form of object re-identification and was concerned about the limited comparisons with related work.  The author response clarified their goal of estimating the states of the objects in the world, which they state is different from the goals of long-term tracking and object reidentification mentioned by the reviewers.  The authors also clarified the relationship to other work in slot-attention and data association filters.  

The ensuing discussion among the reviewers indicated that the paper's contribution remained unclear even after the author response. Two reviewers noted the paper did not clearly communicate the problem being solved (all reviewers had a different view of the problem in the paper).  These reviewers wanted a better motivation for the problem being addressed in this paper.  The third reviewer remained unconvinced that the problem in the paper was different from long-term object tracking.

Three knowledgeable reviewers indicate reject as the contributions of the paper were unclear to all of them. The paper is therefore rejected.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, although some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers were concerned about the conceptual gap between the problem proposed in the introduction and the extent of the experiments. The author response clarified their goal of estimating the states of the objects in the world, which they state is different from the goals of long-term object tracking and object reidentification"
"Meta Review: This paper proposes LIGER to better use the foundation model in a weakly-supervised setting. LIEGER has two key components: 1) a finer estimator of weak source quality by dividing the embedding space and learning the source accuracy for each part, 2) source voting expansion  in the embedding space.

The reviewers agree that the proposed work is innovative and makes a significant technical contribution in using pretrained models under weak supervision. There were some concerns raised by reviewers, but most of them have been well addressed by the authors.  I recommend acceptance of this paper given its novelty and significant technical contribution.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the proposed work is innovative and makes a significant technical contribution in using pretrained models under weak supervision."
"This paper addresses the problem of how best to sample hard negatives during contrastive learning, a topic of importance for the recently resurgent field of metric learning / contrastive loss-based unsupervised representation learning. Backed by theoretical results for a new low-variance version of the NCE, the paper proposes an easy-to-implement ""Ring"" method for selecting negatives that are at just the right level of difficulty, neither too hard nor too easy.

Happily, this is a paper that has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the authors. Perhaps the result that tipped this paper over the line in my assessment: the new experimental results now show significant gains from applying the ""Ring"" approach for hard negative sampling to near-state-of-the-art implementations of the MoCo-v2 approach, which is among the leading unsupervised visual feature learning approaches. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the reviewers."
"This paper presents ODConv, a convolution pattern which uses attention in the convolutions across all dimensions of the weight tensor.
The paper is well motivated and well explained, easy to follow.
This work is built on top of previous work, but reviewers all agree that the contributions of this paper are significant.
The experimental section is comprehensive, with several benchmarks, and show clear improvements.
The reviewers suggested a few additional remarks, and discussions to add to the paper, which the authors have addressed in the rebuttal. Reviewers seem in general happy with the authors answers to their concerns.
This seems like a sound and meaningful paper. I am fully in favour of acceptance, and I recommend this paper to be presented as a spotlight.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The experimental section is comprehensive, with several benchmarks, and show clear improvements. The reviewers agree that the contributions of this paper are significant. The paper is well motivated and well explained, easy to follow."
"The review ratings/confidences were 5/2, 6/2, 4/4, and 5/2. Although the average rating of 5 was just above the acceptance threshold, I think that it should somehow be discounted by the lower confidence levels. Although I myself does not have expertise in the field of BCI, as for the reviewers' evaluation, I think that they basically agreed on the following points:
- The problem is well motivated.
- The proposed method was built on DyEnsemble with some empirically-motivated extensions to have decoder models dynamically evolving. One could then argue that the proposal is not groundbreaking but somehow incremental.
- The authors showed experimentally that the proposed method works well compared with a number of other existing methods.

I also noticed that the authors made revision (adding a paragraph at the end of Section 2: It can be observed in the August 10 revision), which would have improved readability of this paper. I would thus recommend acceptance of this paper, provided that there is room for it.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers showed experimentally that the proposed method works well compared with other existing methods, although some concerns about generalization were raised regarding generalization."
"This paper aims to study the convergence of deep neural networks training via a control theoretic analysis. This is a very interesting approach to establish theoretical understanding of deep learning. However, there are several concerns raised by the reviewers:

1.	The contribution of this paper is limited. The results simply follow from standard optimal control. It is not clear what new insight the paper provides.
2.	There are already quite a few works on control theoretic analysis of deep learning. This paper did not do a good job on presenting its novelty and difference with existing works.
3.	The experimental part is weak. It only involves small data set and very simple networks.

Based on these, I am not able to recommend acceptance for the current manuscript. But the authors are encouraged to continue this research.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. However, there are several concerns raised by the reviewers regarding the generalization of the approach to different datasets."
"This paper studies the problem of choosing the best cloud provider for a task. The problem is formulated as a bandit and solved using algorithm CloudBandit. The algorithm is compared to several baselines, such as SMAC, and performs well. The evaluation is done on 60 different multi-cloud configuration tasks across 3 public cloud providers, which the authors want to share with the public.

This paper has four borderline reject reviews. All reviewers agree that it studies an important problem and that the promised multi-cloud optimization dataset could spark more research in the area of cloud optimization. The weaknesses of the paper are that it is not technically strong and that the quality of the new dataset is not clear from its description. At the end, the scores of this paper are not good enough for acceptance. Therefore, it is rejected.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the paper studies an important problem and that the promised multi-cloud optimization dataset could spark more research in the area of cloud optimization. The authors agree that this paper studies the problem of choosing the best cloud provider for a task."
"This paper introduces a new dataset for evaluating disentanglement and its impact on out of distribution generalization based on the trifinger robotics platform. Using this dataset, the authors rigorously investigate the performance of beta-VAEs in this setting under a number of conditions, finding that weak supervision is necessary to induce disentangled representations, and that, perhaps surprisingly, disentanglement does not help for sim2real settings despite the similarity between the simulator and the real data. Reviewers were divided on the work, but had a number of concerns related to the claims of novel architecture, comparisons to baselines, and issues with the clarity of the paper, some of which were addressed in the authors' response. I agree with some of these concerns, particularly with respect to the claims of novel architectures since the modifications could simply be viewed as tweaking hyperparameters and are not rigorously compared to baselines. However, I think the novelty of the dataset and the rigorous evaluation of OOD generalization settings is likely to be valuable enough to the community to merit acceptance. I'd encourage the authors, however, to tone down some of the claims regarding the architecture (or provide sufficient baseline comparisons), and instead focus on the dataset and the OOD results. I recommend acceptance. ","This paper introduces a new dataset for evaluating disentanglement and its impact on out of distribution generalization based on the trifinger robotics platform. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were divided on the work, but had a number of concerns regarding the claims of novel architecture, comparisons to baselines, and issues with the clarity of the paper."
"Overall, all reviewers see the novelty in the dataset that provides an original new set of tactile data, accounting for a wide range of objects and scene, collected in the wild. The paper is overall well written, clear and easy to follow, and provides a good demonstration of the importance and usefulness of the dataset, going beyond datasets collected by robots in the context of material properties. It also includes initial steps towards benchmarking, although it is not the core contribution of the paper, which lies in the creation of the dataset itself. 

Two reviewers recommend acceptance, two stand marginally below acceptance, and one does recommend rejection. The authors have provided detailed responses to all reviewers’ comments and concerns and updated their manuscript. The main concerns of those reviewers sitting on the fence seem to be answered and limitations acknowledged however the element of bias in the data collection was still considered as a possible concern. Based on my reading and accounting for the authors changes, there approach itself is novel and original and hence provides already a relevant contribution to NeuroIPS; while the authors acknowledge limitations and future work avenues. The one reviewer recommending rejection didn’t engage in the discussion but from my reading of both the paper, the comments, and the authors responses, I believe the main points were addressed by the authors and misunderstandings clarified. Overall, I believe the papers offers a novel, non-existing dataset that can inspire interesting future works; all the dataset is shared and presented clearly for others to use. 
","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have provided detailed responses to all reviewers’ concerns and concerns and updated their manuscript. The authors acknowledge limitations and future work avenues."
"This is a strong submission that benefitted greatly from productive and clarifying discussion between the authors and reviewers, after which the reviewers reached a unanimous stance in favor of acceptance. I recommend the authors to revise the manuscript accordingly in light of these discussions. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The work proposes a continuous-time generalization of diffusion models on a discrete space. The description uses continuous-time Markov chain (CTMC), in parallel to the existing stochastic differential equation description for continuous space. Reverse CTMC and modeling and ELBO objective are described. Some practical considerations and inspirations are also discussed, including avoiding exponentially large model in high dimensions, efficient reverse (generation) process simulation, and a corrector technique that further exploit the model to improve simulation (generation) quality. An error bound on the learned data distribution is also presented that shows a mild dependency on data dimensionality.

All the reviewers agree that this work presents the very right way to describe the continuous-time version of diffusion model on discrete space, and thereafter inspired techniques make a desired contribution to the community. Some concerns are raised, including still inferior performance than the continuous counterpart, and on the independence among dimensions. The authors provide reasonable remarks on them. Hence, I recommend accept to this paper.

One minor point: In Sec. 4.2, it would be clearer if the independence is specified both among the random variables $x^{1:D}$ in “output” and between each $x^d$ in “output” and $x^{1:D\backslash d}$ in “input”. Conventionally independence refers to the former, in which case the size is only reduced to $S^D \times D S^2$.
","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that this work presents the very right way to describe the continuous-time version of diffusion models on discrete space, and thereafter inspired techniques make a desired contribution to the community. Some concerns are raised, including still inferior performance than the continuous counterpart, and on the independence among dimensions."
"The major concerns about this paper are that (1) There are too many hyper-parameters, such as those needed for ADMM. I'd point out that there are adaptive variants of ADMM and heuristics methods for choosing optimization hyper-parameters, although it would be nice if the authors addressed these issues in the paper.  (2) Some reviewers are concerned that, compared to other related attacks, it’s unclear why flipping fewer bits is an important objective - an attacker might only care about poisoning performance and clean data performance.  The authors respond that flipping fewer bits makes the attack more effective when bits are manipulated by a physical method such as manipulating memory.  Despite these criticisms, reviewers agree that the paper is a well thought-out approach that improves the state of the art by some metrics.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The major concerns about the paper are that (1) There are too many hyper-parameters, such as those needed for ADMM."
"This paper proposes an OOD evaluation framework under three categories: irrelevant input detection, novel class detection, and domain shift detection. As with several reviewers, the AC recognizes the importance and effort to distinguish between different cases of OOD detection, as well as the amount of experimental comparison across several prominent methods in literature (MSP, MC-dropout, cosine similarity, ODIN, Mahalanobis).  

Despite being well-motivated, three knowledgeable reviewers find the paper not ready yet for publication at ICLR. The AC recommends a rejection, given the standing major concerns from the reviewers. The AC is hopeful that the paper can be significantly improved by 

- sufficiently discussing and highlighting the novel insights of the results. 
- a more rigorous definition of  ""novel"" vs. ""irrelevant"" inputs. There seem to be overlapping definitions between what Hsu et al. considered vs. this paper. In particular,  Hsu et al distinguish i) samples of a novel class but in a known domain, called semantic shift (S), and ii) samples of known class in a novel domain, called non-semantic shift (NS), both of which are reconsidered in this paper. Therefore, the novelty of this submission is more precisely to distinguish within the category of semantic shift. The AC agrees that this might deem some more rigorous measurement and definition of ""semantic closeness"". 
- The AC also finds the evaluation of domain shift in Section 3.3.2 may be potentially misleading the community, as it falls out of the standard OOD scope. The notion of common corruption is closer to the robustness problem (which is how ML model predictions changes w.r.t some delta changes in the input space). The changes may not be substantial enough to be ""out-of-distribution"".  ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The AC is hopeful that the paper can be significantly improved by - sufficiently discussing and highlighting the novel insights of the results, and - a more rigorous definition of ""novel"" vs. ""irrelevant"" inputs."
"The paper is a nice addition to the developing theory of implicit bias in neural training. While the results are somewhat expected, the technical aspects are fairly involved due to the adversarial component.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is a nice addition to the developing theory of implicit bias in neural training. The results are fairly involved due to the adversarial component."
"Dear authors,

All reviewers pointed out the fact that your result is about the expressivity of the big network rather than its accuracy, a result which is already known for the literature.

I encourage you to carefully read all reviews should you wish to resubmit this work to a future conference.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
This paper proposed an interesting method to induce a low-dimensional feature space for interpretable image classification. The proposed method is supported by comprehensive and strong experimental evaluations. Please consider the reviewers' comments in the final version. One common concern is it is unclear if the paper has novel technical innovation compared to the prior works.,"This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"Motivated by advancing the applicability of backpropagation alternatives, the paper extends predictive coding to non-Gaussian distributions, so it can be used to train effectively complex architectures such as transformers.

The reviews are divided: three reviews give a score of 7 (accept) whereas one review gives a score of 4 (borderline reject). The positive reviews cite the following strengths: clearly stated motivation, technical soundness, potential for impact and convincing experiments. The negative review cites lack of clarity as the main weakness (something also mentioned in one of the positive reviews), while the reviewer is not convinced about the originality of the method given similar advances in variational inference and generative modelling.

On balance, given the potential of impact in the field of predictive coding and the technical soundness, I'm happy to recommend acceptance, even though clarity is somewhat lacking.

Some reviewers requested more details on computational complexity, memory consumption and scalability. I encourage the authors to use the extra content page in the camera-ready version to discuss these aspects further.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviews are divided: three reviews give a score of 7 (accept) whereas one review gives a rating of 4 (borderline reject). The negative review cites lack of clarity as the main weakness, while the reviewer is not convinced about the originality of the method given similar advances in the field of predictive coding."
"This work presents a reconstruction GAN with an additional classification task in the objective loss function. Evaluations are carried out on medical and non-medical datasets. 

Reviewers raise multiple concerns around the following:

- Novelty (all reviewers)
- Inadequate comparison baselines (all reviewers)
- Inadequate citations. (R2 & R3)

Authors have not offered a rebuttal. Recommendation is reject. Work may be more suitable as an application paper for a medical conference or journal. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers raise multiple concerns about the following: Novelty (all reviewers) - Inadequate comparison baselines (all reviewingers) and Inadequacy citations. Recommendation is reject."
"This paper advances the long running thread of sequence modelling research focussed on differentiable instantiations of stack based models. In particular it builds upon recent work on the Nondeterministic Stack RNN (NS-RNN) by introducing three extensions. The first is to relax the need for a normalised distribution over the state and action distribution and allow unnormalised weights, this mostly serves to facilitate gradient flow and thus easier training. The second extension allows the RNN to condition on the top stack state as well as the symbol, improving expressiveness. The third improvement introduces a method for limiting the memory required to run the proposed model on long sequences, thus allowing its application to practical language modelling tasks. Each of these requires substantial algorithmic innovations.

The reviewers all agree that this is a strong paper worthy of publication. The paper includes a useful review of previous differentiable stack models which nicely sets up the rest of the paper where the contributions are well motivated and clearly presented. The reviewers had a number of clarification questions, partly due to the author's use of overly concise citations for key algorithms rather than inline descriptions. This situation has been improved by updates made to the paper.
The evaluation includes a series of synthetic experiments which are clear and provide a good elucidation of the various stack models properties. The practical evaluation on language modelling is more limited and serves mostly to demonstrate that the nondeterministic model can be scaled to a basic language modelling task.

Overall this is a strong paper with a well motivated and clear hypothesis. It provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications.","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers all agree that this is a strong paper worthy of publication. The paper provides a substantial extension to the prior work on nondeterministic stack models and progresses this line of research toward practical applications."
The overall view of the reviewers is that the paper is not quite good enough as it stands. The reviewers also appreciates the contributions so taking the comments into account and resubmit elsewhere is encouraged. ,"Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"Reviewers were mixed about this paper. The stand-out concern is clearly the fact that this paper does very little to set itself in context. I would echo these concerns, and this is clearly the single greatest weakness of the paper. For example I would highlight lines 54--62, in which the proposed technique is compared only against some nebulously-defined ""other"".

Despite these concerns, I am inclined to agree with reviewer QEo4 that the paper is acceptable for a workshop, and would strongly encourage the authors to address the concerns raised by reviewers.

In addition to the review comments, a few comments of my own:

- Equation (2) assumes that the SDE is solved via the Euler--Maruyama method. Whilst analytically convenient, I believe it is more elegant to treat the general case for as long as possible (without reducing to just the EM method at this early stage). If the noise has particular structure then it may become desirable to solve the SDE via some more efficient numerical method, e.g. Milstein if using commutative noise or Heun if using additive noise (Heun converges to the Stratonovich solution, but for additive noise then Itô and Stratonovich are identical).
- I think equation (2) may be missing a $t_{n+1}-t_n$ coefficient for the $f$ term.
- The ""not too far"" of line 67 is usually referred to as a ""trust region"".","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main concern is clearly the fact that this paper does very little to set itself in context, and I strongly encourage the reviewers to address these concerns."
"All the reviewers and AC agrees that the main strength of the paper that it studies a rather important question of the validity of using linear interpolation in evaluating GANs. The paper gives concrete examples and theoretical and empirical analysis that shows linear interpolation is not a great idea. The potential weakness is that the paper doesn't provide a very convincing new evaluation to replace the linear interpolation. However, given that it's largely unclear what are the right evaluations for GANs, the AC thinks the ""negative result"" about linear interpolation already deserves an ICLR paper. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers and AC agree that the main strength of the paper is that it studies a rather important question of the validity of using linear interpolation in evaluating GANs."
"Authors present a method attempting to perform Masked Auto-Encoding (MAE) using semantic knowledge, to try to better approximate the semantic MAE seen in language domain. To do this, they leverage an iBOT framework and add some embeddings of the class token to create ""part tokens"", which are then compared to patch tokens from iBOT to produce attention maps. The objective for this process is a StyleGAN-based image reconstruction.

Once the part attention maps training is done, the network is then used to guide semantic based part masking based on the generated attention maps, for semantic MAE.  

SemMAE pretrained networks are then compared against other forms of SSL pretraining on ImageNet 1k, iNa, CUB, Cars, and ADE-20K, demonstrating improvements in all domains.

Pros:
- [R] Idea is interesting / novel
- [R] Well written
- [AC/R] Results improve over baselines	
	

Cons:
- [AC/R] Pipeline is complicated.
- [R] What about starting from random MAE and then adapting based on parts knowledge? Authors respond that this is future work.
- [AC/R] Not convinced parts are visual analog of words. Authors provide benchmark improvements in performance, and qualitative visualization of the parts. However, there is no quantitative assessment of the parts and whether they have true semantic meaning.
- [AC/R] Some improvements are marginal. Authors respond that although marginal in some cases, they are consistent. 
- [R] Paper does not discuss more how to deal with background. Authors respond that this is future work.

Overall, all reviewers have changed their assessments to accept, including the one reject reviewer. AC recommends accept, though would be preferable if quantitative assessment of the quality of the semantic parts (for example, by segmentation masks) could be provided.

AC Rating: Accept","The paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have changed their assessments to accept, though would be preferable if quantitative assessment of the quality of the semantic parts (for example, by segmentation masks) could be provided."
"The authors develop two metrics for evaluating how different brain zones may be affected by the same stimuli. The framework created shows promising results when applied to actual and simulated data. This is a well-written paper with a particular application in mind. It designs a framework for discussing causal effects for applications in neuroscience. This paper's developed metrics and framework are potentially impactful and present a promising start for future research in this direction. The reviewers and I found the author's replies and rebuttal especially helpful in reaching our decision, so I hope some of these comments and arguments are included in the final version of the paper.

Minor comment: In addition to the comments made by the reviewers below, I would encourage the authors to make the references more consistent. At the moment some authors' first names are included and others are not.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers and I found the reviewers' replies and rebuttal especially helpful in reaching our decision."
"Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with ""revise and resubmit"".","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. Both R3 and R1 argue for rejection, while R2 argues for a weak accept. Given that we have to reject borderline paper, the AC concludes with ""revise and resubmit""."
"To address the problem of unauthorized use of data, methods are proposed to make data unlearnable for deep learning models by adding a type of error-minimizing noise. Based on th fact that the conferred unlearnability is found fragile to adversarial training, the authors design new methods to generate robust unlearnable examples that are protected from adversarial training. In addition, considering the vulnerability of error-minimizing noise in adversarial training, robust error-minimizing noise is then introduced to reduce the adversarial training loss.
The authors have tried to respond to reviewers' comments along with adding more experiments.
Overall, this manuscript finally gets three positive reviews and one negative review, where the possible vulnerability or robustness of error-minimizing noise against (simple) image processing operations was not verified.
In comparison with other manuscripts I'm handling that got consistent positive comments, this manuscript is still recommended to be accepted (poster) with a further study of robustness under simple image transformations in the final version.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors have tried to respond to reviewers' comments along with adding more experiments to address the problem of unauthorized use of data. In addition, considering the conferred unlearnability is found fragile to adversarial training, robust error-minimizing noise is then introduced to reduce the adversarially"
"The paper presents a novel idea of a 2.5D segmentation approach considering the characteristics of medical imaging data. The practical importance and novelty of the proposed approach are well received by all the reviewers. Clear description and illustration are included appropriately in the paper. Some experimental details-related questions were raised by the reviewers, and they were well addressed in the rebuttal.",The paper presents a novel idea of a 2.5D segmentation approach considering the characteristics of medical imaging data. The practical importance and novelty of the proposed approach are well received by all the reviewers. Clear description and illustration are included appropriately in the paper. Some experimental details-related questions were raised in the rebuttal.
"The reviewers carefully analyzed this work and agreed that the topics investigated in this paper are important and relevant to the field. Overall, the reviewers had a generally positive impression of this paper. One reviewer argued that this paper addresses a relevant and valuable question and makes an important step towards a better understanding of regret when reward shaping is used. Even though this paper makes assumptions that were of some concern to other reviewers, this reviewer argued that the paper is nonetheless an important milestone for the community. Another reviewer acknowledged that this paper conducted a formal theoretical investigation of the impact of reward shaping methods on the sample complexity of RL algorithms and argued that all proofs seem to be sound. This reviewer had a few technical questions, which were all addressed by the authors. Post-rebuttal, the reviewer encouraged the authors to incorporate the corresponding details (such as those discussed in the rebuttal) in the updated version of their draft. A third reviewer emphasized that this paper shed light on reward shaping from a theoretical perspective. They argued that the quality and scientific soundness of the paper are objectively excellent, that the paper is original, and that it deserves merit. The reviewer pointed out one main weakness, however, regarding the assumption of the type of the shaped reward function. They wondered whether this assumption could limit the impact and applicability of the paper's results. After reading the authors' thorough rebuttal, however, the reviewer stated that they were satisfied with all responses and updated their score accordingly. Finally, a fourth reviewer also had an overall positive view of this work but pointed out, as a weakness, the seemingly strong assumption that the shaping signal is an approximation of the optimal value function. After reading the authors' response, however, the reviewer stated that the assumptions made in this paper were not as restrictive as they initially thought, and updated their score. Overall, thus, it seems like most reviewers were positively impressed with the quality of this work. They look forward to an updated paper version addressing the suggestions mentioned in their reviews and during the discussion phase.","This paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the topics investigated in this paper are important and relevant to the field, and that the paper is an important milestone for the community."
"This submission studies (a somewhat non-standard version of) tolerant closeness testing of distributions over the n-dimensional hypercube. Instead of only iid samples, it is assumed that the tester is able to efficiently evaluate the probability mass at any point in the domain and to sample from the distribution conditioned on any subset of size two of the domain. The main result is
an algorithm with query complexity scaling near-linearly in the dimension. Using only iid samples, one would need exponential dependence on dimension. The algorithm is evaluated on synthetic and real-world datasets. It is experimentally shown that their algorithm outperforms a previous baseline, which in the worst case has complexity scaling exponentially in the dimension. Overall, this is an interesting work that appears to meet the bar for acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main result is an algorithm with query complexity scaling near-linearly in the dimension. The algorithm is evaluated on synthetic and real-world datasets."
"The paper proposes a framework to perform signal processing tasks on a signal represented with an implicit neural representation directly in the representation space, without the need to instantiate the signal. 

After the rebuttal period, all reviewers recommend acceptance. 

In particular reviewer 1Yx6, an expert on the topic, finds the idea original, and the quality and clarity of the paper to be high. The reviewer finds that while for now the significance is limited (since working as proposed in the representation space is computationally prophibitavely expensive), this is not a major issue, since the the paper is likely to inspire work that will push the idea further. 
Reviesers edqT and qGk1 also liked the general idea of the paper and find the proposed method to directly perform operations on the representation space of implicit neural representations to be novel and interesting

Reviewer Uh51 initially identified a few issues regarding experimental design, the validation, and on the included literature. The main concern regarding the experimental design of the reviewer was that the paper focuses on images (and not signal processing tasks more broadly), which I don't consider a shortcoming, due to the importance of image processing tasks. The concerns on the validation issues have been addressed as well, and the reviewer raised their score. 

I recommend acceptance of the paper. 
","The paper proposes a novel neural network training approach to perform signal processing tasks on a signal represented with an implicit neural representation directly in the representation space, without the need to instantiate the signal. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also found the idea original, and the quality and clarity of the paper to be high. The paper is likely to inspire work that will push the idea further."
"The paper proposes a differentiable approach for monocular VIO estimation based on BEV, without relying on deep neural networks. The reviewers find the paper well written and the idea of using BEV to be interesting. This paper received highly mixed reviews. The major concerns raised by the reviewers include empirical evaluations of the model interpretability, justification for relying on algorithmic priors than parameters, results on more challenging datasets, positioning this work with respect to existing work on deep state estimators, and clarifications regarding the claims made, among others. Most of the concerns raised by the reviewers have been thoroughly addressed in the rebuttal. I thank the authors for the engaging discussions during the rebuttal. Some minor concerns still exist. Nevertheless, I agree with the reviewers that the paper is an interesting contribution.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers found this paper well written and the idea of using BEV to be interesting. The main concerns raised by the reviewers include empirical evaluations of the model interpretability, justification for relying on algorithmic priors than parameters, results on more challenging datasets, and clarifications regarding the"
"This paper provides a generalization analysis for graph embedding methods concluding with the observation that the norm of the embedding vectors provides an effective regularization, more so than dimensionality alone. The main theoretical result is backed up by several experiments.  While the result appears to be correct, norm control, dimensionality reduction and early stopping during optimization are all very well studied in machine learning as effective regularizers, either operating alone or in conjunction. The regularization parameters, iteration count, embedding dimensionality is typically tuned for an application. The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods.   Both reviewer 2 and 3 have also raised points on the need for stronger empirical analysis.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The AC agrees with Reviewer 2 that the paper does not provide sufficiently interesting insights beyond this observation and is unlikely to influence practical applications of these methods."
"This paper studies inverse reinforcement learning through the prism of regularized Markov decision processes, by generalizing MaxEntIRL from the negative entropy to any strongly convex regularizer (as a side note, strict convexity might be enough for many results).
The reviewers appreciated the clarity, the mathematical rigor and the empirical evaluation of this paper. They asked some questions and raised some concerns, that were mostly addressed in the rebuttal and the revision provided by the authors.
This is a strong paper, for which the AC recommends acceptance.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers appreciated the clarity, the mathematical rigor and the empirical evaluation of this paper."
"The paper considers the stochastic proximal point algorithm, the main contribution is a convergence proof (in addition to arguing that it is a practical algorithm for ERM problems).  However, reviewer EvJ6 and xJB8 pointed out a fatal flaw in Lemma 1 that affects all the downstream results, and the other two reviewers and myself agree that this invalidates the core results. Reviewer EvJ6 gives extensive examples of how the lemma cannot be simply fixed. So while this is an interesting stochastic extension of the famous deterministic method and may warrant further study, this paper doesn't yet provide a rigorous convergence proof.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The paper considers the stochastic proximal point algorithm, the main contribution is a convergence proof (in addition to argument that it is practical for ERM problems). However, reviewers agree that this invalidates the core results."
"This is a borderline paper -- while the underlying idea is good and relevcant, the authors don't do a very good job of selling it; their experiments are performed on a very specific task with limited clinical relevance. The reviewers had a number of questions regarding experimental setup, which were largely answered in the rebuttal.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers had a number of questions regarding experimental setup, which were largely answered in the rebuttal."
"There was some discussion on this paper, both with the authors and between reviewers. On the one hand, there is a general agreement that the empirical results suggesting that spectral clustering-based method can be competitive with SOTA methods on node classification benchmark is an interesting result. One the other hand, reviewers did not find a significantly novel contribution in the methodology proposed, and found that the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work or not on some benchmarks). There is therefore a consensus that the paper is not ready for ICLR in its current form, but we hope that the reviews and discussion will help the authors prepare a revised version in the future.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also found that the empirical results suggest that spectral clustering-based method can be competitive with SOTA methods on node classification benchmark is an interesting result, and the empirical evaluation lacks depth and details to be really informative (eg, to understand why some methods work on some benchmarks)."
"This paper proposes a self-exciting temporal point process model with a non-stationary triggering kernel to model complex dependencies in temporal and spatio-temporal event data. The kernel is represented by its finite rank decomposition and a set of neural basis functions (feature functions). The proposed model has superior performance in comparison to other state-of-the-arts methods. All the reviewers recognized that the model is interesting and advances the state of the art in a meaningful way. While they were some concerns regarding the experimental evaluation, particularly in terms of real data, and the presentation, the rebuttal/revision by the authors cleared up these concerns.","This paper proposes a self-exciting temporal point process model with a non-stationary triggering kernel to model complex dependencies in temporal and spatio-temporal event data. The kernel is represented by its finite rank decomposition and a set of neural basis functions (feature functions). The proposed model has superior performance in comparison to other state-of-the-arts methods. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a potentially very interesting and original approach to handle label noise.  The numerical experiments suggest that the method works very well. But the paper  itself has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and  Markov theory. 

Note that due to the low confidence in several review an additional emergency review by an expert was asked and it confirmed the global opinion from  other reviewers that the paper is interesting but needs a major rewriting before acceptance in a ML conference. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation of the method before resubmitting.  ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper has been deemed very hard and demanding to read and understand for a general machine learning crowd and even by experts in the fields of optimal transport and Markov theory. The AC strongly suggest that the authors work on a more pedagogical introduction and explanation before resubmitting."
"The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies. The authors’ rebuttal addressed some of the reviewers’ concerns but not fully. Overall, I believe that the work is interesting and may be useful to the community (though to a small extent., in my opinion). However, the paper would benefit from additional explanations, experiments and discussions pointed in quite some detail by the reviewers. AS is, the paper is below the acceptance threshold for presentation at ICLR.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The reviewers raised a number of concerns including the appropriateness of the chosen application and the terms in which social dilemmas have been discussed, the lack of explanations and discussions, missing references, and the extent of the evaluation studies."
"This paper extends the idea of hindsight experience replay (HER) to learn Q functions with relative goals by constructing a distribution over relative goals sampled from a replay buffer using a clustering algorithm. This approach is evaluated on three multi-goal RL environments and is shown to learn faster than baselines.

${\bf Pros}$:
1. Faster convergence as compared to baselines
2. Interesting use of clustering in the context of HER but this choice is made without strong justifications or formal arguments

${\bf Cons}$:
1. Some of the key choices made in this paper are not justified or explained property, e.g. - the goal sampling strategy, choices made in the clustering algorithm and associated heuristics, implicit assumptions (e.g. R1 raised the question of using L2 distance in measuring metrics between two states) 
2. There are several choices made without sufficient formal arguments, verification or guarantees. 

The paper studies an interesting problem but could be made stronger by incorporating feedback received during the discussion period. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"Equilibrium propagation is a biologically plausible form of backpropagation based learning where the true gradient of an energy based model is computed for infinitesimal perturbations. In this innovative work, authors extend EP using complex analysis that links contour integrals for finite perturbations with the oscillatory dynamics in time. This not only allows better gradient estimates, but also applications to related theories of learning in neuroscience as well as neuromorphic engineering. It represents a significant advance that opens new doors.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"Executive summary:

The authors study the repeated allocation of an identical good over T rounds to n strategic buyers in a ""no monetary transfers"" setting. The buyers have i.i.d. valuations drawn from an unknown distribution, and the algorithm must work with reported valuations. The goal is to maximize social welfare (= sum of valuations) under the constraint that each buyer receives a pre-specified fraction of the total number of goods. 

The main result is an algorithm for this problem that ensures two things: 

(a) approximate Bayesian incentive compatibility (approx-BIC) (Definition 2 and Theorem 1) and 

(b) low individual regret (Definition 3 and Theorem 2). 

The key idea of the algorithm (is to exploit the iid-ness of the problem) and detect misreports from the underlying CDF using Dvoretzky-Kiefer-Wolfowitz type bounds.

Discussion and recommendation:

After some initial set back on the problem motivation, the reviewers bought into the motivation for studying this online allocation problem ""without monetary transfers"" (adding examples such as the foodbank example might be good).  

There was some discussion around ""assuming iid valuations"" limiting the generality of the result, but there is in fact a history of papers that studies learning with strategic agents under this assumption (eg Kanoria and Nazerzadeh 2021).

The main difference of the current work is that it's working in a setting without money.

The idea behind the algorithm is maybe ""the obvious think to do"" - but of course it still requires some work to formally prove that it actually works. 

I think one thing that could strengthen the paper would be to add some discussion around the tightness/non-tightness of the approximate BIC and individual regret bounds.

Weak accept.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main idea of the algorithm is to exploit the iid-ness of the problem and detect misreports from the underlying CDF using Dvoretzky-Kiefer-Wolfowitz type bounds."
"This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classifier and optimizing the student model with advanced loss functions. It received comments from three reviewers: 1 rated “Ok but not good enough - rejection”, 1 rated “Marginally below” and 1 rated “Marginally above”. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillation methods and their special case in face recognition. During the rebuttal, the authors made efforts to response to all reviewers’ comments. However, the rating were not changed. The ACs concur these major concerns and more comprehensive comparisons with the state of the art KD methods are necessary to better illustrate the contribution of this work. Therefore, this paper can not be accepted at its current state.
","This paper presents a knowledge distillation method for face recognition, by inheriting the teacher’s classifier as the student’s classesifier and optimizing the student model with advanced loss functions. The reviewers appreciate the simple yet clear methodology illustration and the well written paper. However, a number of major concerns are raised by the reviewers, including limited novelty, lack of comparison with more advanced knowledge distillations methods and their special case in face recognition. The ACs agree that this paper is not accepted at its current state."
"The paper receives unanimously positive reviews from four knowledgeable experts. They all agree that, though the saliency detection method NormGrad is not invented by the authors, its use in the context of medical image analysis shows promising results, which is clearly demonstrated by the authors. They also express some concerns, which are largely addressed by the authors in the discussions. I, therefore, recommend the acceptance of this paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed some concerns about generalization, although some concerns were raised regarding generalization of the approach to different datasets were noted."
"This paper proposes a contribution aiming at understanding the cause of errors in few-shot learning. The motivation is interesting but the reviewers pointed out many aspects that require more precisions and polishing in addition to the fact that the upper bound provided it rather loose. The rebuttal provided addresses some concerns, but there are still some remarks that require some clarifications en work.
Hence, I propose rejection.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The rebuttal provided addresses some concerns, but there are still some remarks that require clarifications en work."
"This paper proposes a new reverse-engineering method for trojan attack detection. The idea is to focus on feature representation space so that the detection is more robust to dynamic / input-dependent attacks and other feature-based attacks. The reviewers consider the idea generally novel and effective, and the experiments thorough. Some reviewers hope to see more visual analysis that can provide better insights into the effectiveness of the method. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers consider the paper generally novel and effective, and the experiments thorough."
"There was a fair amount of discussion about the paper.  Several reviewers felt that the paper would have been stronger if it tried to do less but better.  The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements.  Doing this requires a better understanding of the algorithm's behavior and a valid ablation study, a new concern raised during the discussion with the authors. 

The reviewers felt that the proposed approach is potentially interesting and would like to see this paper done well.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers felt that the paper would have been stronger if it tried to do less but better. The reviews describe in detail what the reviewers would have found compelling, but the key suggestion is to remove the complexity that is not essential for the approach to provide consistent improvements."
"The submission introduces the sparse hierarchical table ensemble (S-HTE), based on oblivious decision trees for tabular data. The reviewers acknowledged the clarity of the presentation and the importance of the computational complexity analysis. However, they also raised concerns regarding the novelty of the proposed method and the significance of the results compared to competing methods (e.g., CatBoost). Given the consensus that the submission is not ready for publication at ICLR, I recommend rejection at this point.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also raised concerns about the novelty of the proposed method and the significance of the results compared to competing methods."
"This paper set out to show that increasing task diversity during meta-training process does not boost performance. The reviewers mostly  agreed (only reviewer wVFn dissented) that the empirical set up of the paper was convincing, but they also felt it over-emphasized empirics over a deeper understanding of the phenomena observed. In turn, this resulted in discussions around how the experiments and the explanations didn't fully prove that increasing task diversity does not help. Overall, the discussion and the additional analysis tools provided by the authors (such as the diversity metric) will greatly improve the paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers generally agreed that the empirical set up of the paper was convincing, but they also felt it over-emphasized empirics over a deeper understanding of the phenomena observed."
"The paper adopts CVAE to generate OOD samples for training an outliner detector. It consists of two phases that train an OOD detector by leveraging the generated OOD data and shows it outperform other methods. According to reviewers’ discussion, there is a concern from the discussion: why CVAE works but other variants or cGAN doesn’t. The paper needs more motivation or evidence or ablations to support the generality of the work.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The article introduces a Bayesian approach for online learning in non-stationary environments. The approach, which bears similarities with weighted likelihood estimation methods, associate a binary weight to each past observation, indicating if this observation should be including or not to compute the posterior. The weights are estimated via maximum a posteriori. 

The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments. The original submission missed some relevant references that have been added in the revision. The approach has some limitations, highlighted by the reviewers:
* it requires to solve a binary optimisation problem whose complexity scales exponentially with the size of the dataset; although the greedy procedure proposed by the authors seems to work fine on the examples shown, the approach may not be applicable to larger datasets
* it requires to store all the data
* it requires the traceability of the marginal likelihood

Despite these limitations, there was a general agreement that this paper offers a novel and useful contribution, and I recommend acceptance. 

As noted by reviewer o4TK, I also think that the title is not very accurate. Bayesian methods naturally allow recursive updates of one's beliefs, and therefore have ""memory"". Maybe change the title for ""Bayes with augmented selective/adaptive memory""?","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted some concerns about generalization of the approach to different datasets, although some concerns were raised regarding generalization were noted. The paper is well written, the approach is novel and its usefulness demonstrated on a number of different experiments."
"Multiple reviewers point out the interesting improvement to mix attention maps at different layers via convolution based prediction modules. This module is sufficient to show improvements only on encoder side while comparing to concurrent work Synthesizer.
However, the novelty of the work is limited as compared to other papers and the results though improved did not convince the reviewers fully to gain a strong accept.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper presents an extension of the RMLMapper tool to cover Excel features. The three reviewers agreed that the contribution is relevant to the workshop and presents a solid work. Please take into account the comments provided to include them in the camera-ready paper, try to be clear if the work presents a demo or a short research paper.

The recommendation is to accept.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the contribution is relevant to the workshop and presents a solid work."
"This is an interesting contribution to the Boltzmann machine (BM) literature that makes a nice connection to DEQ models. On a positive note, reviewers found that it was well-written, clear, and interesting. Unfortunately, there were significant concerns with the manuscript that were not fully addressed in the revision: inappropriate or incomplete baselines, insufficient credit given to previous works, and the fact that this model is limited as compared to its BM relatives.

I would recommend that the authors take into account the reviewers' feedback in a revision of the work.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers found that the paper was well-written, clear, and interesting. However, there were significant concerns with the manuscript that were not fully addressed in the revision."
"The paper introduces a new approach for generative modeling: a diffusion process is run until it first hits a target set, and then outputs the first point that is hit. 

Three reviewers generally praised the originality, technical quality, and empirical results of the paper. They found the idea very interesting and novel, and technically sound. The numerical results were judged to be compelling and fair. One concern was clarity of exposition. There seemed to be two issues: (1) there were more typos and rough edges than expected, (2) more significantly, there was some difficulty in following all details of the main method given the notational complexity and significant amount of mathematical background on diffusion processes. Reviewer FUwB gave a number of concrete suggestions for improvement.

Reviewer wpmu had a negative overall opinion and critiqued the originality, quality, and clarity. On these issues: (1) the quality concern was based on a misunderstanding that was later resolved, (2) the originality concern does not seem justified to the meta-reviewer (it is based on a shared technical tool with a not-yet-published paper), and (3) the clarity concern is similar to those raised by other reviewers (especially FUwB). Overall, the meta-reviewer does not feel that the low score (3 = “reject”) was fully justified. 

In summary, overall reviewers found the paper sound and novel, with the main area for improvement being clarity of exposition about diffusion processes; one reviewer considered originality a weakness, but the meta-reviewer did not find this position well justified. 
","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers generally praised the originality, technical quality, and empirical results of the paper. They found the idea very interesting and novel, and technically sound, and the numerical results were judged to be compelling and fair."
"This paper presents a method for preventing slips in point-to-point motions while grasping an object using LSTM-based models for detecting and predicting slips. This approach is effective when contact force cannot be increased. Reactive and proactive controllers are presented, and the latter demonstrated better performance. 

The paper proposes an interesting approach to slip prevention especially useful for handling delicate objects. The authors successfully addressed most of the concerns raised by the reviewers, while expanding the dataset and examples will be addressed in a future journal paper. Nevertheless, the paper will make a valuable contribution to the conference.","This paper presents a method for preventing slips in point-to-point motions while grasping an object using LSTM-based models for detecting and predicting slips. This approach is effective when contact force cannot be increased. Reactive controllers are presented, and the latter demonstrated better performance. The authors successfully addressed most of the concerns raised by the reviewers, while expanding the dataset and examples will be addressed in a future paper."
"This paper introduces AlignKGC, a neural model that performs entity alignment (EA), relation alignment (RA), and knowledge graph completion (KGC) on multilingual knowledge graphs. The model jointly optimizes for the linear combination of losses for all 3 tasks. The authors present a convincing set of experiments to show that each of these tasks helps the other two. The paper is well written and demonstrates strong empirical results on DBLP5L containing 5 languages on all 3 tasks. Results are reproducible as both the code and data are made public. No major concerns after the author's response.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors present strong empirical results on DBLP5L containing 5 languages on all 3 tasks. The results are reproducible as both the code and data are made public."
"In this paper, the authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020. Overall, I like the idea of random node initialization because it is simple, effective, and theoretically well-founded. The key concern was that the novelty over the Sato's paper and the reviewers were still not convinced by the response. Therefore, the paper is still below the acceptance threshold.  I strongly encourage authors to revise the paper based on the reviewer's comments and resubmit it to a future venue. 
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The authors show the effect of RNI on the expressive power of GNN for the first time, where the RNI was initially proposed in Sato et al. 2020."
"This paper has received three positive reviews. In general, the reviewers have commented on the importance of the question related to how much selectivity is needed from units of a neural network for good classification -- from both the neuroscience and ML perspectives. The reviewers also commented on the thoroughness of the experiments and the general readability of the paper. This paper should be accepted if possible.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers commented on the importance of the question related to how much selectivity is needed from units of a neural network for good classification."
"This paper describes a new batching strategy for more efficient training of deep neural nets. The idea stems from the observation that some operations can only be batched more efficiently in the backward, suggesting that batching should be different between forward and backward. The results show that the proposed method improves upon existing batch strategies across three tasks. The reviewers find the work novel, but note that it does not properly address the trade-offs made by the technique - such as memory consumption. They also argue that the writing should be improved before acceptance at ICLR.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a few-shot learning method that learns task-adaptive semantic features that can incorporate for both of the support and query sets. Two approaches for modality combination are developed. The additional experiments in the author response addressed some concerns of the reviewers. However, the technical novelty of the proposed method is high enough since the proposed method uses existing techniques.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"All reviewers recommend rejection due to limited novelty and insufficient experimental analysis. The author’s response has addressed several other questions raised by the reviewers, but it was not sufficient to eliminate the main concerns about novelty (as the method is a combination of existing techniques) and missing comparisons to justify the effectiveness of the proposed approach.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers recommend rejection due to limited novelty and insufficient experimental analysis."
"This paper relates the problem of influence maximization and adversarial attacks on GCNs. 
The paper, and its formulation and assumptions stirred up quite a discussion among the reviewers and the authors. I do appreciate the thorough rebuttal that the authors provided, and the reviewers did take it into account (and revised their scores). 
However, all in all, I am afraid that there are just a few too many concerns with this paper. 
If the authors take the reviews to heart, they should be able to improve the manuscript and submit a stronger and improved version to the next conference. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"After reading the reviews and feedbacks, I lean towards acceptance. A majority of reviewers gave a positive score after the rebuttal period and some concern were answered in the authors response. Specifically authors have shown that the recent baselines they use outperform other baselines and therefore those do not need to be added in the paper, they have also clarified some proofs and some notations. Overall, the reviewers found the method presented interesting, the paper well written and appreciated the comparison to other methods of the literature. Finally, experiments shows interesting results on large scale domains which is a sign that the proposed method could scale up.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."""
"This paper investigates the role of representation learning when the distribution over the feature space has a long tail. The main motivation is to determine how much of the overall learning, in this case, is bottlenecked specifically by representation learning. The main findings are that vanilla learning gives brittle long-tailed representations, harming overall performance. The paper suggests a form of data augmentation to remedy this. Reviewers acknowledge that this investigation is worthwhile. However, many concerns were raised as to whether experiments support the drawn conclusions. A more principled approach to the data augmentation methodology is also needed. The authors address some of these, providing further experiments, but these were not enough to sway reviewers. Since results are fundamentally empirical in nature, this shortcoming indicates that the paper is not ready to share with the community just yet. Stronger experiments with clearer evidence are needed to fully support the thesis of the work.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The main motivation is to determine how much of the overall learning is bottlenecked specifically by representation learning. The main findings are that vanilla learning gives brittle long-tailed representations, harming overall performance."
"*Summary:* Study isolated orientations of weights for networks with small initialization depending on multiplicity of activation functions. 

*Strengths:* 
- Interesting analysis of properties in early stages depending on activations. 

*Weaknesses:* 
- Reviewers found the settings limited. 
- Reviewers found experiments limited.  

*Discussion:*

In response to ejGJ authors reiterate scope of covered cases and submit to consideration that their experiments should be adequate for basic research. Reviewer acknowledges the response, but maintains their assessment (limited scope of theory, limited experiments). KucV found the experimental part limited in scope, the settings unclear (notion of early stage, compatibility with theory), and review of previous works lacking. KucV’s sincerely acknowledged authors for their efforts to address their comments and improving the manuscript, and raised their score, but maintained the experimental analysis is not fully convincing and unclear, and the comparison with prior work insufficient. zuZq also expressed concerns with the experiments and the notions and settings under consideration. They also raised questions about the comparison with standard initialization. Authors made efforts to address zuZq concerns. zuZq acknowledged this but maintained initial position that the article is just marginally above threshold. jDJ5 found the paper well written and the conclusion insightful. However, also raised concerns about the experiments the settings under consideration. Authors made efforts to address jDJ5’s concerns, who appreciated this but was not convinced to raise their score. 

*Conclusion:*  
Two reviewers consider this article marginally above and two more marginally below the acceptance threshold. I find the article draws an interesting connection pertaining an interesting topic. However, the reviews and discussion conclude that the article lacks in several regards that in my view still could and should be improved. Therefore I am recommending reject at this time. I encourage the authors to revise and resubmit.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of each review. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."""
"The paper is interesting, and its focus is timely and important, given the continuing rapid rise of transformers (and their dependence of tokenization of images). All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All three reviewers recommend acceptance, to varying degree. The paper will be a valuable contribution to the program at ICLR."
"All reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. The authors suggest in one of their response that their technique could apply to ""any data mixing method with “batched k-sum” structure"". Such a larger level of generality might make the paper more interesting, but at the moment it is an extremely niche result.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that this is a reasonable contribution but that it is also extremely limited in scope. The authors suggest that their technique could apply to ""any data mixing method with ""batched k-sum"" structure."
"This paper looks at chaos in learning in games, extending a line of work in two players zero-sum games (that I found quite restrictive in the past). It somehow reduces the class of more general games to zero-sum and cooperative games (this decomposition is already known) so that the techniques can be transposed here.

The paper is interesting, yet sometimes difficult to follow, and I am not certain that it gives many new insights. 

Nonetheless, we believe its quality justify acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is interesting, yet sometimes difficult to follow, and I am not certain that its quality justify acceptance."
"The reviewers found the ideas presented in the paper interesting -- the use of mutual information to train memory for a model, and the clear presentation. Some questions were raised about demonstrating on a more elaborate set up such as NLP tasks -- the main experiments aside from the toy experiments of copy, etc algorithmic tasks, seem to be on RL experiments, but the method has been advertised more broadly in the motivation. Another reviewer raised the question of the complexity of training multiple networks. Nevertheless, the reviewers found the paper interesting enough to recommend a weak accept and I support that recommendation.

From a reviewers lens, I was a little surprised that the paper made no mention of prior works on maximizing mutual information between features of neural networks to improve results. As an example, see the following paper [1] that uses a mutual information regularizer between states at different steps of a recurrent neural networks. There is also a rich literature of doing so for convolutional neural networks. It would have made sense to compare how the idea in the paper performed in comparison to these methods (and in a sense the ablation study which looked at randomly choosing time steps, k, (regardless of the uncertainty estimator) is an experiment in this direction). I understand that part of the paper deals with the choice of time points to increase mutual information between, and so its probably more efficient than the other alternatives, but a comparison (or discussion in related works) would have made the paper stronger.

[1] Better Long-Range DependencyBy Bootstrapping A Mutual Information Regularizer. https://arxiv.org/pdf/1905.11978v1.pdf","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were surprised that the paper made no mention of prior works on maximizing mutual information between features of neural networks to improve results."
"Overall, all reviewers see the novelty in the dataset that provides an original new set of tactile data, accounting for a wide range of objects and scene, collected in the wild. The paper is overall well written, clear and easy to follow, and provides a good demonstration of the importance and usefulness of the dataset, going beyond datasets collected by robots in the context of material properties. It also includes initial steps towards benchmarking, although it is not the core contribution of the paper, which lies in the creation of the dataset itself. 

Two reviewers recommend acceptance, two stand marginally below acceptance, and one does recommend rejection. The authors have provided detailed responses to all reviewers’ comments and concerns and updated their manuscript. The main concerns of those reviewers sitting on the fence seem to be answered and limitations acknowledged however the element of bias in the data collection was still considered as a possible concern. Based on my reading and accounting for the authors changes, there approach itself is novel and original and hence provides already a relevant contribution to NeuroIPS; while the authors acknowledge limitations and future work avenues. The one reviewer recommending rejection didn’t engage in the discussion but from my reading of both the paper, the comments, and the authors responses, I believe the main points were addressed by the authors and misunderstandings clarified. Overall, I believe the papers offers a novel, non-existing dataset that can inspire interesting future works; all the dataset is shared and presented clearly for others to use. 
","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have provided detailed responses to all reviewers’ concerns and concerns and updated their manuscript. The authors acknowledge limitations and future work avenues."
"The paper proposes to include a component enforcing logical constraints on top of a variational autoencoder (VAE). The resulting method, VAEL. does so by leveraging ProbLog in addition to a neural encoder and decoder. VAEL is employed for simple generative tasks with constraints over the outputs, such as conditional image generation with MNIST and small Mario levels.

The reviewers have appreciated the direction where VAEL is heading and the importance of integrating constraints in deep generative models. Some concerns are still open. Specifically, the motivation behind some architectural choices (and the specific choice of VAEs as deep generative models) and the small-scale nature of the experiments. The complexity and scaling campabilities of performing symbolic reasoning with ProbLog are not discussed in depth.
 ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have appreciated the direction where VAEL is heading and the importance of integrating constraints in deep generative models."
"This paper presents Recommender Transformer (RETR) with a pathway attention mechanism that can dynamically zeroing-out the interactions (e.g., the trivial/noisy ones) in transformer-based sequential recommender systems. Extensive experimental results demonstrate the effectiveness of the proposed architecture. 

Overall this paper received mixed reviews with borderline scores. The reviewers raised concerns around baselines and evaluations, some of which the authors promptly addressed in the revision during the rebuttal period. I also read the paper in details myself. I do agree with some of the concerns from the reviewers but I don't think a method needs to beat every other published papers to be published (and I think the current baselines are more than thorough enough). My biggest complaint about the paper is around the writing, specifically, how the proposed idea is presented. 

This paper tries to tackle an important question, which is that in sequential recommendation, not every interactions are useful in helping predict future interaction. The self-attention mechanism in transformer kind of addresses this problem but in a more ""softer"" fashion with attention weights. This paper presents a simple yet effective method to introduce a pathway mechanism that adaptively zeroing-out some of the interactions via a binary pathway router. In order to train such a model end-to-end, Gumbel-softmax sampling is utilized.

The most important part of the contribution to me is that this is an improvement to the transformer architecture, as opposed to a new model which is what this paper's writing suggests -- the proposed approach is effectively model-agonistic and doesn't marry to a particular loss function or finer-grained architectural choices (number of layers, etc.). Currently there are many baselines in the paper, but each made some different model/architecture choices, which could contribute to the difference in performance (or not, but we wouldn't know). An ideal evaluation should have been to take all the transformer-based baselines that are currently in the paper, add this pathway mechanism without changing anything else, and show that the results improved over the transformer architecture. In this way, we know the improvements are exactly coming from introducing the pathway. The authors might argue some of the current results are already supporting this argument, but my point is to emphasize this point very explicitly rather than leaving it for the readers to infer. 

From what I read in this paper, I truly believe this pathway idea has its potential. Therefore, I would especially want the authors to further refine the presentation to better convey the idea, which in turn will hopefully increase the impact of this paper once it is eventually published. 

Some minor comments:
* The way the paper is currently written seems to suggest there are only three types of pathways and the network is capable of capturing all of them. I am personally not a big of fan of over-interpreting what a neural net is trying to do. Therefore, I wouldn't overly focus on the characterization of different pathways and only show the qualitative examples at the end as a high-level demonstration.
* In Eq 2 ""softmax"" should really be ""sigmoid"" if a 0-1 prediction is made there. Then the following line ""logit"" is probably not the right word here. 
* The qualitative examples at the end (figure 3) can be more carefully examined/labeled. For example, the current categorization is quite ambiguous -- ""Indie"" refers to the type of developers while ""JPG"" refers to the genre of the game, they are certainly not mutually exclusive.  ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received mixed reviews with borderline scores. The reviewers raised concerns about baselines and evaluations, some of which the reviewers promptly addressed in the revision during the rebuttal period."
"This paper proposes an innovative method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continual learning benchmarks. It is a clear accept. There was good discussion between the reviewers and authors that addressed a number of minor issues, including clarifying that the method has the same computational complexity as backpropagation. The authors are encouraged to make sure that these points are addressed in the final version of the paper.",This paper proposes a novel method for continual learning that modifies the direction of gradients on a new task to minimise forgetting on previous tasks without data replay. The method is mathematically rigorous with a strong theoretical analysis and excellent empirical results across multiple continuale learning benchmarks. The paper is a clear accept. The authors are encouraged to make sure that these points are addressed in the final version of the paper.
"The reviewers have improved their scores after the rebuttal, and I agree that the work has value. It proposes a model-driven data augmentation approach to environment-invariant graph representation. Just like most data augmentation works in graph representation learning, the approach relies on graph proposal generator. The work has an implicit mechanism hidden in the graph generator (the authors' reply to a reviewer that ""we target the extrapolation via a new invariant learning approach that is agnostic to specific GNN models"" is a misunderstanding of why & how these types of methods work). The submission could significantly improve the introduction by properly describing the work w.r.t. other OOD efforts in graph representation learning. While the tasks of different works may be different (e.g., graph classification vs node classification), it is important to emphasize what each different approach brings to the table (rather than just state that the tasks are different). I hope the authors take this opportunity to improve the introduction. This work is more similar to the former OOD graph representation works than IRM & REX, since (without modeling assumptions) both IRM and REX require the support of the environments in test to be a subset of the ones in training. The present work does not need this support assumption since it uses an implicit mechanism in the graph generator.

The toy example in Section 3.1 is informative, thank you. The theory looks solid and easy to understand. The technical novelty is limited, since once the input graph has been decomposed into a set of ego-graphs the definitions, formulations, and theory are all straightforward adaptations from the respective versions for IID data. 

Minor:
- In Assumption 2, m() should be defined inside the assumption.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have improved their scores after the rebuttal, and I agree that the work has value. The work is more similar to the former OOD graph representation works than IRM & REX, since both IRM and REX require the support of the environments in test to be a subset of the"
"This paper presents a way to use a translation memory (TM) to improve neural machine translation.  Basically the proposed model uses a n-gram retrieve matching sentence （or pieces） and takes advantage of the useful parts using gated attention and copying mechanism.  Although the idea of leveraging TM in the context of NMT is not new,  this work seems to be a fair contribution. My major concerns are the following
1. The retrieval part  is not clearly presented, raising questions about  complexities and the noise brought by the common words. The authors should give a better exposition on the ranking mechanism. 
2. The experiments are not convincing enough since the proposed model is not compared to the SOTA and the competitive models described in the prior work.

In conclusion I would suggest to reject this paper.",This paper presents a way to use a translation memory (TM) to improve neural machine translation. The proposed model uses a n-gram retrieve matching sentence (or pieces) and takes advantage of the useful parts using gated attention and copying mechanism. The authors should give a better exposition on the ranking mechanism.
"All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree to reject."
"Overall, the paper is capturing a really timely and increasingly important topic. Although the authors mention that they want to make their dataset particularly useful for academics/future research, I believe, that the topic is of great interest for companies, making the raised ethical considerations even more important. A critical reflection upon possibly ‘unintended uses’ can’t be emphasised enough, as well as possible biases in the data. However, I appreciated that the authors have done a fantastic job in their response and strengthening their article. Moreover, all reviewers recognise the utility and relevance of the proposed dataset and the authors did provide satisfactory responses to all raised concerns/comments. I believe this paper will stimulate some interesting and hopefully critical reflections on how to use/or not use the dataset and future improvements to overcome dataset biases. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper proposes a novel approach for estimating the high-dimensional intensity function of a Poisson process. The proposed approach builds on generalized additive models, using lower-dimensional projections. 

The reviewers noted that, although the paper is well written, the position of this paper compared to earlier related work is unclear, and the empirical evaluation of the method should be strenghtened. The authors clarified some points in their response, but the paper would still require some more modifications to be ready for publication. I therefore recommend this paper to be rejected.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted that the position of this paper compared to earlier related work is unclear, and the empirical evaluation of the method should be strenghtened."
"This paper received mostly positive reviews. The reviewers praised the strong performance when compared with previous work.
Also, the evaluation clearly shows the benefit of the proposed contributions in terms of performance.
Most concerns raised by reviewers were properly addressed in the rebuttal.

Lack of comparison to several previous works has been noted in a comment, but the authors clarified this concern, stating that the current work is a “large deviation from prior works”. The authors promised to include the missing references into the comparison.

Given the reviews, comments, and author's answers, I suggest acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers praised the strong performance when compared with previous work, and the evaluation clearly shows the benefit of the proposed contributions in terms of performance. The authors promised to include the missing references into the comparison."
"The paper presents a new method for generation of backdoor attacks against deep networks. The new method uses global warping instead of noise patches which makes the attack much more stealthy than previous approaches. The attack effectiveness is demonstrated on 3 benchmark datasets. A small user study is carried out to demonstrate that the attack is stealthier than conventional backdoor attacks. 

The new attack is a novel and original contribution which is likely to advance the understanding of backdoor attacks. There were some issues with respect to clarity in the original manuscript but the authors adequately addressed the critical remarks raised by the reviewers. ","The paper presents a new method for generation of backdoor attacks against deep networks. The new method uses global warping instead of noise patches which makes the attack much more stealthy than previous approaches. The paper is a novel and original contribution which is likely to advance the understanding of attacks. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"All the reviewers were generally happy with this paper. There were some comments about a better experimental section and maybe a better discussion of results and extensions (e.g. gap-dependent bounds, what happens in the K->L regime), but everyone felt that the manuscript as written was solid enough to merit acceptance. I encourage the authors to incorporate the discussions on these points in the final manuscript.




","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers were generally satisfied with the paper."
"Four reviewers provided reviews for this submission. Several reviewers felt that the idea to unify core vision tasks into a sequential output format is interesting and an entirely new approach and can have a large impact on how we train vision models in the future. There were a few concerns discussed between reviewers and authors. One concern was the comparison to past works that pre-trained on ImageNet vs the proposed model that was pre-trained on Objects365. The second concern was differentiating this work with Pix2Seq. In my opinion, the authors were able to answer both questions well. Overall, given the positive reviews, novelty of the work, potential to cause a significant shift in the approach of future modeling and discussion, I recommend acceptance. 
","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions."""
"The paper presents a method to make CNN focus more on structure rather than texture by constraining a random set of neurons per feature map to have constant activation. 
The paper has limited novelty and unclear analysis of the experimental results, for instance plots of accuracy vs strength of adversarial perturbation should be produced. Tables are not readable and results tend to be cluttered and confusing.  Some comparisons seem to be cherry picked as pointed out by some reviewers.
Although the approach seems to be well received by the reviewers they all shared similar concerns about having a stronger motivation and better validation of the approach (that is not amount of comparisons but the right comparisons that would clear doubts and make the work directly comparable to others).
I strongly encourage the authors to perform a deeper analysis and to clearly work on hypothesis and validation of their work. In my opinion, although the reviewers think different, the experiments are not sufficient to validate the strong claim of the paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper presents a simple approach called PDM for composing non-linear and complex normalizing flows with score-based generative models. Since score-based models can be considered as a special form of continuous-time normalizing flows, PDM corresponds to a composition of different classes of normalizing flows. 

Pros: 
* Combining generic normalizing flows with score-based models is an interesting direction as they have different characteristics and can be complementary to each other.
* Using Ito's lemma to show that the model learns a non-linear SDE in data space is valuable. 
* The authors show that the variational gap can be reduced using normalizing flows.

Cons: 
* The proposed method does not exhibit a clear advantage compared to the diffusion baseline without the normalizing flow component. On the CIFAR10 dataset, the best NLL and FID results are obtained by the diffusion baseline.

* Theorem 2 makes a very unrealistic assumption that a flow network is flexible enough to transform $p_r$ to any arbitrary distribution. If this holds, we wouldn't need the score-based generation model anymore. We could simply train the normalizing flow to map the input data distribution to a Normal distribution.

* This submission chooses to discuss differences with the recent LSGM framework. However, in doing so, several inaccurate claims are made. The lack of inference data diffusion in LSGM is mentioned as one of its drawbacks. However, it is not clear what is the value of having such a mechanism and what implications it may have on the expressivity of the model. Note that mapping from data space to latent space in VAEs can be considered as a stochastic inversion rather than an exact inversion. Ito's lemma does not require invertibility and it can be easily applied to the forward and generative diffusion in LSGM. The authors argue that applying it to the forward diffusion in LSGM will result in $\hat{p_{r}}\ne p_{r}$. But, $\hat{p_{r}}$ would be only considered for visualization of the forward diffusion and it is not used for training or any other purposes. LSGM, the proposed PDM, and score-based models are all trained with a reweighting of ELBO (see [here](https://arxiv.org/abs/2106.02808)). It is not clear if the drawback mentioned above has an impact on the training or expressivity of the model.

* The presentation in the paper requires improvement. The motivation on why invertibility plays a key role is not clear beyond generating the visualization in Figure 2. 

In summary, the paper proposes an interesting idea and explores directions very relevant to the current focus in generative learning. However, given the concerns above, we don't believe that the paper in its current form is ready for presentation at ICLR.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors argue that applying the method to the forward diffusion in LSGM will result in $hatp_rne p_r$. However, the lack of invertibility is mentioned as one of its drawbacks."
"The reviewers found this paper novel, significant for the community, and well written. All four reviewers recommended accepting the paper. I also appreciated the numerous illustrative examples in the paper. You addressed many of the remaining questions/concerns in your rebuttal and in the author-reviewer discussion. Please, go through the reviews once more for the camera-ready and take these into account.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. The reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction.  ,"Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. The reviewers agree that this paper overcomes a number of difficult algorithmic and technical challenges in parallelizing the RED method for image reconstruction. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the reviewers recommended acceptance with minor revisions."
"The authors describe a new method of large-batch optimisation for dense prediction computer vision tasks. The reviewers appreciate the simplicity of the method, convincing experiments and the potential practical importance. AC recommends acceptance.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers appreciate the simplicity of the method, convincing experiments and the potential practical importance."
"The reviewers found that the article tackles of problem of relevance in formation visualisation, and that the studies bring relevant insights to the community. They nonetheless highlight of number of problems remaining in the article that should be addressed before being published.

1. Clarify the presentation of the study conditions (all reviewers).
2. Better integrate of the literature in relation to the problem tackled in the article, both in the related work section (R1) *and* in the discussion (R2).
3. Clarify the visualizations used in study 2 (all reviewers).
4. Expand on the limitations of the experiments and their analysis (R2)
5. Reflect and contextualise of the results may apply in the wild (R3)
","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. The reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The paper presents a new video text VQA dataset, along a new baseline. The reviewers’s key concerns are around the size and difficulty of the dataset; the authors successfully address those during the discussion and revision period. The dataset is likely to be of interest to the community and was non-trivial to construct. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers’ concerns are around the size and difficulty of the dataset; the authors successfully address those concerns during the discussion and revision period."
"This paper proposes a method to resolve ""language drift,"" where a pre-trained X->language model trained in an X->language->Y pipeline drifts away from being natural language. In particular, it proposes to add an auxiliary training objective that performs grounding with multimodal input to fix this problem. Results are good on a task where translation is done between two languages.

The main concern that was raised with this paper by most of the reviewers is the validity of the proposed task itself. Even after extensive discussion with the authors, it is not clear that there is a very convincing scenario where we both have a pre-trained X->language, care about the intermediate results, and have some sort of grounded input to fix this drift. While I do understand the MT task is supposed to be a testbed for the true objective, it feel it is necessary to additionally have one convincing use case where this is a real problem and not just the artificially contrived. This use case could either be of practical use (e.g. potentially useful in an application), or of interest from the point of view of cognitive plausibility (e.g. similar to how children actually learn, and inspired by cognitive science literature).

A concern that offshoots from this is that because the underlying idea is compelling (some sort of grounding to inform language learning), a paper at a high-profile conference such as ICLR may help re-popularize this line of research, which has been a niche for a while. Normally I would say this is definitely a good thing; I think considering grounding in language learning is definitely an important research direction, and have been a fan of this line of work since reading Roy's seminal work on it from 15 years ago. However, if the task used in this paper, which is of questionable value and reality, becomes the benchmark for this line of work I think this might lead other follow-up work in the wrong direction.  I feel that this is a critical issue, and the paper will be much stronger after a more realistic task setting is added.

Thus, I am not recommending acceptance at this time, but would definitely like the authors to think hard and carefully about a good and realistic benchmark for the task, and follow up with a revised version of the paper in the future.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. The main concern that was raised with this paper is the validity of the proposed task itself."
"The paper proposes two constraints, transferring the knowledge from 2D domain to 3D domain and vice versa, to do weakly supervised semantic segmentation in 2D and 3D.  During inference the model doesn't require paired data.

The paper is clearly written, and explains how the 3D geometrical information complements the redundant dense 2D information.  The reviewers mention limited experiments.  However, the ablations are thorough, and show significant improvement over SotA performance.  

The authors addressed the reviewers' comments during the rebuttal.  The reviewers agree that the benefits of the proposal outweigh its flaws.

I recommend the paper for publication.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the benefits of the proposal outweigh its flaws."
"Four reviewers have evaluated this submission with one score 6 and three scores 8. Overall, reviewers like the work and note that *a rigorous and principled approach is taken by this work*. AC agrees and advocates an accept.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers like the work and note that *a rigorous and principled approach is taken by this work*."
"While the reviewers raised some concerns about the experimental results, I agree with the overall sentiment that this paper proposes a promising temporal convolutional network for segmentation in 4D CTP data. The authors may want to consider updating the title of the paper as the ""outcome prediction"" term seems to be a source of common confusion.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper proposes a boosting algorithm for RL based on online boosting. The main advantage of the result is that the sample complexity does not explicitly depend on number of states. Post rebuttal, some of the reviewers have changed their opinion on the paper. However, overall the reviewers still seem to be on the fence about this paper. Seems like the paper combines the techniques from Hazan Singh’21 along with a frank-wolfe algorithm to deal with non-convex sets but the reviewers seem to view this as not as significant a new contribution. 

I see the paper as being interesting but do agree with some of the comments of the reviewers and am leaning to a reject.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. Overall the reviewers still seem to be on the fence about this paper."
"Thank you for submitting your paper to NeurIPS! This paper makes a valuable contribution to the scoring model literature, providing a fast and scalable algorithm to derive sparse risk scores. The reviewers uniformly appreciated the methodological approach (integrating beam search with logistic regression, diverse feature selection, and star search for choosing integer coefficients), and noted that the stand-alone Python implementation is also advantageous over competitors that rely on mathematical programming solvers. I am pleased to recommend acceptance of this practically relevant work.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
all reviewers agree that the weaknesses of this work outweigh the few positive aspects and thus the paper cannot accepted in the current form. No rebuttal or revision was provided.,"Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper studies semantic type detection.
 The problem is of practical significance to  i  tabular data.
 However, in its current form, there are concerns about  the scope of novelty and technical significance.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper studies semantic type detection. The problem is of practical significance to i tabular data."
"The paper presents an actor critic type of method consisting of two types of features -- dynamics and tasks, in the multi-task continuous control setting. While the topic of the research is interesting and relevant to ICLR, the reviewers have concerns with the novelty and technical significance of the work. Specifically, the proposed method is very similar to several other works leading to an incremental novelty. In addition, the method is evaluated only on simple environments. The concerns remain after the discussion period. 

In the next version of the manuscript, the authors are encouraged to pursue more difficult settings and modify the method to work on those problems. That would make the paper stronger, and lead to a more novel method evaluated on harder problems.","The paper presents an actor critic type of method consisting of two types of features -- dynamics and tasks, in the multitask continuous control setting. The reviewers have concerns with the novelty and technical significance of the work. The authors are encouraged to pursue more difficult settings and modify the method to work on those problems. This would make the paper stronger, and lead to a more novel method evaluated on harder problems."
"The paper studies nonconvex-strongly concave min-max optimization using  proximal gradient descent-ascent (GDA), assuming Kurdyka-Łojasiewicz (KŁ) condition holds. The main contribution is a novel Lyapunov function, which leads to a clean analysis. The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis. Nevertheless, the paper was overall viewed favorably by the reviewers, who considered it a worthwhile contribution to the area min-max optimization.  ","The paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main downsides of the paper as discussed by the reviewers are the lack of experiments and somewhat stringent assumptions needed in the analysis."
"The paper received positive leaning reviews (2x borderline accept, 1x weak accept, 1x accept). The meta-reviewer agrees with the reviewers' assessment of the paper.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"This paper addresses the task of unsupervised knowledge base construction. The reviewers like that the authors present a novel unsupervised approach, and are happy with the thorough experiments. However, they also point out that the approach could be motivated better, and that it makes many assumptions that are not explained properly.  We recommend acceptance but nudge the authors to consider the reviewer suggestions.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers like that the authors present a new unsupervised approach, and are happy with the thorough experiments. The authors also point out that the approach could be motivated better, and that it makes many assumptions that are not explained properly. We recommend acceptance but nudge the reviewers to consider the reviewer suggestions."
"although the idea is a straightforward extension of the usual (flat) attention mechanism (which is positive), it does show some improvement in a series of experiments done in this submission. the reviewers however found the experimental results to be rather weak and believe that there may be other problems in which the proposed attention mechanism could be better utilized, despite the authors' effort at improving the result further during the rebuttal period. this may be due to a less-than-desirable form the initial submission was in, and when the new version with perhaps a new set of more convincing experiments is reviewed elsewhere, it may be received with a more positive attitude from the reviewers.","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"Strengths:  

-- Solid experiments 
-- The paper is well written

Weaknesses:

-- The findings are not entirely novel and not so surprising, previous papers (e.g., Brevlins et al (ACL 2018)) have already 
suggested that LM objectives are preferable and also using LM objective for pretraining is already the  standard practice (see details in R1 and R3). 

There is a consensus between the two reviewers who provided detailed comments and engaged in discussion with the authors.  ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper treats the problem of running gradient descent-ascent (GDA) in min-max games with a different step-size for the two players. Earlier work by Jin et al. has shown that, when the ratio of the step-sizes is large enough, the stable fixed points of GDA coincide with the game's strict local min-max equilibria. The main contribution of this paper is an explicit characterization of a threshold value $\tau^*$ of this ratio as the maximum eigenvalue of a specific matrix that involves the second derivatives of the game's min-max objective at each (strict local) equilibrium.

This paper generated a fairly intense discussion, and the reviewers showed extraordinary diligence in assessing the authors' work. Specifically, the reviewers raised a fair number of concerns concerning the initial write-up of the paper, but these concerns were mostly addressed by the authors in their revision and replies. As a result, all reviewers are now in favor of acceptance.

After my own reading of both versions of the paper and the corresponding discussion, I concur with the reviewers' view and I am recommending acceptance subject to the following revisions for the final version of the paper:
1. Follow the explicit recommendations of AnonReviewer3 regarding the numerical simulations (or, failing that, remove them altogether). [The authors' phrase that ""The theory we provide also does not strictly apply to using RMSprop"" does not suffice in this regard]
2. Avoid vague statements like $\tau \to \infty$ in the introduction regarding the work of Jin et al. and state precisely their contributions in this context. In the current version of the paper, a version of this is done in page 4, but the introduction is painting a different picture, so this discussion should be transferred there.
3. A persisting concern is that the authors' characterization of $\tau^*$ cannot inform a practical choice of step-size scaling (because the value of $\tau^*$ derived by the authors depends on quantities that cannot be known to the optimizer). Neither the reviewers nor myself were particularly convinced by the authors' reply on this point. However, this can also be seen as an ""equilibrium refinement"" result, i.e., for a given value of $\tau$ only certain equilibria can be stable. I believe this can be of interest to the community, even though the authors' characterization cannot directly inform the choice of $\gamma_1$ and $\gamma_2$ (or their ratio).

Modulo the above remarks (which the authors should incorporate in their paper), I am recommending acceptance.","This paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed some concerns regarding the initial write-up of the paper, and the reviewers showed extraordinary diligence in assessing the authors' work."
"although the way in which the authors characterize existing rnn variants and how they derive a new type of rnn are interesting, the submission lacks justification (either empirical or theoretical) that supports whether and how the proposed rnn's behave in a ""learning"" setting different from the existing rnn variants.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"All reviewers agree that the paper has value and proposed a fairly novel idea.
This method is solely applied to digital pathology images and deserves future work, but it is interesting for the MIDL community.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that the paper has value and proposed a fairly novel idea. This method is solely applied to digital pathology images and deserves future work."
"The paper provides a uniform generalization bound for overparameterized neural networks using the notion of maximal information gain. The analysis relies on the eigendecay of the eigenvalues of the NTK, which has recently been the object of a lot of work in the literature, including the work of Bietti and Bach (the proof actually uses one of their key lemma).

The paper originally received a set of reviews with a large disagreement between the reviewers (including two reviewers with a negative opinion and three reviewers being more positive). After the discussion period, two reviewers kept a very negative opinion, while other reviewers slightly lowered their score. Some of the problems raised by the reviewers include the restrictions imposed on the data, a missing proof (which was eventually added by the authors), the discussion of prior work being inadequate (including for instance the differences with more classical generalization bounds), and the novelty of the analysis.

Overall, the paper clearly has some merits but some of the concerns above are too important at this stage to accept the paper. I recommend the authors address the concerns mentioned in the reviews before re-submission.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers raised some concerns about generalization, although some concerns were raised regarding generalization of the approach to different datasets were noted."
"The reviewers agree the paper studies an interesting problem on training robust binary neural networks and the paper does a good job in evaluating the proposed approach on multiple datasets and compares well with baselines. However the paper also has some drawbacks such as the proposed method has limited novelty and is a combination of existing techniques, missing comparisons to standard training based approaches, evaluations limited to ResNets.  Overall the paper is on borderline. I suggest acceptance and encourage the authors to include all the changes that came up during discussion in the final version and discuss the limitations.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. The reviewers agree the paper studies an interesting problem on training robust binary neural networks and the paper does a good job in evaluating the proposed approach on multiple datasets and compare well with baselines. However, the proposed method has limited novelty and is a combination of existing techniques, missing comparisons to standard training based approaches, evaluations limited to ResNets."
"The paper shows the success of a relatively simple idea -- fine tune a pretrained BERT Model using Variational Information Bottleneck method of Alemi to improve transfer learning in low resource scenarios.

I agree with the reviewers that novelty is low -- one would like to use any applicable method for controlling overfitting when doing transfer learning, and of the suite of good candidates, VIB is an obvious one -- but at the same time, I'm moved by the results because of: the improvements and the success on a wide range of tasks and the surprising success of VIB over other alternatives like dropout etc, and hence I'm breaking the tie in the reviews by supporting acceptance.  Its a nice trick that the community could use, if the results of the paper are an indication of its potential.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that novelty is low -- one would like to use any applicable method for controlling overfitting when doing transfer learning"
"The authors study an inverse reinforcement learning problem where the goal is to infer an underlying reward function from demonstration with bias.  To achieve this, the authors learn the planners and the reward functions from demonstrations. As this is in general impossible, the authors consider two special cases in which either the reward function is observed on a subset of tasks or in which the observations are assumed to be close to optimal. They propose algorithms for both cases and evaluate these in basic experiments. The problem considered is important and challenging. One issue is that in order to make progress the authors need to make strong and restrictive assumptions (e.g., assumption 3, the well-suited inductive bias). It is not clear if the assumptions made are reasonable. Experimentally, it would be important to see how results change if the model for the planner changes and to evaluate what the inferred biases would be. Overall, there is consensus among the reviewers that the paper is interesting but not ready for publication.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have expressed concerns about the generalization of the approach to different datasets, although some concerns were raised about generalization were noted."
"Four out of five reviewers recommend acceptance of the paper with a score of 7. These reviewers praise the quality and importance of the dataset. Moreover, all reviewers acknowledge the quality of the writing.

The authors have constructively engaged into the discussion with the reviewers, and have put a lot of effort into drastically improving their submission based on all reviewers' comments. The weaknesses raised by the reviewers have been successfully addressed.

In particular, the reviewer (dm8f) who gave a score of 4 raised the following concerns:

1. There exist other time series anomaly detection datasets.
2. More anomaly detection methods should be considered in the experiments. 
3. The finetune mode and distillation mode are not detailed, their pseudocodes should be provided.

The authors have addressed these concerns as follows:

1. In their answer, the authors went through an extensive list of existing benchmarks and justified why they are not appropriate to solve their focused problem, and how their dataset differ from those. The main added value of the proposed dataset is its timespan and the fact that its occurring distribution shifts are natural (as opposed to synthesized).
2. The authors have expanded their experiments with more anomaly detection models. The new results corroborate the previous conclusions.
3. The authors have added the pseudocode algorithm in Appendix and have added code to replicate their experiments in the git repository.

I believe that the authors have sufficiently addressed the reviewer's concerns (and more).

For these reasons, I recommend acceptance of the paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers acknowledge the quality and importance of the dataset. The authors have improved their experiments with more anomaly detection models. The new results corroborate the previous conclusions."
"The paper describes an adversarial training approach that, in addition to the commonly used robustness loss, requires the network to extract similar representation distributions for clean and attacked data. The proposed method is inspired by domain adaptation approaches that require a model to extract domain invariant/agnostic features from two domains. Although the experimental results are solid and technically sound, the novelty of the methodology is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation such as ""unsupervised domain adaptation by backpropagation"". On the other hand, more recent SOTA methods are missing and only smaller scale datasets are used for evaluation. During the discussions, the major concerns from three reviewers are novelty. 

I totally agree that the simplicity of the method should be a virtue. However, the idea of domain-invariant representation learning is already established well, and its application to adversarial training is quite intuitive to the community. Also, the similar methodology already exists in domain adaptation. According to the top-tier conference culture in the ML community, what most valuable is the novelty and insight, not the performance. In the end, I think that this paper may not be ready for publication at ICLR, but the next version must be a strong paper.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about generalization of the approach to different datasets. However, the novelty of the method is not enough, as the domain classifier and the gradient reversal layer are the same with those methods in domain adaptation."
"The paper proposes a new pipeline-parallel training method called WPipe. WPipe works (on a very high level) by replacing the two-buffer structure of PipeDream-2BW with a two-partition-group structure, allowing resources to be shared in a similar way to PipeDream-2BW but with less memory use and less delays in weight update propagation across stages. The 1.4x speedup it achieves over PipeDream-2BW is impressive.

In discussion, the reviewers agreed that the problem WPipe tries to tackle is important and that the approach is novel and interesting. But there was significant disagreement among the reviewers as to score. A reviewer expressed concern about the work being incremental and difficult to follow. And while these were valid concerns, and the authors should take note of them when revising their paper, I do not think they should present a bar to publication, both based on my own read of the work and also in light of the fact that other reviewers with higher confidence scores did not find novelty to be a disqualifying concern. As a result, I plan to follow the majority reviewer opinion and recommend acceptance here.","The paper proposes a novel neural network training method called WPipe. The method works by replacing the two-buffer structure of PipeDream-2BW with a two-partition-group structure, allowing resources to be shared in a similar way to Pipedream-1BW but with less memory use and less delays in weight update propagation across stages. The reviewers expressed concerns about the work being incremental and difficult to follow, and the reviewers did not consider novelty to be a disqualifying concern."
"The paper introduces model-agnostic ways of quantifying predictive uncertainty and latent space differences in the situation when the distribution of data encountered during deployment differs from what the system was trained on and there is no access to the data itself. The model is evaluated on large scale EEG data.

The paper solves an important problem, in particular to the field of healthcare which has previously seen instances of models underperforming significantly at the time of deployment. As mentioned by the reviewers, this direction has not been sufficiently explored, so there is novelty in the problem itself, as well as in the solution. The experiments on EEG data were seen as convincing by the reviewers, though some questions were raised about the scope of the paper.

Overall, there is considerable merit in the work and recommend acceptance of this paper. In the camera ready, the authors should make sure not to overstate the applicability of their method. While this work could, in theory, be applied (or adapted) to other data, the merits of it outside of models trained on EEG data have not been demonstrated and should therefore not be stated as a given.

Reviewers x119 and PL6S have not engaged in the discussion although the authors responded to the issues they raised, which I kept in mind when issuing my recommendation.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions."" Overall, there is considerable merit in the work and recommend acceptance of the paper. In the camera ready, the authors should make sure not to overstate the applicability of their method."
"This manuscript proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The manuscript shows how existing Byzantine-robust methods suffer vulnerabilities when the devices are non-iid, and describe a simple coordinated attack that defeats many existing defenses. In response, the primary algorithmic contribution is a bucketing approach that aggregates subgroups of devices before robust aggregation. This approach is also easily composed with existing Byzantine-robust methods. The manuscript includes an analysis of the performance of the proposed approach, including an information-theoretic lower bound for certain settings.

During the review, the main concerns are related to the clarity of the technical contributions, and unclear technical statements. The authors respond to these concerns and have satisfied the reviewers. After discussion, reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are reminded to make the final changes agreed in the public discussion e.g., discussion of the reduction to SGD when  $\delta=0$","This paper proposes and analyses a bucketing method for Byzantine-robustness in non-iid federated learning. The main concerns are related to the clarity of the technical contributions, and unclear technical statements. The reviewers are generally strongly positive about the strength of the manuscript contributions. The authors are encouraged to make the final changes agreed in the public discussion."
"The paper presents an empirical analysis of Vision Transformers - and in particular multi-headed self-attention - and ConvNets, with a focus on optimization-related properties (loss landscape, Hessian eigenvalues). The paper shows that both classes of models have their strengths and weaknesses and proposes a hybrid model that takes the best of both worlds and demonstrates good empirical performance.

Reviewers are mostly very positive about the paper. Main pro is that analysis is important and this paper does a thorough job at it and draws some useful insights. There are several smaller issues with the presentation and the details of the content, but the authors did a good job addressing these in their responses.

Overall, it's a good paper on an important topic and I recommend acceptance.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main pro is that analysis is important and does a thorough job at it and draws some useful insights. The reviewers are mostly very positive about the paper."
"The paper considers model-based RL, and focuses on approaches that benefit from the differentiability of the model in order to compute the policy gradient. It theoretically shows that the error in the gradient of the model w.r.t. its input appears in an upper bound of the error in the policy gradient computing using the learned model. Motivated by this, it suggests a MBRL approach that learns two models, one of them minimizes the next-state prediction error (as commonly done) and the other minimizes a combination of prediction error and the gradient error. 
The paper empirically studies the method through extensive experiments.

Reviewers are generally positive about this work. They believe that the paper is insightful and the method is original. At first, there were some important concerns raised by the reviewers, but the authors revised their paper in the discussion period, and it appears that the reviewers are all satisfied now. I also read the paper during the rebuttal phase, and I should say that I have some concerns myself, especially on the theory part of the paper. Given that the authors did not have an opportunity to answer my questions, I do not put much weight on my concerns (and I believe most of them can be addressed with some clarifications). Considering the positive response of reviewers and promising results, I am going to recommend **acceptance** of this paper.

I strongly encourage the authors to consider the comments by reviewers, as well as the following ones, in the revision of their paper.


**Comments**

1) The true dynamics $f$ is defined as a stochastic one, i.e., $s_{t+1} = f(s_t, a_t, \epsilon_t)$ (just before Eq. 1), and similarly for the learned model. Here $\epsilon_t$ is the noise causing the stochasticity of the model. But later, when the errors on the model and its gradient are introduced (i.e., $\epsilon_f$ and $\epsilon_f^g$), the role of stochasticity becomes unclear.
For example, we have
$\|| \tilde{f}(s,a) - f(s,a) \||  \leq \epsilon_f$.

What happened to the noise term?

The same is true for Eq. (5). The next-state s' (either according to the true dynamics or the learned model) is random. In that case, it is not obvious how to interpret Eq. (5). Is it the error of the expected gradient of the next state? Or is it something else?

In case the dynamics is assumed to be deterministic, this should be clarified early in the paper.

2) The upper bound in Theorem 1 might be vacuous if the Lipschitz constant $L_f$ of the model is larger than 1.
To see this, consider Lemma 1. The constant $C_0$ is $\min [D/\epsilon_f, (1-L_f^{t+1})/(1 - L_f)]$.
If $L_f$ is larger than 1, for large enough t, the term $(1-L_f^{t+1})/(1 - L_f)$ blows up and $C_0$ becomes $D/\epsilon_f$. Therefore, the upper bound of Lemma 1 becomes $D$. Here $D$ is the diameter of the state space, which is assumed to be bounded.

This carries to in the next lemmas. In Lemma 4, $C_5$ would be of the same order as $C_0$ (multiplied by an extra $L_1 L_f / (1 - \gamma) )$, so the upper bound of this lemma becomes proportional to D too.

The $C_0$'s appearance continues in the proof of Theorem 1, in which $C_8$ is proportional to $C_0$ and $C_5$. So, $C_8$ is also become proportional to $D/\epsilon_f$. When we have $C_8 \epsilon_f$ in Eq. (34), we get a constant term $D$.
A similar dependence appears in the proof of Theorem 2, where B_3 is proportional to $C_8 \epsilon_f$, which can be as large as $D$. And in Eq. (47), we have $B_3^2$. So the upper bound in Eq. (47), which seems to the be upper bound of Theorem 2, is proportional to $D^2$. This means that if $L_f$ is larger than one, the upper bound does not go to zero, no matter how small the model error $\epsilon_f$ is (unless it is actually zero). This makes the bound meaningless.

This might be unavoidable. I am not sure about it at the moment. But it definitely requires a discussion.

3) Assumption 2 has a term in the form of $E[\frac{s_{t_2}}{ s_{t_1}} ]$ (I have simplified the form). The states $s_{t_2}$ and $s_{t_1}$ are vectors in general. How is the division defined here?

4) Please improve the clarify of the proofs. For example, in Lemma 2 it seems that a negative sign is missing in Eq. (49). Also how do we get Eq. (50) and Eq. (52)? (I couldn't easily verify them).

5) I believe the ""periodicity property"" used in Assumption 1 should be ""ergodicity property"".

6) The paper still has a lot of typos, e.g., ""To optimize the objective, One can ..."" (P3), ""argument data"" (instead of augmented) (p4), ""Superpose"" (p5), ""funcrion"" (p6).","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns were raised about generalization. Overall, the paper received a positive recommendation with minor revisions. The reviewers are generally positive about the paper, and the reviewers were satisfied with the reviewer's comments."
"The reviewer praises the creation of this potentially very valuable dataset and while there are some shortcomings in the presented benchmark/baseline results, I believe these will be quickly addressed by the community if the data becomes more widely used. MIDL aims to give some space for the presentation of open source data and I therefore also recommend acceptance.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."""
"The reviewers are in agreement that the paper should be rejected. Furthermore, the authors have declined to rebut the arguments of the reviewers. As such, I agree with the suggested rejection.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are in agreement that the paper should be rejected."
This paper studies the effect of pre-training data on the robustness of pre-trained vision-language models such as CLIP. Both empirical and theoretical results are provided to show that simple scaling may not always improving robustness. This is a timely results that shed light on how to perform better pre-training to improve robustness to distribution shifts. All reviewers agree that the work is technically solid and the contribution is significant.,"This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that the work is technically solid and the contribution is significant."
"Dear Authors,

Thank you very much for your detailed feedback to the initial reviews and also for further answering additional questions raised by a reviewer. Your effort has been certainly contributed to clarifying some of the concerns raised by the reviewers and improving their understanding of this paper.

Overall, all the reviewers found a merit in this paper and thus I suggest its acceptance. However, as Reviewer #2 suggested, investigating the convergence in the stochastic case is very important. More discussion on this would be a valuable addition to the paper, which the authors can incorporate in the final version.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"This paper considers the multi-fidelity variant of the best-arm identification problem. I recommend its acceptance and I strongly encourage the authors to take the several fantastic points raised by the reviewers while crafting their next draft. For instance, please include a discussion about (and clarification of) Assumption 1 that reflects the discussion with the reviewers.  ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. I strongly encourage the authors to take the several fantastic points raised by the reviewers while crafting their next draft."
"This paper has proposed a method named zero-shot learning for attributes to deal with a research problem about novel attribute classification and attribute labeling. The reviewers have many questions in the intial round. After the rebuttal, the authours clarify most unclear points, and some reviewers raise the score. In general, all the reviewers agree with the acceptance of this paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. In general, all reviewers agree with the acceptance of this paper."
"This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids the exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. Experiments on Librispeech and Wall Street Journal show that OCD improves test performance over both maximum likelihood and scheduled sampling, yielding state-of-the-art results. The primary concerns expressed by the reviewers pertained to the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers. The revision addresses the problem with a substantially larger number of references and discussion relating OCD to the previous work. Some issues of clarity were also well addressed by the revision.","This paper proposes an algorithm for training sequence-to-sequence models from scratch to optimize edit distance. The algorithm, called optimal completion distillation (OCD), avoids exposure bias problem inherent in maximum likelihood estimation training, is efficient and easily implemented, and does not have any tunable hyperparameters. The reviewers expressed concerns about the relationship of OCD to methods such as SEARN, DAgger, AggreVaTe, LOLS, and several other papers."
"The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information. This is done by adapting UNets to handle features defined in the complex space, and by adapting the loss function to improve an appropriate evaluation metric.

Strengths
- Modifies existing techniques well to better suit the domain for which the algorithm is being proposed. Modifications like extending UNet to complex Unet to deal with phase, redefining the mask and loss are all interesting improvements.
- Extensive results and analysis.

Weaknesses
- The work is centered around speech enhancement, and hence has limited focus. 

Even though the paper is limited to speech enhancement, the reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The paper is well written with interesting results and analysis. Therefore, it is recommended that the paper be accepted.
","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the contributions made by the paper are significant and can help improve related applications like ASR. The authors propose an algorithm for enhancing noisy speech by also accounting for the phase information."
"
pros:
- novel idea for multi-step QA which rewrites the query in embedding space
- good comparison with related work
- reasonable evaluation and improved results

cons:

There were concerns about missing training details, insufficient evaluation, and presentation.  These have been largely addressed in revision and I am recommending acceptance.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper builds on a promising line of literature developing connections between Gaussian processes and deep neural networks.  Viewing one model under the lens of (the infinite limit of) another can lead to neat new insights and algorithms.  In this case the authors develop a connection between convolutional networks and Gaussian processes with a particular kind of kernel.  The reviews were quite mixed with one champion and two just below borderline.

The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: ""the paper presents a novel efficient way to compute the convolutional kernel, which I believe has merits on its own"" and R2: ""I really like the idea of authors that kernels based on convolutional networks might be more practical compared to the ones based on fully connected networks"").  All the reviewers found the contribution of the covariance function to be novel and exciting.

Some cited weaknesses of the paper were that the authors didn't analyze the uncertainty from the model (arguably the reasoning for adopting a Bayesian treatment), novelty in appealing to the central limit theorem to arrive at the connection, and scalability of the model.

In the review process it also became apparent that there was another paper with a substantially similar contribution.  The decision for this paper was calibrated accordingly with that work.

Weighing the strengths and weaknesses of the paper and taking into account a reviewer willing to champion the work it seems there is enough novel contribution and interest in the work to justify acceptance.

The authors provided responses to the reviewer concerns including calibration plots and timing experiments in the discussion period and it would be appreciated if these can be incorporated into the camera ready version.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers all believed the paper had contributions which would be interesting to the community (such as R1: ""the paper presents an efficient way to compute the convolutional kernel, which I believe has merits on its own"" and R2: ""I really like the idea of authors that kernels based on"
"This paper proposes a variant of stochastic gradient descent that parallelizes the algorithm for distributed training via delayed gradient averaging. While the algorithm (DaSGD) proposed is sensible and seems to work, it also seems to miss a lot of related work. As pointed out by one of the reviewers, the class of asynchronous decentralized methods already seem to cover the space of DaSGD, and it's not clear how DaSGD differs from the existing methods in this space. As a result of this lack of comparison to related work, the reviewers recommended that the paper not be accepted at this time, and this evaluation was not challenged by an author response. I agree with this consensus.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, showing significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers recommended that the paper not be accepted at this time, and this evaluation was not challenged by an author response."
"This paper presents some new results on near-optimum algorithms for distributed optimization, nearly matching lower bounds. Most of the reviewers are positive about the contributions of this work. However, one issue that came up is the assumption of bounded gradient dissimilarity, which is essentially a gap between upper and lower bounds. While I am recommending to accept this paper, I believe this gap should be more prominently discussed in the abstract and introduction. 

","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets."
" 
The paper aims at controllable generation by introducing an additional ""content-conditioner"" block in the Transformer models. The paper further provides 4 different variants of a pre-training task to train the content-conditioner model. 

While the proposed approach seems an incremental contribution over CTRL and PPLM, certain reviews praised the approach being novel while keeping the architecture changes minimal. Overall, reviews indicate that the overall proposed method of fine-grained controlled generation with self-supervision is valuable, and empirical results support its effectiveness. 

All reviewers initially raised concerns regarding clarity and lack of human evaluation. However, clarity issues seem to be resolved through author/reviewer discussions and the updated revision.

R3 had important concerns regarding topic and sentiment relevance evaluations.  
While the reviewer remains unconvinced after discussions with authors, after carefully reading the revised paper and discussions, I feel that the authors tried to address this point fairly  through their additional experiments and also edited their contribution statement accordingly.

Overall, at least two reviewers sounded very excited about this work and other than R3's concerns, the general sentiment about this work was positive. Therefore, I recommend weak accept.  

There are still some writing issues that I strongly encourage authors to carefully address in the future versions. Quoting from reviewer discussions:

> Differentiability of the adversarial loss. Authors just added one statement saying "" Through continuous approximation.."" without any more details are given, which continuous approx was used (Gumbel softmax?) and how they overcame the problem of its training instability. 

> Table 6, can be misleading, authors bold the results when cocon+ is performing better than baselines (mostly in content similarity) but not the other way around topic/sentiment accuracy. The latter is arguably more important.","The paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviews indicated that the overall proposed method of fine-grained controlled generation with self-supervision is valuable, and empirical results support its effectiveness. However, some reviewers initially raised concerns regarding clarity and lack of human evaluations."
"This paper deals with continual learning in semantic segmentation.  Authors introduce wider convolution at final feature extraction layer and apply dropout to limit the overcompression issue.  
No reviewer was convinced by the approach and they have raised many issues, including model design choice, training protocol and missing experiments.  
No rebuttal has been provided by the authors.  
As it is, this submission is not ready for publication, and we encourage the authors to consider the reviewers feedbacks for future publication.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The paper presents a multi-granularity input represenaion and a pyramid attention mechanism for code summarizaiton tasks. After extensive discussion, the reviewers still cannot agree on accepting or rejecting this paper. The key discussion points and my opinion are summarized in below. 


1. Performance improvement -- a few reviewers point out the performance improvement is relatively small (about 1%) compared with baseline. With the additional error bar provided by the authors, it seems to me the improvement is statistically significant. The authors also provide sufficient ablation study to justify the improvement. Although it's arguable if the proposed appracoh is substantial, the progress of AI is often driven by incremental improvement in terms of performance. Therefore, I'm less concerned by this issue. 

2. Comparison only on 1 language and 2 datasets. I partially agree with the authors and reviwers Xtyo that the paper already conducted extensive experiments and the merits of the proposed approach are justified. However, I disagree with the attritue that the comparison on 1 language is sufficient given the recent progress of code summariziton. As the proposed approach is mainly justified by empirical comparison, conducting results on a limited dataset raises the concern whether the proposed approach is generalizble to other languages and datasets. It also makes future work harder to compare with this work. I especially disagree the point that some earlier papers only compared on limited datasets. These papers are published earlier than the benchmark CodeXGlue has been released. As most recent baselines are compared on CodeXGlue, there is a need to justify the missing of results on this dataset. Besides, the argument of the dataet is noisy the performance is low do not seem rigorous and reasonable to me.

3. Human evaluation. The authors provide a preliminary study of human evaluation on the generated outputs. On one hand, it shows the proposed approach indeed improve the quality of the summary, but on the other hand, the study requires more rigorous design. I would suggest including the human evaluation on the main text rather than in appendix. 

4. Presentation. The paper is mostly well-written and provide nice intuition behind the propose method. However, I also agree with Q2Zz that some statements might overclaim and require justification. The later part of the paper has significant number of typos and require a careful proofread. It's pity that the authors do not take the opportunity during the rebuttal period to revise the paper.

Overall, I think the paper has sufficient merits but still have room to improve. ","The paper presents a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers disagreed with the attritue that the comparison on 1 language is sufficient given the recent progress of code summariziton. The authors also provide a preliminary study of human evaluation on the generated outputs."
"After reading the reviews and rebuttal and looking over the paper, I feel that the results are indeed strong, and the paper could have an impact in terms of exploiting the relationship among action dimensions. Maybe the only detail that I would add is that going through the example given in Fig 1 completely could be useful as it might not be perfectly obvious how (e.g. considering a simple mixing function like summation) one retrieves the q values for someone not familiar with this particular topic. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"The authors analyze the latent representation of visual features from natural movies in the salamander retina using a U-net. They train an encoder to learn a compressed latent representation from a large population of salamander retinal ganglion cells responding to natural movies, while a decoder samples from this compressed latent space to generate the appropriate movie frame. They characterize its representation of “time in the natural scene” in the latent space of their model.

Overall, the reviewers expressed a lot of interest in the topic and valued the novel application to salamander retinal data. There were some questions about the significance of the finding, and through the rebuttal period, the authors provided a number of experiments to compare against other variants of their baselines (and ablations), with some reviewers increasing their scores in favor of acceptance. 

At the same time, there was concern that the U-Net architecture could potentially reconstruct the movie without any retinal data. Thus, it was not entirely clear whether the model was truly leveraging the retinal data to obtain meaningful outputs. Unfortunately, this concern was not fully addressed in the revision, leaving the reviewers overall with mixed views but the majority in favor of acceptance.
","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed a lot of interest in the topic and valued the novel application to salamander retinal data."
"Dear Authors,

Thank you for submitting your manuscript to CoRL. I'm happy to inform you that your paper is acceptable for publication. We have completed the review of your manuscript and a summary is appended below. 
The reviewers have advised accepting your manuscript as a poster after improvement of the quality of the manuscript presentation based on the comments.
Please note it is crucial to incorporate all provided explanations from authors to convince the reviewers and recommended editing into the final manuscript.

Regards,","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"Meta Review: In this paper, the authors proposed a new method for forecasting and other prediction  analyses for multiple time series dynamic networks. All the reviewers consider that the proposed method is fundamental and useful.

","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers consider that the proposed method is fundamental and useful."
"This paper studies a learning scenario in which there exist 2 classes of examples: ""predictable"" and ""noise"". Learning theory is provided for this setting and a novel algorithm is devised that identifies predictable examples and makes predictions at the same time. A more practical algorithm is devised as well. Results are supported by experiments.

Reviewers have raised a number of concerns (ranging from how realistic this settings is to missing references). Overall they found this work interesting and relevant to ML community and appreciate the effort that authors have put in in their thoughtful response. However, after a thorough deliberation conference program committee decided that the paper is not sufficiently strong in its current form to be accepted.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have raised a number of concerns regarding the generalization of the approach to different datasets."
"This manuscript proposes spread divergences as a technique for extending f-divergences to distributions with different supports. This is achieved by convolving with a noise distribution. This is an important topic worth further study in the community, particularly as it related to training generative models.

The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work, or expressing issues about the clarity of the presentation. Further improvement of the clarity, combined with additional convincing experiments would significantly strengthen this submission.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the paper, or expressed issues about the clarity of the presentation."
"The paper gives an extension of the transformer model that is suited to computing representations of source code. The main difference from transformers is that the model takes in a program's abstract syntax tree (AST) in addition to its sequence representation, and utilizes several pairwise distance measures between AST nodes in the self-attention operation. The model is evaluated on the task of code summarization for 5 different languages and shown to beat two state-of-the-art models. One interesting observation is that a model trained on data from all languages outperforms the monolingual version of the model.

The reviewers generally liked the paper. The technical idea is simple, but the evaluation is substantial and makes a convincing case about setting a new state of the art. The observation about multilingual models is also interesting. While there were a few concerns, many of these were addressed in the authors' responses, and the ones that remain seem minor. Given this, I am recommending acceptance as a poster. Please incorporate the reviewers' comments in the final version. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers generally liked the paper. The technical idea is simple, but the evaluation is substantial and makes a convincing case about setting a new state of the art."
"The paper unanimously receives positive rates thanks to strong motivations and interesting results. As the reviews show satisfaction on the authors’ feedback, the final draft needs to respect it accordingly, for example, about the limitations of this research. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The manuscript proves a new (finite-sample) generalization bound for generalized linear models, using Moreau envelope theory. The paper also provides experimental validation.

While the results only hold for Gaussian data, I believe there is some interesting novel results which might inspire some future work in the learning theory community. (In comparison, several novel frameworks have been created in the past for other problems, in which the initial versions assumed Gaussianity, e.g., the work on support recovery on sparse linear regression, for instance.)

Several technical clarifications regarding the assumptions (e.g., surrogate distribution, comparison to prior results) were asked by the reviewers, which the authors thoroughly and successfully addressed during the rebuttal phase. I recommend adding this to the camera-ready version of the paper, as well as other discussions and clarifications raised by all the reviewers.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"In this paper, the authors tackle the problem of demonstrators and learners having different observation spaces, by proposing an importance-weighted learning algorithm to bring the support of the imitator state marginal closer to that of the expert demonstrator's state marginal. 

All reviewers have voted to weak accept, but it would seem that this year's NeurIPS mechanism of reviewer assignment has resulted in a much higher acceptance rate than typical, with 80%+ of the AC batch having accept votes. As such, I am tasked with the tough job of rejecting some papers that reviewers were only lukewarmly excited about. It pains me to do this to a paper that reviewers have found methodologically correct. I will be recommending this paper be rejected based on calibration against other papers I'm AC'ing, mostly for the following reason:

The HOIL paper assumes a problem setting where:

1. expert observations with more privilege observations than learner: 
2. access to expert observations being high cost and invasive 
3. importance-weighting the data to close the support between learner and expert state distributions mitigates (1) and (2)

However, it's not demonstrated that (1) and (2) is an actual problem in practical applications. Is sensor mismatch between human and autonomous vehicles the actual problem for learning? Do self-driving cars even utilize a policy formulation explored in the HOIL paper? How big are these state / support mismatches in practice, and couldn't they be mitigated by simpler methods? This paper is the first I've heard suggested that sensor mismatch between humans and autonomous vehicle sensors is the bottleneck for performance. If we remove this motivation from the paper and only consider the importance weighting algorithm used for removing out of distribution examples, then it doesn't feel quite as novel as there are many works that propose some kind of distribution-projection step as a way to mitigate state marginal differences between teachers and learners (e.g. CQL). 

In fact, the setting (1) is not only avoided, but actively exploited by some learning papers (Asymmetric Actor Critic, Guided Policy Search, and other ideas) to *boost* the sample efficiency of learning, with the idea that the asymmetry in state support allows experts to provide useful learning signal in a way that the learner cannot adversarially overfit easily. 

From reviewer pD56, 
```
I didn't find the authors' response to be particularly convincing (the self-driving example doesn't seem like a good fit for what they're doing, and I think they're conflating differences in perceptual accuracy with differences in sensing hardware).
```
","This paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have voted to weak accept, but it is not demonstrated that (1) and (2) is an actual problem in practical applications. The paper is the first I've heard suggested that sensor mismatch between humans and autonomous vehicle sensors is the bottleneck for performance."
"This paper was reviewed by 4 reviewers who scored the paper below acceptance threshold even after the rebuttal. Reviewer 4 is concerned about motivation, Reviewer 2 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets - something lacking in this work. Reviewer 3 is concerned about limited discussion on lie groups and the overall benefit of expm(.). Reviewer 1 reverberates the same comments regarding insufficient experiments,  comparisons and limited motivation. We encourage authors to consider all pointers given by reviewers in any future re-submission.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewer 4 is concerned about motivation, reviewer 3 rightly points out that there exist numerous works that use some form of spectral layers in a deep setting on challenging datasets. Reviewer 1 reverberates the same comments regarding insufficient experiments, comparisons and limited motivation."
"The reviewers are in agreement that this is a well-motivated paper and should be accepted. As R1 mentioned, the contribution does not lie in a novel visualization but rather in the process, insights, and patterns learned during the design and evaluation process. 

The reviewers also agreed that the design implications section lacked depth. This is the one area where the paper has the biggest scope of improving. The reviewers have offered suggestions for different approaches to addressing this shortcoming. 

Some other comments worth highlighting:
R1 has raised some concerns regarding how the 5 questions were selection and would like added details regarding the process. 
R2 would like some discussion around the prior approach or set-up that this system replaced. 
R3 has provided detailed feedback on minor changes which will improve the overall readability of the paper and can be accomplished prior to the camera ready submission.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are in agreement that this is a well-motivated paper and should be accepted."
"This paper proposes a latent variable approach to the neural module networks of Andreas et al, whereby the program determining the structure of a module network is a structured discrete latent variable. The authors explore inference mechanisms over such programs and evaluate them on SHAPES.

This paper may seem acceptable on the basis of its scores, but R1 (in particular) and R3 did a shambolic job of reviewing: their reviews are extremely short, and offer no substance to justify their scores. R2 has admirably engaged in discussion and upped their score to 6, but continue to find the paper fairly borderline, as do I. Weighing the reviews by the confidence I have in the reviewers based on their engagement, I would have to concur with R2 that this paper is very borderline. I like the core idea, but agree that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved. I appreciate that the authors have made some updates on the basis of R2's feedback, but unfortunately due to the competitive nature of this year's ICLR and the number of acceptable paper, I cannot fully recommend acceptance at this time.

As a complete side note, it is surprising not to see the Kingma & Welling (2013) VAE paper cited here, given the topic.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are extremely short, and offer no substance to justify their scores. I agree with R2 that this paper is very borderline, and that the presentation of the inference techniques for V-NMN is complex and its presentation could stand to be significantly improved."
"## Description
The paper discovers interesting phenomena in training neural networks with binary weights:
- Connection between latent weight magnitude and how important its binarized version for the network performance
-training dynamics, indicating that large latent weights are identified and stabilize early on
- Observation that amongst learned binary kernel, several specific patterns prevail, up to the bits who's reversal has very little effect. This is so regardless of the architecture, the layer considered or the dataset. 
The paper further demonstrates how these observations may be used to compress binary neural networks below 1 bit per weight.

## Review Process and Decision
The reviewers welcomed the experimental investigation of new phenomena, but commented the overall technical quality of the work as somewhat substandard. The redundancy of consecutive affine transforms is known and not connected to binary weights investigation. The investigation itself lacks a more in-depth analysis. The proposed compression results appeared not convincing to reviewers since a significant drop of accuracy occurs. The AC shares these concerns and supports rejection.

## General Comments
From my perspective, the study undertaken is methodologically „wrong“. An ad-hoc training method is investigated, which is not even clearly defined in the paper (there are many „STE“ variants) and for which it is not known what it is doing, what are the real-valued weights for and whether they are needed at all (as empirically argued by Helwegen et al. (2019)). As such, the investigation makes impression of poking a black box (the training method in this case). At the same time, there are more clear learning formulations, applicable in the setting of the paper (binary weights), in particular considering the stochastic relaxation:
* Shayer et al. (2017): Learning Discrete Weights Using the Local Reparameterization Trick
* Roth et al. (2019): Training Discrete-Valued Neural Networks with Sign Activations Using Weight Distributions
* Peters et al. (2018): Probabilistic binary neural networks

These methods are approximate, but at least the optimization is well posed and it is known what do the real-valued weights represent (e.g. logits of binary weight probabilities).
From this perspective, it can be seen that latent weights close to 0 correspond to Bernoulli weights that are almost fully random (and thus only contribute noise) and are fragile to gradient steps. Therefore the model can only perform well if it learns to be robust to their state or their state becomes more deterministic (corresponding to large latent weight). So one would actually expect to see in these models phenomena similar to the observed in the paper and not bee too much surprised or astonished by them. Furthermore, there are recent works explaining STE and its latent weights as optimizing the stochastic relaxation:
* Meng et al. (2020): Training Binary Neural Networks using the Bayesian Learning Rule
* Yanush et al. (2020): Reintroducing Straight-Through Estimators as Principled Methods for Stochastic Binary Networks. 

The authors are encouraged to make the observed phenomena more explainable by connecting to the mentioned works.

## Further Details

*  „We show that in the context of using batch normalization after convolutional layers, adapting scaling factors with either hand-crafted or learnable methods brings marginal or no accuracy gain to final model.“

From theoretical perspective, this is obvious and known to me. Practically, there could be in principle some difference due to the learning dynamics, and verifying that there is none is a useful but a weak contribution. The section devoted to this issue can be given in the appendix but is not justified in the main paper.

* „change of weight signs is crucial in the training of BWNs“

The sign determines the binary weights, so this is by definition.

* „ Firstly, the training of BWNs demonstrates the process of seeking primary binary sub-networks whose weight signs are determined and fixed at the early training stage, which is akin to recent findings on the lottery ticket hypothesis for efficient learning of sparse neural networks“

In the lottery ticket hypothesis paper it is shown explicitly that the identified sparse subnetwork changes during the learning the most rather than retains its initialization state or the state in the beginning of the training. It is therefore could be of a different nature.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, although some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers welcomed the experimental investigation of new phenomena in training neural networks with binary weights, but commented the overall technical quality of the work was somewhat substandard."
"This paper demonstrates that deep networks can memorize adversarial examples of training data with completely random labels, which motivates some analyses on the convergence and generalization of adversarial training (AT). The authors identify a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback. Experiments on benchmark datasets validate the effectiveness of the proposed algorithm. One of the reviewers is concerned about (1) the validity of stability analysis where 80% of the data labels are noisy, and the perturbation (64/255) is large, (2) the gap between theory and practice, and (3) novelty. The authors have made a great effort to address these concerns. Although there is still no consensus after the author's response, the majority of the reviewers are in strong support. I, therefore, recommend acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers identified a significant drawback of memorization in AT that could result in robust overfitting and propose a new algorithm to mitigate this drawback."
"This submission proposes a method for learning convolutional filters with trainable size, that builds on top of multiplicative filter networks.  Anti-aliasing is achieved by parametrization with anisotropic Gabor filters.  The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR.  The authors are encouraged to make use of the extensive reviewer discussion in improving the final version of the paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were unanimous in their opinion that the paper is suitable for acceptance to ICLR."
"Strengths 

The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well.
The challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the
separation of internal state from external state is a clean principle that can potentially be broadly employed. 
The method does well in outperforming the alternative baselines.

Weaknesses

There is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses 
a policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as ""Virtual Windup Toys for Animation"" 
exploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help. 
The separation of internal and external state is an assumption that may not hold in many cases.
The results are locomotion focussed. There are only two timescales.

Decision

The reviewers are largely in agreement to accept the paper. 
There are fairly-simple-but-useful lessons to be found in the paper
for those working on HRL problems, particularly those for movement and locomotion. 
The AC sees the novely with respect to different pieces of related work is the weakest point of the paper.  
The reviews contain good suggestions for revisions and improvements;  the latest version may take care
of these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution
to ICLR 2019.
","The paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well in outperforming the alternative baselines. The results are locomotion focused and there are only two timescales. Overall, the paper will make a good contribution to ICLR 2019."
"This paper proposes a differentiable meta-solver applicable to large-scale combinatorial optimization. After a thorough discussion phase, all the reviewers are on the positive side of this paper. The reviewers appreciated the novelty of this paper and the importance of scaling neural combinatorial optimization for large-scale instances. Overall, I recommend acceptance for this paper.

However, the reviewers also showed concerns about the presentation of this paper. The gap between generalization and testing performance is not clearly discussed and the connection to prior works using continuous latent space should be clearly stated. Since scalability is an important issue, it would be useful to clear up time/objective comparison and unify experimental settings as suggested by Reviewer fQdp and fe3B.


","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers appreciated the novelty of this paper and the importance of scaling neural combinatorial optimization for large-scale instances."
"The paper contains interesting ideas and fits well within MIDL's short paper scope of also providing a live discussion space for accepted journal publications. Given that unfortunately only one reviewer was available, whose weak reject is mostly based on formatting, I believe that the paper can still be accepted. However, I strongly urge the authors to reformat their paper to be consistent with the MIDL style when preparing the camera-ready version.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes improving human interpretability and manipulability of neural representations by obtaining syntactic roles (here, subject, object, prepositional object, and main verb) without supervision by means of them becoming linked to latent variables in a novel proposed attention-driven VAE (ADVAE) model, which provides cross attention between a language transformer and latent variables. The paper argues that syntactic roles are quite central to meaning interpretation and that the ADVAE recovers them better than LSTM or Transformer (with mean pooling) VAEs.

This is a quite interesting direction and paper. There was active discussion with the reviewers, one of whom (9pDc) moved their rating from reject to quite strong support, while the other reviewers either sat on the fence or raised from reject to borderline. Nevertheless, I overall tend to agree that the paper is still lacking in empirical support, a view clearly shared by reviewers WuPD and 7uFL. The SNLI data is very simple descriptive sentences, nearly all in the form of S V O or S V PP. Would this work on more complex data, in other languages, or with more word order variation? There isn't very much investigation, but the new results added during reviewing based on Yelp data seem to offer more concerns than confidence. These are also very short sentences but with more varied structure and some complementation. It seems like D_{dec} is now very low (much lower than for the sequence VAE), the ability to distinguish out grammatical roles seems limited to {subj} vs. {dobj, pobj} in the encoder and none at all in the decoder (Figure 6/7). And then for the examples in Appendix D, the disentanglement abilities barely seem stronger than being able to pick out subjects, though when there are sentences with subordinate clauses, it is perhaps random which subject you get. The resampled realizations in appendix H also seem to show limited disentanglement: resampling the subject usually seems to change the object as well, often markedly. No convincing downstream applications are shown. As such, while I agree that disentanglement is at the heart of representation learning, I can't get on board with reviewer 9pDc feeling that this paper now has convincing results. Reviewer 7uFL also emphasizes that there is no strong reason that the latent variables have to align with syntactic roles. In particular, the motivation in NMT whereby constituents clump and reorder together does not exist here. It may only work for the very simple and regular sentences of SNLI.

Hence, overall, I feel that this method needs more extensive validation on harder, more varied data sets before it becomes a convincing contribution, and so I propose rejecting the paper at this point in time. Nevertheless, I do think the topic is interesting and this approach has the potential to be good.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, although some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers also expressed concerns about the generalization of the approach to different datasets."
"This paper introduces a prompting technique for eliciting factual knowledge from frozen pretained transformer LMs. The key idea is to modify the embeddings produced by the embedding layer before they are passed to the first attention layer and the paper investigates several different design choices. The Reviewers all agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns, raised by Reviewer jddf, were about clarifying the connections to the robust optimization literature and evaluating on OOD relations. The former has been addressed in the revised version. While the latter point remains valid, I find that the paper in its current state has enough useful experiments and analysis to warrant publication. The authors have clarified most of the other points raised by the reviewers in their rebuttal.","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the paper tackles an important problem with interesting methods, that it is well written and has strong results. The main concerns raised by Reviewer jddf were about clarifying the connections to the robust optimization literature and evaluating on OOD relations."
"The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results.  

During the review period, the reviewers agreed that the paper has certain merits, and on the other hand, they also raised some concerns, regarding some missing technical details, whether the empirical finding could be trusted, the generalization of the findings to more scenarios, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which removed many of the above concerns (although not all) and convinced the reviewers to raise their scores. As a result, we believe it is fine to accept the paper (although somehow like a weak accept).","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, but some concerns about generalization were noted. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the paper has certain merits, and on the other hand, they raised some concerns regarding some missing technical details, whether the empirical finding could be trusted, and the comparison with some highly related papers. The authors did a good job in their rebuttal, which is based on the results of the reviewers. The paper studies the behavior cloning based strategies of offline RL algorithms."
"This paper considers the problem of online reinforcement learning in two-player zero-sum Markov games. They consider a class of Markov games called kernel mixture Markov games, which extend the linear mixture MDP setting considered in prior work to the RKHS setting. The main result is to propose and algorithm called KernelCCE-VTR, which uses the principle of value-targeted regression to achieve regret bounds that scale with the effective dimension of the kernel. The authors also provide an improved variant of the algorithm based on Bernstein-type confidence bonuses.

The reviewers found the setting to be important and found the paper to be well-written, and agreed that the main results are technically challenging and likely to be a useful starting point for future work on learning Markov games with function approximation. The main issue raised by the reviewers is that the algorithm design and analysis appears to be based on a combination of well-known existing techniques for simpler settings---for the final revision, the paper can be strengthened by more strongly advocating for the novelty required in combining these techniques.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main issue raised by the reviewers is that the algorithm design and analysis appears to be based on well-known existing techniques for simpler settings. The reviewers agreed that the main results are technically challenging and likely to be useful starting point for future work on learning Markov games with function approximation."
"The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. Improvements are seen on only some datasets and the comparisons are not exact.
A reject.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. The reviewers agree that the idea of introducing structural biases in the attention mechanism is interesting but the results and presentation right now is not convincing. The reviewer agrees that the paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods."
"Most reviewers were positive about the paper, seeing that the proposed method is practical and has convincing experimental performances. One reviewer was a bit negative and raised questions about clarity. After the authors responded, the negative reviewer didn't respond further. After reviewing all the comments, the AC feels that there is enough support from reviewers to accept this paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The AC feels that there is enough support from reviewers to accept the paper."
"The reviewers unanimously agree that the theory here exhibiting a particular case where Gaussian process priors are inferior to deep Gaussian processes is interesting, and furthermore that the proof techniques themselves are novel. Indeed, reviewers had minimal or no substantial concerns about the paper, and most of the questions asked by reviewers txpX and sPbe read as simple follow up questions that the authors may choose to include discussion on.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The reviewers unanimously agree that the theory here exhibiting particular case where Gaussian process priors are inferior to deep Gausssian processes is interesting, and furthermore that the proof techniques themselves are novel."
"Reviewers agreed that the model is new and interesting and the theoretical results are solid. The main criticisms are about the model: some reviewers felt that it is too specific and there is not enough motivation. Some reviewers liked the technical depth while others felt that it is not enough to compensate for the lack of motivation. Overall, reviewers felt that the paper in its current form is not ready for publication at NeurIPS.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"All reviewers agree that the writing is not precise. It does not help to find any novelty in the ideas, and the limited and too quickly described experiences are not convincing enough to forgive this problem. The authors chose not to oppose or comment on the detailed arguments provided by the reviewers. I agree with the reviewers in recommending the rejection of this paper. ","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
The paper considers the question of identifying bad data so that models can be trained on the subset of data that is good. This question is formulated as a utility optimization problem. The paper shows that some popular heuristics are quite bad in the framework they propose. They also propose a new algorithmic framework called DataSifter. There is empirical evaluation provided for this. Questions have been raised in the reviews about the size of the models that have been used in the empirical evaluation. The authors have responded to this by suggesting the use of proxy model techniques. There are also questions about learnability of data utility for which some responses are provided in the rebuttal.,"The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers have responded to this by suggesting the use of proxy model techniques. There are also questions about learnability of data utility."
"This paper presents a new task and model for predicting the hierarchical structure of organizations / institutes. The model predicts is-ancestor relationships between the institutions by modeling the set operations between the strings. The authors develop a new dataset, automatically derived from GRID, and compare their set-based model against a few baseline approaches. 

The reviewers’ comments were generally positive and praised the usefulness of the task, which makes us recommend acceptance. However, there were some concerns about the experimental setup (choice of baselines and evaluation).   We strongly recommend improving these aspects on the final version, as per the reviewers’ suggestions.","This paper presents a new task and model for predicting the hierarchical structure of organizations / institutes. The model predicts is-ancestor relationships between the institutions by modeling the set operations between the strings. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This work presents a straightforward and easy to understand method for using hypernetworks to adapt existing models to be able to increase their output space. The method itself is also interesting and is detailed enough for reproducibility. However, the experiments and results should be improved by expanding the demonstration of CHNs beyond the narrow P-VAE application and comparing against relevant baselines in the recommendation system literature.

Pros
- Clear writing.
- Detailed hyperparameters to aid reproducibility.
- Straightforward model.

Cons
- Lack of sufficient comparison to related work, especially to existing recommendation systems that handle the cold-start issue and to Vartak, 2017.
- Limited results that only demonstrate application to P-VAE meaning it's still unknown if CHNs work well with other models. The result on the synthetic dataset is also less persuasive.
- Lack of sufficient ablations, i.e. training a SVM/linear regression model until convergence.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The main goal of this paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of DGS (Zhang et al., 2020) method. This paper applies a line-search of the step-size parameter of DGS  to reduce tuning.  A heuristic update rule of the smooth parameter in DGS (Zhang et al., 2020)) is also used. 

Pros
+ The topic of learning hyperparameters on the fly is well-motivated and an important direction to improve many existing methods.
+ A wide range of tasks are considered in the experiments are interesting, with a wide range of tasks considered. 


Cons
- Reviewers have found the contribution to be incremental and marginal. Specifically, the reviewers have expressed concerns on the impact of the work since it verifies improvement upon DGS only. The paper could be stronger by showing evidence of improving other algorithms. 
-  In the work of [Zhang et. al., 2020], there are two parameters that require careful selection - the learning rate and smoothing radius. However, the proposed approach still relies on some hyperparameters. Although the authors claim that the method is not sensitive to these hyperparameters, it could be better justified. 
- The initial version lacks evaluation of the adaptive mechanism, although the authors added the comparison in the revised version. 
- The paper could be further improved by comparing against other hyperparameter optimization methods. 

We acknowledge the detailed response and the modifications of the manuscript. We believe the paper will make a more profound contribution and impact after addressing some of the major concerns raised by the reviewers. 
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main goal of the paper is to develop a new adaptive strategy to remove the need of hyperparameter fine tuning, which hinders the performance of DGS (Zhang et al., 2020) method."
"The paper proposes a new algorithm for dataset distillation, based on two key ideas:  (1) train a linear layer given the fixed feature extractor, and (2) use a diverse set of modes as feature extractors. The paper has received overwhelmingly positive reviews. Many reviewers find the algorithm effective, the paper well-written, and the results compelling. The rebuttal further addressed the concerns regarding the backbone models and missing experiments as well as provided additional clarifications. The AC agreed with the reviewers’ consensus and recommended accepting the paper. ","The paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed with the reviewers’ consensus and recommended accepting the paper."
"Strengths:
* Well-written paper
* Strong empirical results on three benchmarks
* Interesting approach of producing semantically augmented LMs using dependency parses to extract svo triples, and finding coreferences between them across multiple sentences

Weaknesses:
* None of the reviewers seem particularly excited about the paper
* Stronger baseline comparisons would have improved the paper
* Authors re-define a lot of terminology, but the novelty of the method is more from the type of graph used to initialize their method, which seems to be a function of OpenIE triplets","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of each review. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"The authors take two algorithmic components that were proposed in the context of discrete-action RL - priority replay and parameter noise - and evaluate them with DDPG on continuous control tasks. The different approaches are nicely summarized by the authors, however the contribution of the paper is extremely limited. There is no novelty in the proposed approaches, the empirical evaluation is inconclusive and limited, and there is no analysis or additional insights or results. The AC and the reviewers agree that this paper is not strong enough for ICLR.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that the paper is not strong enough for ICLR."
"One way of avoiding catastrophic forgetting in continual learning is through keeping a memory buffer for experience replay.  This paper addresses the problem of online selection of representative samples for populating such memory buffer for experience replay.  The paper proposes novel information-theoretic criteria that selects samples that captures surprise (samples that are most informative) and learnability (to avoid outliers).  They utilize a Bayesian formulation to quantify informativeness. They provide two algorithms: a greedy approach, and an approach that takes timing (when to) update memory into account based on reservoir sampling to mitigate possible issues with class imbalance.

Pros:  The paper is well written and organized.  It was easy to follow. The formulation is novel and technically sound. The idea of taking learnability into account is novel and interesting.  It provides a nice way of avoiding outliers and balancing surprising information. The authors presented the motivation for each part of the framework well.  

Cons: To understand the contribution of each component of the formulation and competing criteria, an ablation study is needed.
Reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters, additional citations, additional data sets beyond MNIST and CIFAR10, etc.  

In the rebuttal, the authors have addressed several of these concerns.  Please make sure to include and incorporate reviewer suggestions in the final revised version.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is well written and organized. The formulation is novel and technically sound. The authors presented the motivation for each part of the framework well. The reviewers had several detailed suggestions and questions, including sensitivity to hyperparameters and additional citations."
"None of the reviewers championed the paper. Many weaknesses were shared across the reviewers: none of the individual contributions is individually novel, paper is not well written and the results do not show significant improvement over the prior state of the art. No rebuttal was provided. The AC agrees with the reviewers that the paper is not ready for publication at ILCR.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The AC agrees that the paper is not ready for publication at ILCR."
"The paper presents a new deep learning approach for combinatorial optimization
problems based on the Transformer architecture. The paper is well written
and several experiments are provided. A reviewer asked for more intuition to
the proposed approach and authors have responded accordingly. Reviewers are
also concerned with scalability and theoretical basis.
Overall, all reviewers were positives in their scores, and I recommend accepting the paper.","The paper presents a novel neural network training approach for combinatorial optimization problems based on the Transformer architecture. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were also concerned with scalability and theoretical basis, and I recommend accepting the paper."
"There are some interesting ideas raised on continuous-time models with latent variables in machine learning. However, the reviewers argue, and I agree, that the connection to causal models as typically required in applications about the effects of interventions is not addressed with as much care as it might have been needed.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The papers studies machine learning tasks in the presence of adversarially corrupt data (during training). In particular, it is assumed that the labels of a small constant fraction of the datapoints are arbitrarily corrupted.  The paper proposes a natural method to solve this problem and evaluates it on various datasets. As pointed out by the reviewers, the theoretical contributions of this paper are subsumed by a number of prior works (which were not initially cited). The experimental results of the paper are interesting. However, the method proposed  and evaluated is not particularly novel. In my opinion, the problems studied in this submission are important (in particular, the memory/space consideration in the context of robustness). However, this work still needs work and is not ready for publication.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The theoretical contributions of the paper are subsumed by a number of prior works (which were not initially cited)."
"The submission received split reviews: two reviewers recommended weak accepts, and the other two weak rejects.  The AC went through the reviews, responses, and discussions carefully.  The AC agrees that this paper is well-written and has demonstrated the possibility of using transformers for particle-based physical simulation.  The AC also believes that the authors have addressed the concerns of reviewer dYcg, despite that the reviewer didn't engage in the discussions.  The contributions are however not most exciting, and none of the reviewers would like to champion the submission.

Further, the AC agrees with the knowledgeable and responsible reviewer ZsKn that the presentation and experiments can be better positioned to highlight the key contribution. As reviewer ZsKn has summarized, it's recommended that ""the authors took the approach of integrating the different parts of the newly proposed layer into existing architectures (possibly including non-simulation settings), and try to understand better that way how the new layer may help in a more apples-to-apples comparison.""

The recommendation is reject, and the authors are encouraged to revise the paper for the next venue.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper proposes a model for learning using ensemble clustering. The reviewers found the general idea promising.
However, while promising, all reviewers noted that in its curent form the paper is not fit for publication. The reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work.
Because of all these reasons, this paper does not meet the bar of acceptance. I recommend the authors take into account the feedback provided in the reviews and discussion and resubmit to another venue.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted that in its curent form the paper is not fit for publication. However, the reviewers pointed out missing references, issues with the abstract, lack of motivation for some of the algorithmic choices, limited novelty over clarity in the description of difference w.r.t. previous work"
"The paper conducts thorough analysis of the Conformer architecture and brings insights and techniques from other fields to simplify and improve the model structure, which is also demonstrated to show nice gains. Though as pointed by reviewers the novelty is limited, the study is very useful to the field. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The authors show how to perfectly sample a discrete distribution, given sample access to the Bradley-Terry-Luce model on subsets of size 2. While the learning problem has been previously studied extensively, this work initiates the study of sampling. Technically, the authors introduce re-weighting and rejection sampling ideas that speed up coupling from the past by utilizing an approximate learning algorithm; these techniques could be useful in other applications, as the authors hint in the rebuttal.

The reviewers agreed that the paper is technically quite strong and that it's quite well written. The authors responded to all remaining questions by the reviewers, clearing the path to the paper being accepted.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the paper is technically quite strong and that it's quite well written. The authors also introduced re-weighting and rejection sampling ideas that speed up coupling from the past by using an approximate learning algorithm. These techniques could be useful in other applications, as the authors hint."
"This work analyses the use of parameter averaging in GANs. It can mainly be seen as an empirical study (while also a convergence analysis of EMA for a concrete example provides some minor theoretical result) but experimental results are very convincing and could promote using parameter averaging in the GAN community. Therefore, even if the technical novelty is limited, the insights brought by the paper are intesting. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This is a very high-quality dataset, which is especially noteworthy since legal NLP benchmarks are very difficult to build. The authors approached every aspect of the process with extreme care. It will likely have an impact on law practice and start interesting discussions about the use of ML in these settings. The paper is also very well written and enjoyable to read.

Most reviewers are heavily in favor of acceptance. Reviewer Aeqk brought up some weaknesses, but at least from the AC's perspective, these seem to be answered well by the authors.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are strongly in favor of acceptance. The paper is very well written and enjoyable to read."
"### Paper summary 
This paper investigates theoretically and empirically the effect of increasing the number of parameters (""overparameterization"") in GAN training. By analogy to what happens in supervised learning with neural networks, overparameterization does help to stabilize the training dynamics (and improve performance empirically). This paper provides an explicit threshold for the width of a 1-layer ReLU network generator so that gradient-ascent training with a linear discriminator yields a linear rate of convergence to the global saddle point (which corresponds to the empirical mean of the generator matching the mean of the data). The authors also provides a more general theorem that generalizes this result to deeper networks.

### Evaluation
The reviewers had several questions and concerns which were well addressed in the rebuttal and following discussion, in particular in terms of clarifying the meaning of ""overparameterization"". After discussing the paper, R1, R2 and R4 recommend acceptance while R3 recommends rejection. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator  distribution (produced from a *fixed* set of latent variables z_i) and the empirical mean of the data. R3 argues that this is not sufficient to ""understanding the training of GANs"". At least two aspects are missing: how the distribution induced by the generator converges according to other notion of divergence (like KL, Wasserstein, etc.); and what about the true generator distribution (not just its empirical version from a fixed finite set of samples z_i)? While agreeing these are problematic, the other reviewers judged that the manuscript was useful first step in understanding the role of overparameterization in GANs and thus still recommend acceptance. And importantly, this paper is the first to study this question theoretically.

I also read the paper in more details. I have a feeling that some aspects of this work were already developed in the supervised learning literature; but the gradient descent-ascent dynamic aspect appears novel to me and the important question of the role of overparameterization here is both timely, novel and quite interesting. I side with R1, R2 and R4: this paper is an interesting first step, and thus I recommend acceptance. See below for additional comments to be taken in consideration for the camera ready version.

### Some detailed comments
- Beginning of section 2.3: please be clearer early on that you will keep V fixed to a random initialization rather than learning it. The fact that this is standard in some other papers is not a reason to not be clear about it.
- Theorem 2.2: in the closed form of the objective when $d$ is explicitly optimized, we are back to a more standard supervised learning formulation, for example (5) could look like regression. The authors should be more clear about this, and also mention in the main text that the core technical part used to prove Theorem 2.2 is from Oymak & Soltanolkotabi 2020 (which considers supervised learning). This should also a bit more clear in the introduction -- it seems to me that the main novelty of the work is to look at the gradient-descent dynamic, which is a bit different than the supervised learning setup, even though some parts are quite related (like the full maximization with respect to $d$).
- p.6 equation (8): typo -- the  $-\mu d_t$ term is redundant and should be removed as already included from $\nabla_d h(d,\theta)$.
- p.7 ""numerical validations"" paragraph: Please describe more clearly what is the meaning of ""final MSE"". Is this a global saddle point (and thus shows the limit of the generator to match the empirical mean), or is this coming from slowness of convergence of the method (e.g. after a fixed number of iterations, or according to some stopping criterion?). Please clarify.
","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also reviewed the paper in terms of clarifying the meaning of ""overparameterization"" in GAN training. The main concern of R3 is that the GAN formulation analyzed in the paper is mainly doing moment matching between the generator distribution (produced from a fixed finite"
"Meta Review: The paper studies the intersection between fairness, privacy, and accuracy. Reviewers are overall positive about the novel insights that the paper provides. Minor concerns are well covered by the rebuttal.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are overall positive about the novel insights that the paper provides."
"The paper presents a conformal prediction approach to supervised classification, with the goal of reducing the overconfidence of standard soft-max learning techniques. The proposal is based on previously published methods, which are extended for use with deep learning predictors. Empirical evaluation suggests the proposal results in competitive performance. This work seems to be timely, and the topic is of interest to the community.

The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the proposed work or expressing issues about the strength of the empirical evidence supporting the claims. Additional experiments would significantly strengthen this submission.","The paper presents a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers and AC opinions were mixed, with reviewers either being unconvinced about the novelty of the paper or expressing issues about the strength of empirical evidence supporting the claims."
"The reviewers are in agreement that this paper provides a minimax optimal solution to the problem of offline linear contextual bandits. This new family of learning rules beat state of the art approaches and provide a unified view on existing approaches, such as Lower Confidence Bound and Bellman-Consistent Pessimism. The theoretical results are backed by reasonable numerical simulations. Accept. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are in agreement that this paper provides a minimax optimal solution to the problem of offline linear contextual bandits."
"Interesting idea, well written paper. I suggest the authors to include and compare with relevant previous work as suggested by the reviewers.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"Summary:
This paper provides an interesting and unique challenge problem on human-AI collaboration, with sample baselines. I think this is an extremely important topic and the community should embrace such challenge problems.

Discussion:
Reviewers agreed this paper should be accepted, particularly after seeing that ICLR has accepted such challenge papers in the past.

Recommendation:
I'd really like to see this get a spotlight as it would be great to highlight this innovative challenge to the community. ","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Reviewers agreed this paper should be accepted, especially after seeing that ICLR"
"The paper is well written and develops a novel and original architecture and technique for RNNs to learn attractors for their hidden states (based on an auxiliary denoising training of an attractor network). All reviewers and AC found the idea very interesting and a promising direction of research for RNNs. However all also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope. Reviewers demand experimental comparisons with other (simpler) denoising / regularization techniques; more in depth experimental validation and analysis of the state-denoising behaviour; as well as experiments on larger datasets and more ambitious tasks. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also agreed that the experimental validation was currently too limited, in type and size of task and data, as in scope."
"The methodological contributions and technical novelties are relatively small, but the dataset is very useful. Therefore, if the authors want to claim that the dataset is a major contribution, the dataset should be publicly released; otherwise, this contribution cannot be realized.
","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The methodological contributions and technical novelties are relatively small, but the dataset is very useful. Therefore, the dataset should be publicly released; otherwise, this contribution cannot be realized."
"The reviewers all appreciated the novel concept behind the work. I agree with this, I think the principles behind the work are novel and interesting, and I would encourage the authors to improve the validation of this method and publish it in the future.

However, reviewers also raised a number of issues with the current paper: (1) the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablations/comparisons; (2) it's not clear if the improvements from the method are especially significant; (3) the writing could be improved (I do see that the authors made a significant number of changes and improved parts of the paper in response to reviewer concerns to a degree). Probably the writing issues could be fixed, but the skepticism about the experiment results seems harder to address, and while I recognize that the authors made an effort to point some existing ablations in the paper that do address parts of what the reviewers raised, I do think that in the balance the experimental results leave the validation of the work as somewhat borderline.

While less important for the decision, I found that the paper is somewhat overselling the contribution in the opening -- while the particular concept of using gradients as features in this way is interesting, similar ideas have been proposed in the past, and the paper would probably be better if it was more clearly positioned in the context of prior work rather than trying to present a new ""framework"" like this. It kind of feels like it's biting off too much in the opening, and then delivering a comparatively more modest (but novel and interesting!) technical component.","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers also raised a number of issues with the paper: (1) the evaluation appears a bit preliminary, and could be improved significantly with additional datasets and more ablations/comparisons; (3) the reviewers made a significant number of changes and improved parts of the paper in response to reviewers' concerns."
"The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.

The experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning. 

There are concerns on writing style and comprehension. 
- The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods. 
- The presentation is very dense and quite hard to grasp, even with the Appendix.
- The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper. 
I would recommend including the response to R1 in the paper.

Other relevant and concurrent papers to potentially take note of:
- Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization (https://openreview.net/forum?id=wiSgdeJ29ee) 
- Robust Offline Reinforcement Learning from Low-Quality Data (https://openreview.net/forum?id=uOjm_xqKEoX)

Given the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re-writing to make the manuscript more accessible, which in turn to increase impact of this work. ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper studies offline RL, which is an important topic in high risk domains. This method is pretty general and could be paired with other pessimistic model-based RL methods."
"This paper aims to look at the relationship between disentanglement
and multi-task learning.  The authors claim to show that disentanglement
emerges naturally from MTL.

The main discussion was whether the claim that disentanglement emerges
naturally from MTL has been adequately demonstrated.  The main  
issue is that MTL results in more extraction of information and that
is hard to disentangle from the disentanglement metrics used.

Reviewers agreed the work was interesting but not as complete as would
be desirable.  I also feel it is not ready for ICLR presentation, but  
with further work could be a nice future contribution.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main issue is that MTL results in more extraction of information and that is hard to disentangle from the disentanglement metrics used."
"This paper introduces sparse modeling-inspired regularizations to improve deep neural network-based image generators. Experimental results on both (low-resolution) image synthesis and deep image prior-based inverse problems are used to validate the proposed method.
The majority of the reviewers were against the acceptance of the paper. As summarized by Reviewer tsoA: ""There are shortcomings in the overall concept as well as its evaluation. The findings suggest that this might be a promising avenue of research, but it would need to be taken further. At present, the paper boils down too much into simply adding a simple regularizer at the end and observing that it somewhat improves some metrics in a limited number of scenarios. Due to the limitations of the evaluation, it remains unclear whether the proposed improvement carries over to state of the art models and datasets. Similarly, the promised elucidation of the purpose of the feature values never really materializes."" The AC agrees with that summarization and recommends rejection.","This paper introduces sparse modeling-inspired regularizations to improve deep neural network-based image generators. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The AC agrees with that summarization and recommends rejection."
"The paper studies the approximation properties of group convolutional neural networks. It establishes the “cc-universality” of group CNNs, i.e. that such networks can approximate any continuous function over any compact set, using a new constructive approach which is based on a generalization of the ridgelet transform. The proof is constructive, in the sense that approximating networks are given in closed form by discretizing the transform. This approach may have applications beyond the scope of the paper — most immediately, to identifying classes of functions for which neural network approximations are accurate in a quantitative sense; this can be tied to the decay properties of the ridgelet transform. Reviewers found the paper to be clearly written and of high technical quality, albeit somewhat mathematically dense in its presentation. Universal approximation theorems provide an important piece of theoretical background, as well as a “sanity check” for new network architectures; having a unified, constructive approach to derive them could stimulate further work. ","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper studies the approximation properties of group convolutional neural networks. It establishes the “cc-universality” of group CNNs, i.e. that such networks can approximate any continuous function over any compact set, using a new constructive approach which is based on the"
"The 4 reviewers all had a consistent view of this paper:  concern that the scope of the work was overstated (paper claims, without evidence, to apply in more generality than the 1 example scenario shown); concern about the difficulty of implementing this approach (1 TSNN required for each rendered viewpoint); and lack of examples showing how the method performs under more challenging scenarios.

The AC encourages the authors to revise the work in response to the reviews.  That would involve additional experimentation and examples, and some attention to revising the manuscript.   After two of the reviewers complained of lack of clarity in the algorithm description, the authors replied, ""We explain our algorithm in the paper; the reader can refer to our code for implementation details.""  I hope the authors can be more responsive to the readers' concerns than that in their revisions.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" The AC encourages the authors to revise the work in response to the reviews. The reviewers expressed concerns about the scope of the work, although some concerns were raised about generalization."
"There are contrasting review rating ranging from 9 to 3. The reasonable concerns about this paper not been well validated for ML community. The proposed datasets has been benchmarked with some classical and some relevant DL methods. I have taken into account the expertise of reviewers and their concerns carefully. I do agree with some reviewers opinion about this dataset will be importance to medical CV community. I do agree with authors response ""The open questions for the ML community that OpenSRH may foster innovative discoveries include the following: 1) domain adaptation between SRH images and other histology images such as H&E images in the large scale TCGA project; 2) using multiple instance learning (MIL) to avoid expensive dense patch annotations; 3) different aggregation methods for patch-based training, including clustering, attention, or MIL; 4) self supervised learning and comparing different augmentation strategies for SRH images; and 5) data efficient training of ViT architectures using SRH data. "" Even though the authors have not verified these innovative applications on the openSRH dataset. I will go with acceptance opinion of few reviewers for this paper.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns about generalization were raised regarding the generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
"This paper presents a new NAS benchmarks for hardware-aware NAS. For each of the architectures in the search space of NAS-Bench-201, it measures hardware performance (energy cost and latency) for six different hardware devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these hardware-aware measurements on as many as six (very different) devices. 

The code has been made available to the reviewers during the author response window and has been checked by the reviewers in the meantime. All reviewers appreciated the paper and gave (clear) acceptance scores. 

Before this work, it was very hard for the average NAS researcher to assess their method properly in a hardware-aware setting, and I expect this work to change this, and to open up the very important field of hardware-aware NAS to many more researchers. For this reason I recommend to accept this paper as a spotlight.

","This paper presents a new NAS benchmarks for hardware-aware NAS. For each of the architectures in the search space of NAS-Bench-201, it measures hardware performance (energy cost and latency) for as many as six (very different) devices. This is extremely useful for the NAS research community, since it takes very specialized hardware domain knowledge (including machine learning development frameworks, device compilation, embedded systems, and device measurements) as well as the hardware to make these measurements on up to six different hardware devices, e.g.,"
"This paper gives a way to learn one-hidden-layer neural networks on when the input comes from Gaussian mixture model. The main algorithm uses [Janzamin et al. 2014] as an initialization and then performs gradient descent. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give a local convergence result when the samples come from a mixture of Gaussian. The paper claims certain behavior in the input data would make the problem harder and slow down the convergence, although the claim is based on an upperbound and would be stronger if there is some corresponding lowerbound.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main contribution of this paper is 1. to give a characterization of sample complexity for estimating the moment tensors when the input distribution comes from a mixture of Gaussian; 2. to give an local convergence result when the samples come from an input mixture."
"The review highlights numerous problems with the current progress of the work. While MIDL encourages early submission of ongoing research as short paper, this work is not mature enough for publication. I therefore recommend rejection.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The review highlights numerous problems with the current progress of the work."
"The paper shows how to make use of a linear program for extracting logical rules for knowledge graph completion. Overall, the reviewers and I agree that this is an interesting and important direction for research. Moreover, the presented approach shows good performance with rather small sets of rules extracted. However, all reviewers point out that the related work is not well discussed. While the authors have improved the related work sections during the rolling discussion, overall the positioning of the new method has still to be improved, including a better empirical comparison across different datasets. Overall, we would like to encourage the authors to polish their line of research based on the feedback from the reviews.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."" Overall, reviewers agree that this is an interesting and important direction for research. The authors have improved the related work sections during the rolling discussion but overall the positioning of the new method has still to be improved, including a better empirical comparison across different datasets."
"The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers.

At the same time, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. In particular, during the discussion phase the reviewers were not able to decipher the algorithm based on the description presented in the paper. It is not clear how the problem is modeled as a bandit problem, what the loss function $\ell$ is minimized and why minimizing it makes sense (assuming, e.g., that $\ell$ it the hinge loss as suggested and the initial prediction is good with a large margin, that is, the loss is zero, equation 6 never changes $x_t$ when the procedure is started from $x$). This connection, since it is the fundamental contribution of the paper, should be much better explained. Once the problem is set up to estimate (maximize?) the reward, it is changed to calculating the difference in the minimization (cf. equation 11), which is again unmotivated. (Other standard aspects of the algorithm should also be explained properly, e.g., the stopping condition of Algorithm 1)

Unfortunately, the paper is written in a mathematically very imprecise manner. As an example, consider equation (6), where $B_p$ and the projection operator are not defined, and while these can be guessed, a projection of the argmin seems to be missing as well in the end (otherwise nothing guarantees that $x_T$, which is the final outcome of the algorithm, remains in the $L_p$ ball). Another example is the $Discrete\ Approximate\ CorrAttack_{Flip}$ paragraph which requires that every coordinate of $x$ should be changed by  $\pm\epsilon$. It is also not clear what ""dividing the image into several blocks"" means in Section 4.1 (e.g., are these overlapping, do they cover the whole image, etc., not to mention that previously $x$ was a general input, not necessarily an image). It is also unlikely that the stopping condition in Algorithm 1 would use the exact same $\epsilon$ for the acquisition function as the perturbation radius for adversarial examples, etc. While some of these inaccuracies and unclear definitions are also mentioned in the reviews, unfortunately there are more in the paper.

The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation.

","The paper presents a new Bayesian optimization method based on the Gaussian process bandits framework for black-box adversarial attacks. The method achieves good performance in the experiments, which was appreciated by all the reviewers. However, the presentation of the method is quite confusing, which currently precludes acceptance of the paper. The authors are encouraged to resubmit the paper to the next venue after significantly improving and cleaning up the presentation."
"The paper presents a method to regularize the discriminator in  GAN training with a ranking loss based on the user preference for a desired set within a larger dataset. The tradeoff between GAN loss and preference loss dependence on the distance of the set to the full dataset and the authors consider two regimes : ""small and major correction"". A major correction is needed when the targeted set is very different from the whole density, authors propose in this scenario to replace samples from the data by samples from the generator. The setting in the paper is interesting and can be useful in practice. 

There was a lengthy discussions between the authors and the reviewers, the discussion pinpointed issues , some of them were addressed in the rebuttal . Some issues remain unanswered regarding the clarity and some claims in the paper.

The clarity of the paper needs further improvement and  1)  clarify section 3  the setup and the background section   2)  justify claims about the method, in  the strong correction scenario  when fresh generated samples are introduced how  is this an effective procedure? (conceptually / theoretically). 
","The paper presents a method to regularize the discriminator in GAN training with a ranking loss based on the user preference for a desired set within a larger dataset. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The paper provides new techniques (algorithmic as well as analytical) to solve black box optimization of smooth functions with constraints. The reviewers are largely in favor of the paper's contributions, and the author responses have helped to clarify several aspects of the presentation and connections to existing work. Therefore, I recommend that the paper be accepted.  ","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers are strongly in favor of the paper's contributions, and the author responses have helped clarify several aspects of the presentation and connections to existing work."
"In general there is agreement under reviewers that the ideas/method presented are somewhat interesting/promising but also that the paper lacks a lot of clarity. Reviewers agree that the paper needs more work (on the method) and more extensive experiments to be convincing, and that in its current form it is not mature enough for publication at ICLR.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"This submission describes an approach to compressing the communication in federated learning. The key idea is using a set of random samples from a prior distribution and then performing importance weighed sampling. The work performs an analysis of the privacy guarantees of this process and experimental evaluation.
The main issue with this work is the authors appear to be unaware that the basic problem they pose is solved in a more comprehensive and lossless way in a recent work https://arxiv.org/abs/2102.12099 (Feldman and Talwar, ICML 2021). That work shows that any differentially private randomizer can be compressed via a simpler algorithm that performs rejection sampling using a PRG. The algorithm does not loose privacy or utility (under standard cryptographic assumptions) while guaranteeing low communication. In contrast this work loses significantly in utility and provides opaque privacy guarantees.
This submission analyzes  a randomized that adds Gaussian distribution and, in particular, the compression technique in (Feldman and Talwar) applies to it. The technique proposed in this work is very similar in spirit (with prior distribution corresponding to reference distribution in the earlier work.
In light of the earlier work I do not think the contributions in this submission are sufficient for publication.","This paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main issue with this work is that the authors appear to be unaware that the basic problem they pose is solved in a more comprehensive and lossless way in an earlier work https://arxiv.org/abs/2102.12099 (Feldman and Talwar, ICML 2021)"
"This paper proposes a method to solve high-dimensional, continuous robotic tasks offering a trajectory optimization and a distill policy. The paper is well-written and the work is promising. It is very relevant for the robotics and RL communities.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper is well-written and the work is promising."
"This paper studies Offline Model-Based Optimization. This paper proposes a gradient-based method for solving Offline MBO problems using infinite-width Deep learning models.
The key novelty of the paper is in proposed use of a distillation objective to constrain the optimized design-score pairs.

All three reviewers identify the novelty of the problem and the approach. 
The paper also presents strong empirical evaluation on standard benchmarks. 

The rebuttal discusses yielded constructive changes in the paper, and the authors are expected to account for the discussion and suggestions in the next iteration of the manuscipt. 
The AC concurs with the reviews and the discussion thereafter. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper presents strong empirical evaluation on standard benchmarks, and the authors are expected to account for the discussion and suggestions in the next iteration of the manuscipt."
"This is an interesting submission, which was overall well received by the reviewers. I would recommend the authors to discuss further the vast modern litterature on efficient computation of Wasserstein distances and their minimization (see, e.g. Peyré and Cuturi 2019, and references therein)","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"This paper focuses on the problem of performing imitation learning from trajectory-level data that includes optimal as well as suboptimal demonstrations.  The authors wish to avoid the requirement of a separate filtering process that would throw away the bad trajectories.  The authors propose a clever innovation that allows for leveraging the policy that is itself being learned to reweight the samples for a next round of weighted behavioral cloning.  The paper is also somewhat theoretically rigorous and provides insight into the problem. 

The reviewers pointed out some initial issues related to clarity and the authors did a good job of addressing reviewer concerns.  Ultimately all reviewers agreed that the core innovation of the paper was interesting and empirically worked reasonably well.  

One older line of work that I think is quite relevant, but which is not discussed, is the empirically observed ""clean-up effect"", described by Michie and colleagues in the 90s (e.g. ""Learning to fly"" Sammut et al 1992).  This clean-up effect is intuitive and reportedly achieved for free in settings where the learning objective is mode-seeking and the dataset is large, insofar as the mean value of the resulting policy *should* produce actions that corresponds to the average action produced by demonstrators in the same situation.  I think it would be worth discussing how the analysis of this paper relates to this empirical phenomenon. In particular, it would be worth clarifying in what regimes the suboptimality of training from a dataset with noisy examples arises and how likely this is to effect the mean value of the learned policy (for context, it is fairly common in practice to evaluate the student policy in BC settings by only using the mean action value; perhaps this point was present in the paper, and I missed it). From a certain perspective, the innovation of this paper is to accentuate the clean-up effect.

As noted by a reviewer, and subsequently incorporated into the paper, the actual algorithm has some similarities to versions of recent ""offline RL"" algorithms (though of course it does not leverage rewards).  In particular, the motif of performing a weighted regression could perhaps be a bit more thoroughly contextualized by connecting it to other weighting factors (e.g. see Critic Regularized Regression).  That said, I leave this entirely to the discretion of the authors.

The final scores were 8, 7, & 6.  I see this as a strong paper and will endorse it for a spotlight.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed that the core innovation of the paper was interesting and empirically worked reasonably well. The paper is also somewhat theoretically rigorous and provides insight into the problem."
"This paper proposes an extra loss to add on top of the contrastive learning. The contrastive learning seek representations invariant to transformation, while the extra loss the authors proposed encourage representations to be equivariant to the transformation (i.e. retain information about transformation in later representations). While reviewers and I agree this is a sensible motivation, and acknowledge good results that authors have obtained, the fact that most, if not all, improvement is combing from the 4-way rotation transformation is a bit unsatisfactory. Furthermore, this additional loss was proposed before and is actually quite well known, so the actual novelty in the proposed technique is somewhat limited. Nevertheless, this paper provides a comprehensive evaluation, obtaining a reasonable improvement, and makes a good case for using an equivariant seeking loss. The authors are strongly encouraged to release their code (including training details for reproducing ImageNet results) as the improvements they present are central to the acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors are strongly encouraged to release their code (including training details for reproducing ImageNet results) as the improvements they present are central to the acceptance."
"This paper presents a closed loop planning system that leverages LLMs for providing candidate plans given the success of action execution and the world state. Central to the approach is querying the LLM given the instruction and text prompts capturing success of skill execution and a description of the scene. 

This is a systems oriented paper demonstrating and evaluating the utility of a LLM for interleaved planning and execution. The paper is well organised and clearly written. The results highlight the utility of LLMs to enable task execution before an explicit learning of a task model (given primitive robot skills amenable to composition). Further, experiments demonstrate contextual plan adaptation and success goal reaching for simulated and real data sets. 

The main limitation of the work was observed to be the specialised nature of feedback prompts necessary for successful task execution. A clarification was sought if the proposed system has a principled way to determining the necessary feedback and to what extent the feedback is to be adjusted for long-running plans. In response, the authors clarified that the feedback is not hand crafted. Further,  the authors articulated their contribution towards demonstrating  the ability of LLM-based planners to integrated multi-modal feedback without focusing on the specific problem of determine how much or how best such a feedback may be generated. In response to a reviewer’s suggestion, the authors provided a comparison of models with and without feedback with an increase in the task horizon. 

Additionally, the reviewers inquired the degree to which plan recovery is possible in the proposed framework. In response, the authors drew attention to results where plan adaptation occurs when an object falls is no longer in the reachable workspace. The reviewers further requested for a comparison with a baseline that searches over a set of high-candidate plans. As suggested, the authors provided a comparison with a Next Best Decoding that uses the next most likely skill. 

Overall, the inclusion of new experimental insights strengthens the paper. A clear articulation of the technical advance in relation to Zang et al. 2022 is also recommended in the main manuscript. ","This paper presents a closed loop planning system that leverages LLMs for providing candidate plans given the success of action execution and the world state. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers noted its potential for significant advancements in the field, although some concerns about generalization were raised regarding the generalization of the approach to different datasets."
The submission receives 5 reviews form 5 reviewers and all are positive about the proposed dataset presented in the submission. AC reads all reviews and discussions and agree with the reviewers. AC recommends to accept the submission as a spotlight.,"Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
"Most prior works on neural kernels have focused on using the ReLU activation. In this work, the authors provide new methods that can approximate multi-layered Neural Network Gaussian Process (NNGP) kernels and Neural Tangent Kernel (NTK) matrices for a wide range of activation functions. All the four reviewers recommended acceptance of the paper.","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All the reviewers recommended acceptance of the paper."
"This paper proposes a transformer-based GAN method and a two-step masking mechanism for time series anomaly detection. The proposed method is demonstrated on a variety of datasets. 

After rebuttals, both Reviewer 73J5 and Reviewer W6VQ remained negative. The main concern is the novelty and significance of the proposed method. ","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The main concern is the novelty and significance of the proposed method."
"This paper introduces new and useful losses, presents a good experimental setup, supply analysis on the bias, and is clearly written. I encourage the authors to discuss similarities and differences to NeuraWarp and pointcloud->SDF methods in their revision. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors are encouraged to discuss similarities to NeuraWarp and pointcloud->SDF methods in their revision."
"This paper presents a significant collection of unlabelled signing data (4.6K hours) across 10 sign languages, which have been pre-processed and converted to pose-keypoints to remove identifiable information. The paper also presents Multi-ISLR a multilingual dataset with label-alignment extracted from 11 other labelled datasets for 7 sign languages. Furthermore, the paper also provides a multilingual model, Sign2Vec, pre-trained on the unlabelled data, and fine-tuned on the labelled data, which shows the SOTA results for known tasks. Lastly, the paper also introduces a dataset for fingerspelling across 7 languages. It's likely that this work will pave the way for new research on multilingual signed language tasks.

**pros:**
* The paper provides a significant expansion for unlabelled and labelled signed data (previous work focused only on Indian Signed Languages)
* The data gathering procedure is well documented and justified
* The experimental results provide evidence of the value of the data.


**cons**
* For the SignCorpus there is a lack of a human verification process, that assesses the quality of the pose keypoints; i.e. whether they can be used by native signers to extract the target gloss.","This paper presents a significant collection of unlabelled signing data (4.6K hours) across 10 sign languages, which have been pre-processed and converted to pose-keypoints to remove identifiable information. The paper also presents Multi-ISLR a multilingual dataset with label-alignment extracted from 11 other labelled datasets for 7 sign languages. The reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"Meta Review: This paper studies the problem of top-K recovery from a fixed set of queries. In each query, the user is presented m items and they select the item with the highest noisy utility. The authors propose a simple solution to this problem, take top-K most frequently chosen items, and prove that it is near optimal. The proposed solution is simple and valid only under strong assumptions, the utility of the query is additive in its items and the item utility noise is independent. Nevertheless, the problem is studied in depth, including a lower bound and comparison to model-based approaches to solving the problem. All reviewers liked the paper and I support acceptance.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors propose a simple solution to this problem, take top-K most frequently chosen items, and prove that it is near optimal. The reviewers liked the paper and I support acceptance."
"This paper proposes a unified model-based framework for high-level skill learning and composition through hierarchical RL. The proposed approach combines high-level planning in a low dimensional space with low-level skill learning, where each low-level skill is a policy conditioned on the high-level task. The low-level policies are learned by using a mutual information objective. The proposed approach is evaluated on locomotion tasks, and is shown to be overall more data efficient than alternative baselines.
The reviewers agree that this work is original and sufficiently empirically motivated for acceptance. Two reviewers were concerned by the experimental setup and the transfer setting that are somehow too simple, but the authors fixed these issues in the improved version based on the feedback.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions. The reviewers agree that this work is original and sufficiently empirically motivated for acceptance."
Non autoregressive modelling for text to speech (TTS) is an important and challenging problem. This paper proposes a deep VAE approach and show promising results. Both the reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper. This paper will not be the final VAE contribution to TTS but represents a significant enough contribution to the field to warrant publication. It is highly recommended that the authors take into account the reviewers' comments.,"This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. Both reviewers and the authors have engaged in a constructive discussion on the merits and claims of the paper."
"The motivation for using semi-supervised learning in medical imaging is clear in order to reduce the cost of acquiring dense expert annotations. This paper presents an interesting study on applying recent semi-supervised learning methods to chest x-ray segmentation. There are some conflicting reviews here but in general, the reviewers find the work interesting enough to be presented but with too limited technical novelty to justify an oral presentation.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The motivation for using semi-supervised learning is clear in order to reduce the cost of acquiring dense expert annotations."
"The paper studies the batch RL problem, in which the algorithm first decide a switching schedule and then switch the policies based on this schedule. The proposed approach achieves a good regret upper bound matching existing non-batch algorithms (although the lower-order terms are still large). The batch complexity on the other hand matches the lower bound (up to log factors). 

The reviewers believe that the theoretical contributions are solid and qualified to be published in NeurIPS. The authors did a good a job in addressing the computation complexity in the rebuttal phase. The meta-reviewer suggests the authors to further clarify presentation issues. Also, it would be good to cite the recent RL theory papers in the tabular setting (including those with a generative model).","The paper introduces a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers believe that the theoretical contributions are solid and qualified to be published in NeurIPS."
"The authors proposes augmented tensor decomposition (ATD) to adapt data reconstruction towards the downstream tasks, e.g., appropriate feature clustering.  It leverages data augmentation and self-supervised learning, with optimization accomplished akin to alternating least square (ALS).  Significantly superior performance is obtained in experiment, compared with existing CP methods and ALS.    

All the reviewers, including myself, find the paper a solid contribution to the methodology and analysis. There were a few concerns and clarification requests, and the rebuttal did a good job addressing them. These additional results and insights can be included in the final version of the paper.","The paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers, including myself, find the paper a solid contribution to the methodology and analysis, and the rebuttal did a good job address them."
"The paper proposes a comprehensive benchmark for program understanding and generation consisting of 10 tasks across 14 datasets. All the reviewers agree that this dataset has potentially be used as a standard benchmark to measure progress in this domain. Therefore, I recommend acceptance of this paper.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. All reviewers agree that this dataset has potentially be used as a standard benchmark to measure progress in this domain. Therefore, I recommend acceptance of this paper."
"This paper describes a clever new class of piecewise-linear RNNs that contains a long-time scale memory subsystem. The reviewers found the paper interesting and valuable, and I agree. The four submitted reviews were unanimous in their vote to accept. The theoretical insights and empirical results are impactful and would be suitable for spotlight presentation.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers were unanimous in their vote to accept. The theoretical insights and empirical results are impactful."
"The reviewers unanimously acknowledge the clarity of the paper and the relevance of the theoretical contribution. One weakness pointed out by reviewers is the limited experimental evaluation. Overall, this constitutes a good theoretical contribution at the intersection of causal representation learning and nonlinear ICA, which we recommend for acceptance.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. The reviewers unanimously acknowledge the clarity of the paper and the relevance of the theoretical contribution. Overall, this paper constitutes a good theoretical contribution at the intersection of causal representation learning and nonlinear ICA, which we recommend for acceptance."
"This work presents a method for training neural nets on synthetic data. This data is collected from a collection of thousands of OpenGL programs that rendered images, which are then used for representation learning. The big advantage of this approach is that it avoids of a lot of the biases that are present in natural image datasets. The proposed method is competitive for supervised and unsupervised scenarios. I find the results, especially those with finetuning (as done during the rebuttal period) relatively compelling. 

While I agree with reviewer vdrw that, on the whole, the major contribution (an image collection) is not very strong, I still think this kind of approach will be widely interesting to the NeurIPS community. Precisely because the work shows carefully (albeit empirically only) that procedurally generated datasets could be useful for representation learning, especially if you want to avoid the various pitfalls of natural image sets.","This paper proposes a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. I agree with reviewer vdrw that, on the whole, the major contribution is that it avoids biases that are present in natural image datasets."
"This paper addresses an importnant and more realistic setting of multi-task RL where the reward function changes; the approach is elegant, and empirical results are convincing. The paper presents an importnant contribution to the challenging multi-task RL problem.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The paper presents an importnant contribution to multi-task RL problem."
"In this paper the authors propose an approach for semi-supervised salient object detection using a combination of pseudo-label prediction and adversarial training, showing improved results on a number of benchmarks. Some concerns about more detailed analysis of aspects of the approach were raised, but seemed to be mostly addressed in the authors’ response. Some reviewers also expressed concerns about there being sufficient technical contributions, but seemed satisfied enough with the analysis and strong positive results. ","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers expressed concerns about generalization of the approach to different datasets."
"While all reviewers agree that the topic is interesting and the work has merit, several issues have been pointed out, especially by R1 and R3, that indicate that the work is not  ready for acceptance at this stage. the authors are strongly encouraged to continue to work on this topic, taking into account the feedback received.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"the reviewer's opinion was split on this paper, but after rebuttal no major concerns remained so I recommend acceptance. However, the authors should aim to incorporate the answers given to the reviewers concerns in the final version.","Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Your summary should be between 2 to 4 sentences long. Focus on highlighting the main contributions, strengths, weaknesses, and overall recommendation as evaluated by the reviewers. Overall, the paper received a positive recommendation with minor revisions."
The reviewers have reached consensus after processing the authors' feedback. They all agree that this manuscript presents an interesting approach to applying variational inference in a setting of probabilistic programming that is of interest to the community. The reviewers raise tangible points that the authors have incorporated into their revision. I recommend that the authors continue to polish their manuscript to clearly address these points in the final version of their manuscript.,"Your task is to generate concise summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. The reviewers have reached consensus after processing the reviewers' feedback. They all agree that this paper presents an interesting approach to applying variational inference in a setting of probabilistic programming that is of interest to the community."
"Reviewers are in agreement that this work is a useful, clear, documentary piece of work that shows the utility of CLIP on a number of popular V+L tasks.  There is a somewhat persistent concern that simply demonstrating that a stronger visual encoder leads to improvements downstream is not an insightful result on which the community can build.","Your task is to generate summaries for the metareviews of various papers in the field of [insert specific field, e.g., machine learning]. Ensure that the summary is concise, informative, and accurately represents the content of the paper. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions."
"The authors design an algorithm for composite stochastic optimization that leverages both smoothness and strong convexity with respect to the same (general) norm, using a stochastic counterpart to recent work by Diakonikolas and Guzman. They then show how to leverage this algorithm and randomized smoothing in order to create an algorithm for constrained smooth convex optimization based on exact gradient evaluations and linear optimization computations. Compared to Frank-Wolfe, the algorithm requires strictly less gradient evaluations and parallelizes the same amount of linear optimization computations.

The paper received generally favorable reviews, with the exception of reviewer 3QVT who did not engage in discussion and whose critique I found unclear. I agree with reviewer rQnJ’s assessment that even though “all the building block are quite known in optimization community (accelerated methods, duality, Bregman distances, smoothing, etc.), the whole approach fits perfectly together and provides the reader with a number of nice and useful observations.” Consequently, I recommend acceptance.","The paper introduces a novel neural network training approach, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The reviewers agreed with reviewers that the whole approach fits perfectly together and provides the reader with some nice and useful observations."
This paper uses a masked autoencoder as visual pretraining for robot manipulation tasks. The paper shows strong results and received positive initial reviews.  The additional experiments provided during the rebuttal strengthen the paper's contribution claim. ,"This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The additional experiments provided during the rebuttal strengthen the paper's contribution claim."
"This paper received unanimous recommendations of acceptance from the reviewer. The authors did a good job addressing concerns from the reviewers, especially with the additional ablation studies to decouple the gains from other techniques such as SupCon. The AC agrees with the reviewers regarding the contribution of this paper and recommends acceptance.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. Overall, the paper received a positive recommendation with minor revisions. The authors did a good job addressing concerns from the reviewers, especially with the additional ablation studies to decouple the gains from other techniques such as SupCon."
"This paper presents a model to identify entity mentions that are synonymous.  This could have utility in practical scenarios that handle entities.

The main criticism of the paper is regarding the baselines used.  Most of the baselines that are compared against are extremely simple.  There is a significant body of literature that models paraphrase and entailment and many of those baselines are missing (decomposable attention, DIIN, other cross-attention mechanisms).  Adding those experiments would make the experimental setup stronger.

There is a bit of a disagreement between reviewers, but I agree with the two reviewers who point out the weakness of the experimental setup, and fixing those issues could improve the paper significantly.","This paper proposes a novel approach to neural network training, leveraging recent advancements in optimization techniques. Reviewers found the experimental results compelling, demonstrating significant improvements over existing methods. However, some concerns were raised about generalization of the approach to different datasets. Overall, the paper received a positive recommendation with minor revisions."
